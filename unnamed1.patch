Index: unnamed.patch
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
--- unnamed.patch	(date 1573624570679)
+++ unnamed.patch	(date 1573624570679)
@@ -0,0 +1,34586 @@
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/webencodings/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/webencodings/__init__.py	(date 1573549702097)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/webencodings/__init__.py	(date 1573549702097)
+@@ -0,0 +1,342 @@
++# coding: utf-8
++"""
++
++    webencodings
++    ~~~~~~~~~~~~
++
++    This is a Python implementation of the `WHATWG Encoding standard
++    <http://encoding.spec.whatwg.org/>`. See README for details.
++
++    :copyright: Copyright 2012 by Simon Sapin
++    :license: BSD, see LICENSE for details.
++
++"""
++
++from __future__ import unicode_literals
++
++import codecs
++
++from .labels import LABELS
++
++
++VERSION = '0.5.1'
++
++
++# Some names in Encoding are not valid Python aliases. Remap these.
++PYTHON_NAMES = {
++    'iso-8859-8-i': 'iso-8859-8',
++    'x-mac-cyrillic': 'mac-cyrillic',
++    'macintosh': 'mac-roman',
++    'windows-874': 'cp874'}
++
++CACHE = {}
++
++
++def ascii_lower(string):
++    r"""Transform (only) ASCII letters to lower case: A-Z is mapped to a-z.
++
++    :param string: An Unicode string.
++    :returns: A new Unicode string.
++
++    This is used for `ASCII case-insensitive
++    <http://encoding.spec.whatwg.org/#ascii-case-insensitive>`_
++    matching of encoding labels.
++    The same matching is also used, among other things,
++    for `CSS keywords <http://dev.w3.org/csswg/css-values/#keywords>`_.
++
++    This is different from the :meth:`~py:str.lower` method of Unicode strings
++    which also affect non-ASCII characters,
++    sometimes mapping them into the ASCII range:
++
++        >>> keyword = u'Bac\N{KELVIN SIGN}ground'
++        >>> assert keyword.lower() == u'background'
++        >>> assert ascii_lower(keyword) != keyword.lower()
++        >>> assert ascii_lower(keyword) == u'bac\N{KELVIN SIGN}ground'
++
++    """
++    # This turns out to be faster than unicode.translate()
++    return string.encode('utf8').lower().decode('utf8')
++
++
++def lookup(label):
++    """
++    Look for an encoding by its label.
++    This is the spec’s `get an encoding
++    <http://encoding.spec.whatwg.org/#concept-encoding-get>`_ algorithm.
++    Supported labels are listed there.
++
++    :param label: A string.
++    :returns:
++        An :class:`Encoding` object, or :obj:`None` for an unknown label.
++
++    """
++    # Only strip ASCII whitespace: U+0009, U+000A, U+000C, U+000D, and U+0020.
++    label = ascii_lower(label.strip('\t\n\f\r '))
++    name = LABELS.get(label)
++    if name is None:
++        return None
++    encoding = CACHE.get(name)
++    if encoding is None:
++        if name == 'x-user-defined':
++            from .x_user_defined import codec_info
++        else:
++            python_name = PYTHON_NAMES.get(name, name)
++            # Any python_name value that gets to here should be valid.
++            codec_info = codecs.lookup(python_name)
++        encoding = Encoding(name, codec_info)
++        CACHE[name] = encoding
++    return encoding
++
++
++def _get_encoding(encoding_or_label):
++    """
++    Accept either an encoding object or label.
++
++    :param encoding: An :class:`Encoding` object or a label string.
++    :returns: An :class:`Encoding` object.
++    :raises: :exc:`~exceptions.LookupError` for an unknown label.
++
++    """
++    if hasattr(encoding_or_label, 'codec_info'):
++        return encoding_or_label
++
++    encoding = lookup(encoding_or_label)
++    if encoding is None:
++        raise LookupError('Unknown encoding label: %r' % encoding_or_label)
++    return encoding
++
++
++class Encoding(object):
++    """Reresents a character encoding such as UTF-8,
++    that can be used for decoding or encoding.
++
++    .. attribute:: name
++
++        Canonical name of the encoding
++
++    .. attribute:: codec_info
++
++        The actual implementation of the encoding,
++        a stdlib :class:`~codecs.CodecInfo` object.
++        See :func:`codecs.register`.
++
++    """
++    def __init__(self, name, codec_info):
++        self.name = name
++        self.codec_info = codec_info
++
++    def __repr__(self):
++        return '<Encoding %s>' % self.name
++
++
++#: The UTF-8 encoding. Should be used for new content and formats.
++UTF8 = lookup('utf-8')
++
++_UTF16LE = lookup('utf-16le')
++_UTF16BE = lookup('utf-16be')
++
++
++def decode(input, fallback_encoding, errors='replace'):
++    """
++    Decode a single string.
++
++    :param input: A byte string
++    :param fallback_encoding:
++        An :class:`Encoding` object or a label string.
++        The encoding to use if :obj:`input` does note have a BOM.
++    :param errors: Type of error handling. See :func:`codecs.register`.
++    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
++    :return:
++        A ``(output, encoding)`` tuple of an Unicode string
++        and an :obj:`Encoding`.
++
++    """
++    # Fail early if `encoding` is an invalid label.
++    fallback_encoding = _get_encoding(fallback_encoding)
++    bom_encoding, input = _detect_bom(input)
++    encoding = bom_encoding or fallback_encoding
++    return encoding.codec_info.decode(input, errors)[0], encoding
++
++
++def _detect_bom(input):
++    """Return (bom_encoding, input), with any BOM removed from the input."""
++    if input.startswith(b'\xFF\xFE'):
++        return _UTF16LE, input[2:]
++    if input.startswith(b'\xFE\xFF'):
++        return _UTF16BE, input[2:]
++    if input.startswith(b'\xEF\xBB\xBF'):
++        return UTF8, input[3:]
++    return None, input
++
++
++def encode(input, encoding=UTF8, errors='strict'):
++    """
++    Encode a single string.
++
++    :param input: An Unicode string.
++    :param encoding: An :class:`Encoding` object or a label string.
++    :param errors: Type of error handling. See :func:`codecs.register`.
++    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
++    :return: A byte string.
++
++    """
++    return _get_encoding(encoding).codec_info.encode(input, errors)[0]
++
++
++def iter_decode(input, fallback_encoding, errors='replace'):
++    """
++    "Pull"-based decoder.
++
++    :param input:
++        An iterable of byte strings.
++
++        The input is first consumed just enough to determine the encoding
++        based on the precense of a BOM,
++        then consumed on demand when the return value is.
++    :param fallback_encoding:
++        An :class:`Encoding` object or a label string.
++        The encoding to use if :obj:`input` does note have a BOM.
++    :param errors: Type of error handling. See :func:`codecs.register`.
++    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
++    :returns:
++        An ``(output, encoding)`` tuple.
++        :obj:`output` is an iterable of Unicode strings,
++        :obj:`encoding` is the :obj:`Encoding` that is being used.
++
++    """
++
++    decoder = IncrementalDecoder(fallback_encoding, errors)
++    generator = _iter_decode_generator(input, decoder)
++    encoding = next(generator)
++    return generator, encoding
++
++
++def _iter_decode_generator(input, decoder):
++    """Return a generator that first yields the :obj:`Encoding`,
++    then yields output chukns as Unicode strings.
++
++    """
++    decode = decoder.decode
++    input = iter(input)
++    for chunck in input:
++        output = decode(chunck)
++        if output:
++            assert decoder.encoding is not None
++            yield decoder.encoding
++            yield output
++            break
++    else:
++        # Input exhausted without determining the encoding
++        output = decode(b'', final=True)
++        assert decoder.encoding is not None
++        yield decoder.encoding
++        if output:
++            yield output
++        return
++
++    for chunck in input:
++        output = decode(chunck)
++        if output:
++            yield output
++    output = decode(b'', final=True)
++    if output:
++        yield output
++
++
++def iter_encode(input, encoding=UTF8, errors='strict'):
++    """
++    “Pull”-based encoder.
++
++    :param input: An iterable of Unicode strings.
++    :param encoding: An :class:`Encoding` object or a label string.
++    :param errors: Type of error handling. See :func:`codecs.register`.
++    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
++    :returns: An iterable of byte strings.
++
++    """
++    # Fail early if `encoding` is an invalid label.
++    encode = IncrementalEncoder(encoding, errors).encode
++    return _iter_encode_generator(input, encode)
++
++
++def _iter_encode_generator(input, encode):
++    for chunck in input:
++        output = encode(chunck)
++        if output:
++            yield output
++    output = encode('', final=True)
++    if output:
++        yield output
++
++
++class IncrementalDecoder(object):
++    """
++    “Push”-based decoder.
++
++    :param fallback_encoding:
++        An :class:`Encoding` object or a label string.
++        The encoding to use if :obj:`input` does note have a BOM.
++    :param errors: Type of error handling. See :func:`codecs.register`.
++    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
++
++    """
++    def __init__(self, fallback_encoding, errors='replace'):
++        # Fail early if `encoding` is an invalid label.
++        self._fallback_encoding = _get_encoding(fallback_encoding)
++        self._errors = errors
++        self._buffer = b''
++        self._decoder = None
++        #: The actual :class:`Encoding` that is being used,
++        #: or :obj:`None` if that is not determined yet.
++        #: (Ie. if there is not enough input yet to determine
++        #: if there is a BOM.)
++        self.encoding = None  # Not known yet.
++
++    def decode(self, input, final=False):
++        """Decode one chunk of the input.
++
++        :param input: A byte string.
++        :param final:
++            Indicate that no more input is available.
++            Must be :obj:`True` if this is the last call.
++        :returns: An Unicode string.
++
++        """
++        decoder = self._decoder
++        if decoder is not None:
++            return decoder(input, final)
++
++        input = self._buffer + input
++        encoding, input = _detect_bom(input)
++        if encoding is None:
++            if len(input) < 3 and not final:  # Not enough data yet.
++                self._buffer = input
++                return ''
++            else:  # No BOM
++                encoding = self._fallback_encoding
++        decoder = encoding.codec_info.incrementaldecoder(self._errors).decode
++        self._decoder = decoder
++        self.encoding = encoding
++        return decoder(input, final)
++
++
++class IncrementalEncoder(object):
++    """
++    “Push”-based encoder.
++
++    :param encoding: An :class:`Encoding` object or a label string.
++    :param errors: Type of error handling. See :func:`codecs.register`.
++    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.
++
++    .. method:: encode(input, final=False)
++
++        :param input: An Unicode string.
++        :param final:
++            Indicate that no more input is available.
++            Must be :obj:`True` if this is the last call.
++        :returns: A byte string.
++
++    """
++    def __init__(self, encoding=UTF8, errors='strict'):
++        encoding = _get_encoding(encoding)
++        self.encode = encoding.codec_info.incrementalencoder(errors).encode
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/pkg_resources/py31compat.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/pkg_resources/py31compat.py	(date 1573549701584)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/pkg_resources/py31compat.py	(date 1573549701584)
+@@ -0,0 +1,22 @@
++import os
++import errno
++import sys
++
++
++def _makedirs_31(path, exist_ok=False):
++    try:
++        os.makedirs(path)
++    except OSError as exc:
++        if not exist_ok or exc.errno != errno.EEXIST:
++            raise
++
++
++# rely on compatibility behavior until mode considerations
++#  and exists_ok considerations are disentangled.
++# See https://github.com/pypa/setuptools/pull/1083#issuecomment-315168663
++needs_makedirs = (
++    sys.version_info < (3, 2, 5) or
++    (3, 3) <= sys.version_info < (3, 3, 6) or
++    (3, 4) <= sys.version_info < (3, 4, 1)
++)
++makedirs = _makedirs_31 if needs_makedirs else os.makedirs
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/pkg_resources/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/pkg_resources/__init__.py	(date 1573549701577)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_vendor/pkg_resources/__init__.py	(date 1573549701577)
+@@ -0,0 +1,3125 @@
++# coding: utf-8
++"""
++Package resource API
++--------------------
++
++A resource is a logical file contained within a package, or a logical
++subdirectory thereof.  The package resource API expects resource names
++to have their path parts separated with ``/``, *not* whatever the local
++path separator is.  Do not use os.path operations to manipulate resource
++names being passed into the API.
++
++The package resource API is designed to work with normal filesystem packages,
++.egg files, and unpacked .egg files.  It can also work in a limited way with
++.zip files and with custom PEP 302 loaders that support the ``get_data()``
++method.
++"""
++
++from __future__ import absolute_import
++
++import sys
++import os
++import io
++import time
++import re
++import types
++import zipfile
++import zipimport
++import warnings
++import stat
++import functools
++import pkgutil
++import operator
++import platform
++import collections
++import plistlib
++import email.parser
++import errno
++import tempfile
++import textwrap
++import itertools
++import inspect
++from pkgutil import get_importer
++
++try:
++    import _imp
++except ImportError:
++    # Python 3.2 compatibility
++    import imp as _imp
++
++from pip._vendor import six
++from pip._vendor.six.moves import urllib, map, filter
++
++# capture these to bypass sandboxing
++from os import utime
++try:
++    from os import mkdir, rename, unlink
++    WRITE_SUPPORT = True
++except ImportError:
++    # no write support, probably under GAE
++    WRITE_SUPPORT = False
++
++from os import open as os_open
++from os.path import isdir, split
++
++try:
++    import importlib.machinery as importlib_machinery
++    # access attribute to force import under delayed import mechanisms.
++    importlib_machinery.__name__
++except ImportError:
++    importlib_machinery = None
++
++from . import py31compat
++from pip._vendor import appdirs
++from pip._vendor import packaging
++__import__('pip._vendor.packaging.version')
++__import__('pip._vendor.packaging.specifiers')
++__import__('pip._vendor.packaging.requirements')
++__import__('pip._vendor.packaging.markers')
++
++
++if (3, 0) < sys.version_info < (3, 3):
++    raise RuntimeError("Python 3.3 or later is required")
++
++if six.PY2:
++    # Those builtin exceptions are only defined in Python 3
++    PermissionError = None
++    NotADirectoryError = None
++
++# declare some globals that will be defined later to
++# satisfy the linters.
++require = None
++working_set = None
++add_activation_listener = None
++resources_stream = None
++cleanup_resources = None
++resource_dir = None
++resource_stream = None
++set_extraction_path = None
++resource_isdir = None
++resource_string = None
++iter_entry_points = None
++resource_listdir = None
++resource_filename = None
++resource_exists = None
++_distribution_finders = None
++_namespace_handlers = None
++_namespace_packages = None
++
++
++class PEP440Warning(RuntimeWarning):
++    """
++    Used when there is an issue with a version or specifier not complying with
++    PEP 440.
++    """
++
++
++def parse_version(v):
++    try:
++        return packaging.version.Version(v)
++    except packaging.version.InvalidVersion:
++        return packaging.version.LegacyVersion(v)
++
++
++_state_vars = {}
++
++
++def _declare_state(vartype, **kw):
++    globals().update(kw)
++    _state_vars.update(dict.fromkeys(kw, vartype))
++
++
++def __getstate__():
++    state = {}
++    g = globals()
++    for k, v in _state_vars.items():
++        state[k] = g['_sget_' + v](g[k])
++    return state
++
++
++def __setstate__(state):
++    g = globals()
++    for k, v in state.items():
++        g['_sset_' + _state_vars[k]](k, g[k], v)
++    return state
++
++
++def _sget_dict(val):
++    return val.copy()
++
++
++def _sset_dict(key, ob, state):
++    ob.clear()
++    ob.update(state)
++
++
++def _sget_object(val):
++    return val.__getstate__()
++
++
++def _sset_object(key, ob, state):
++    ob.__setstate__(state)
++
++
++_sget_none = _sset_none = lambda *args: None
++
++
++def get_supported_platform():
++    """Return this platform's maximum compatible version.
++
++    distutils.util.get_platform() normally reports the minimum version
++    of Mac OS X that would be required to *use* extensions produced by
++    distutils.  But what we want when checking compatibility is to know the
++    version of Mac OS X that we are *running*.  To allow usage of packages that
++    explicitly require a newer version of Mac OS X, we must also know the
++    current version of the OS.
++
++    If this condition occurs for any other platform with a version in its
++    platform strings, this function should be extended accordingly.
++    """
++    plat = get_build_platform()
++    m = macosVersionString.match(plat)
++    if m is not None and sys.platform == "darwin":
++        try:
++            plat = 'macosx-%s-%s' % ('.'.join(_macosx_vers()[:2]), m.group(3))
++        except ValueError:
++            # not Mac OS X
++            pass
++    return plat
++
++
++__all__ = [
++    # Basic resource access and distribution/entry point discovery
++    'require', 'run_script', 'get_provider', 'get_distribution',
++    'load_entry_point', 'get_entry_map', 'get_entry_info',
++    'iter_entry_points',
++    'resource_string', 'resource_stream', 'resource_filename',
++    'resource_listdir', 'resource_exists', 'resource_isdir',
++
++    # Environmental control
++    'declare_namespace', 'working_set', 'add_activation_listener',
++    'find_distributions', 'set_extraction_path', 'cleanup_resources',
++    'get_default_cache',
++
++    # Primary implementation classes
++    'Environment', 'WorkingSet', 'ResourceManager',
++    'Distribution', 'Requirement', 'EntryPoint',
++
++    # Exceptions
++    'ResolutionError', 'VersionConflict', 'DistributionNotFound',
++    'UnknownExtra', 'ExtractionError',
++
++    # Warnings
++    'PEP440Warning',
++
++    # Parsing functions and string utilities
++    'parse_requirements', 'parse_version', 'safe_name', 'safe_version',
++    'get_platform', 'compatible_platforms', 'yield_lines', 'split_sections',
++    'safe_extra', 'to_filename', 'invalid_marker', 'evaluate_marker',
++
++    # filesystem utilities
++    'ensure_directory', 'normalize_path',
++
++    # Distribution "precedence" constants
++    'EGG_DIST', 'BINARY_DIST', 'SOURCE_DIST', 'CHECKOUT_DIST', 'DEVELOP_DIST',
++
++    # "Provider" interfaces, implementations, and registration/lookup APIs
++    'IMetadataProvider', 'IResourceProvider', 'FileMetadata',
++    'PathMetadata', 'EggMetadata', 'EmptyProvider', 'empty_provider',
++    'NullProvider', 'EggProvider', 'DefaultProvider', 'ZipProvider',
++    'register_finder', 'register_namespace_handler', 'register_loader_type',
++    'fixup_namespace_packages', 'get_importer',
++
++    # Deprecated/backward compatibility only
++    'run_main', 'AvailableDistributions',
++]
++
++
++class ResolutionError(Exception):
++    """Abstract base for dependency resolution errors"""
++
++    def __repr__(self):
++        return self.__class__.__name__ + repr(self.args)
++
++
++class VersionConflict(ResolutionError):
++    """
++    An already-installed version conflicts with the requested version.
++
++    Should be initialized with the installed Distribution and the requested
++    Requirement.
++    """
++
++    _template = "{self.dist} is installed but {self.req} is required"
++
++    @property
++    def dist(self):
++        return self.args[0]
++
++    @property
++    def req(self):
++        return self.args[1]
++
++    def report(self):
++        return self._template.format(**locals())
++
++    def with_context(self, required_by):
++        """
++        If required_by is non-empty, return a version of self that is a
++        ContextualVersionConflict.
++        """
++        if not required_by:
++            return self
++        args = self.args + (required_by,)
++        return ContextualVersionConflict(*args)
++
++
++class ContextualVersionConflict(VersionConflict):
++    """
++    A VersionConflict that accepts a third parameter, the set of the
++    requirements that required the installed Distribution.
++    """
++
++    _template = VersionConflict._template + ' by {self.required_by}'
++
++    @property
++    def required_by(self):
++        return self.args[2]
++
++
++class DistributionNotFound(ResolutionError):
++    """A requested distribution was not found"""
++
++    _template = ("The '{self.req}' distribution was not found "
++                 "and is required by {self.requirers_str}")
++
++    @property
++    def req(self):
++        return self.args[0]
++
++    @property
++    def requirers(self):
++        return self.args[1]
++
++    @property
++    def requirers_str(self):
++        if not self.requirers:
++            return 'the application'
++        return ', '.join(self.requirers)
++
++    def report(self):
++        return self._template.format(**locals())
++
++    def __str__(self):
++        return self.report()
++
++
++class UnknownExtra(ResolutionError):
++    """Distribution doesn't have an "extra feature" of the given name"""
++
++
++_provider_factories = {}
++
++PY_MAJOR = sys.version[:3]
++EGG_DIST = 3
++BINARY_DIST = 2
++SOURCE_DIST = 1
++CHECKOUT_DIST = 0
++DEVELOP_DIST = -1
++
++
++def register_loader_type(loader_type, provider_factory):
++    """Register `provider_factory` to make providers for `loader_type`
++
++    `loader_type` is the type or class of a PEP 302 ``module.__loader__``,
++    and `provider_factory` is a function that, passed a *module* object,
++    returns an ``IResourceProvider`` for that module.
++    """
++    _provider_factories[loader_type] = provider_factory
++
++
++def get_provider(moduleOrReq):
++    """Return an IResourceProvider for the named module or requirement"""
++    if isinstance(moduleOrReq, Requirement):
++        return working_set.find(moduleOrReq) or require(str(moduleOrReq))[0]
++    try:
++        module = sys.modules[moduleOrReq]
++    except KeyError:
++        __import__(moduleOrReq)
++        module = sys.modules[moduleOrReq]
++    loader = getattr(module, '__loader__', None)
++    return _find_adapter(_provider_factories, loader)(module)
++
++
++def _macosx_vers(_cache=[]):
++    if not _cache:
++        version = platform.mac_ver()[0]
++        # fallback for MacPorts
++        if version == '':
++            plist = '/System/Library/CoreServices/SystemVersion.plist'
++            if os.path.exists(plist):
++                if hasattr(plistlib, 'readPlist'):
++                    plist_content = plistlib.readPlist(plist)
++                    if 'ProductVersion' in plist_content:
++                        version = plist_content['ProductVersion']
++
++        _cache.append(version.split('.'))
++    return _cache[0]
++
++
++def _macosx_arch(machine):
++    return {'PowerPC': 'ppc', 'Power_Macintosh': 'ppc'}.get(machine, machine)
++
++
++def get_build_platform():
++    """Return this platform's string for platform-specific distributions
++
++    XXX Currently this is the same as ``distutils.util.get_platform()``, but it
++    needs some hacks for Linux and Mac OS X.
++    """
++    try:
++        # Python 2.7 or >=3.2
++        from sysconfig import get_platform
++    except ImportError:
++        from distutils.util import get_platform
++
++    plat = get_platform()
++    if sys.platform == "darwin" and not plat.startswith('macosx-'):
++        try:
++            version = _macosx_vers()
++            machine = os.uname()[4].replace(" ", "_")
++            return "macosx-%d.%d-%s" % (
++                int(version[0]), int(version[1]),
++                _macosx_arch(machine),
++            )
++        except ValueError:
++            # if someone is running a non-Mac darwin system, this will fall
++            # through to the default implementation
++            pass
++    return plat
++
++
++macosVersionString = re.compile(r"macosx-(\d+)\.(\d+)-(.*)")
++darwinVersionString = re.compile(r"darwin-(\d+)\.(\d+)\.(\d+)-(.*)")
++# XXX backward compat
++get_platform = get_build_platform
++
++
++def compatible_platforms(provided, required):
++    """Can code for the `provided` platform run on the `required` platform?
++
++    Returns true if either platform is ``None``, or the platforms are equal.
++
++    XXX Needs compatibility checks for Linux and other unixy OSes.
++    """
++    if provided is None or required is None or provided == required:
++        # easy case
++        return True
++
++    # Mac OS X special cases
++    reqMac = macosVersionString.match(required)
++    if reqMac:
++        provMac = macosVersionString.match(provided)
++
++        # is this a Mac package?
++        if not provMac:
++            # this is backwards compatibility for packages built before
++            # setuptools 0.6. All packages built after this point will
++            # use the new macosx designation.
++            provDarwin = darwinVersionString.match(provided)
++            if provDarwin:
++                dversion = int(provDarwin.group(1))
++                macosversion = "%s.%s" % (reqMac.group(1), reqMac.group(2))
++                if dversion == 7 and macosversion >= "10.3" or \
++                        dversion == 8 and macosversion >= "10.4":
++                    return True
++            # egg isn't macosx or legacy darwin
++            return False
++
++        # are they the same major version and machine type?
++        if provMac.group(1) != reqMac.group(1) or \
++                provMac.group(3) != reqMac.group(3):
++            return False
++
++        # is the required OS major update >= the provided one?
++        if int(provMac.group(2)) > int(reqMac.group(2)):
++            return False
++
++        return True
++
++    # XXX Linux and other platforms' special cases should go here
++    return False
++
++
++def run_script(dist_spec, script_name):
++    """Locate distribution `dist_spec` and run its `script_name` script"""
++    ns = sys._getframe(1).f_globals
++    name = ns['__name__']
++    ns.clear()
++    ns['__name__'] = name
++    require(dist_spec)[0].run_script(script_name, ns)
++
++
++# backward compatibility
++run_main = run_script
++
++
++def get_distribution(dist):
++    """Return a current distribution object for a Requirement or string"""
++    if isinstance(dist, six.string_types):
++        dist = Requirement.parse(dist)
++    if isinstance(dist, Requirement):
++        dist = get_provider(dist)
++    if not isinstance(dist, Distribution):
++        raise TypeError("Expected string, Requirement, or Distribution", dist)
++    return dist
++
++
++def load_entry_point(dist, group, name):
++    """Return `name` entry point of `group` for `dist` or raise ImportError"""
++    return get_distribution(dist).load_entry_point(group, name)
++
++
++def get_entry_map(dist, group=None):
++    """Return the entry point map for `group`, or the full entry map"""
++    return get_distribution(dist).get_entry_map(group)
++
++
++def get_entry_info(dist, group, name):
++    """Return the EntryPoint object for `group`+`name`, or ``None``"""
++    return get_distribution(dist).get_entry_info(group, name)
++
++
++class IMetadataProvider:
++    def has_metadata(name):
++        """Does the package's distribution contain the named metadata?"""
++
++    def get_metadata(name):
++        """The named metadata resource as a string"""
++
++    def get_metadata_lines(name):
++        """Yield named metadata resource as list of non-blank non-comment lines
++
++       Leading and trailing whitespace is stripped from each line, and lines
++       with ``#`` as the first non-blank character are omitted."""
++
++    def metadata_isdir(name):
++        """Is the named metadata a directory?  (like ``os.path.isdir()``)"""
++
++    def metadata_listdir(name):
++        """List of metadata names in the directory (like ``os.listdir()``)"""
++
++    def run_script(script_name, namespace):
++        """Execute the named script in the supplied namespace dictionary"""
++
++
++class IResourceProvider(IMetadataProvider):
++    """An object that provides access to package resources"""
++
++    def get_resource_filename(manager, resource_name):
++        """Return a true filesystem path for `resource_name`
++
++        `manager` must be an ``IResourceManager``"""
++
++    def get_resource_stream(manager, resource_name):
++        """Return a readable file-like object for `resource_name`
++
++        `manager` must be an ``IResourceManager``"""
++
++    def get_resource_string(manager, resource_name):
++        """Return a string containing the contents of `resource_name`
++
++        `manager` must be an ``IResourceManager``"""
++
++    def has_resource(resource_name):
++        """Does the package contain the named resource?"""
++
++    def resource_isdir(resource_name):
++        """Is the named resource a directory?  (like ``os.path.isdir()``)"""
++
++    def resource_listdir(resource_name):
++        """List of resource names in the directory (like ``os.listdir()``)"""
++
++
++class WorkingSet(object):
++    """A collection of active distributions on sys.path (or a similar list)"""
++
++    def __init__(self, entries=None):
++        """Create working set from list of path entries (default=sys.path)"""
++        self.entries = []
++        self.entry_keys = {}
++        self.by_key = {}
++        self.callbacks = []
++
++        if entries is None:
++            entries = sys.path
++
++        for entry in entries:
++            self.add_entry(entry)
++
++    @classmethod
++    def _build_master(cls):
++        """
++        Prepare the master working set.
++        """
++        ws = cls()
++        try:
++            from __main__ import __requires__
++        except ImportError:
++            # The main program does not list any requirements
++            return ws
++
++        # ensure the requirements are met
++        try:
++            ws.require(__requires__)
++        except VersionConflict:
++            return cls._build_from_requirements(__requires__)
++
++        return ws
++
++    @classmethod
++    def _build_from_requirements(cls, req_spec):
++        """
++        Build a working set from a requirement spec. Rewrites sys.path.
++        """
++        # try it without defaults already on sys.path
++        # by starting with an empty path
++        ws = cls([])
++        reqs = parse_requirements(req_spec)
++        dists = ws.resolve(reqs, Environment())
++        for dist in dists:
++            ws.add(dist)
++
++        # add any missing entries from sys.path
++        for entry in sys.path:
++            if entry not in ws.entries:
++                ws.add_entry(entry)
++
++        # then copy back to sys.path
++        sys.path[:] = ws.entries
++        return ws
++
++    def add_entry(self, entry):
++        """Add a path item to ``.entries``, finding any distributions on it
++
++        ``find_distributions(entry, True)`` is used to find distributions
++        corresponding to the path entry, and they are added.  `entry` is
++        always appended to ``.entries``, even if it is already present.
++        (This is because ``sys.path`` can contain the same value more than
++        once, and the ``.entries`` of the ``sys.path`` WorkingSet should always
++        equal ``sys.path``.)
++        """
++        self.entry_keys.setdefault(entry, [])
++        self.entries.append(entry)
++        for dist in find_distributions(entry, True):
++            self.add(dist, entry, False)
++
++    def __contains__(self, dist):
++        """True if `dist` is the active distribution for its project"""
++        return self.by_key.get(dist.key) == dist
++
++    def find(self, req):
++        """Find a distribution matching requirement `req`
++
++        If there is an active distribution for the requested project, this
++        returns it as long as it meets the version requirement specified by
++        `req`.  But, if there is an active distribution for the project and it
++        does *not* meet the `req` requirement, ``VersionConflict`` is raised.
++        If there is no active distribution for the requested project, ``None``
++        is returned.
++        """
++        dist = self.by_key.get(req.key)
++        if dist is not None and dist not in req:
++            # XXX add more info
++            raise VersionConflict(dist, req)
++        return dist
++
++    def iter_entry_points(self, group, name=None):
++        """Yield entry point objects from `group` matching `name`
++
++        If `name` is None, yields all entry points in `group` from all
++        distributions in the working set, otherwise only ones matching
++        both `group` and `name` are yielded (in distribution order).
++        """
++        for dist in self:
++            entries = dist.get_entry_map(group)
++            if name is None:
++                for ep in entries.values():
++                    yield ep
++            elif name in entries:
++                yield entries[name]
++
++    def run_script(self, requires, script_name):
++        """Locate distribution for `requires` and run `script_name` script"""
++        ns = sys._getframe(1).f_globals
++        name = ns['__name__']
++        ns.clear()
++        ns['__name__'] = name
++        self.require(requires)[0].run_script(script_name, ns)
++
++    def __iter__(self):
++        """Yield distributions for non-duplicate projects in the working set
++
++        The yield order is the order in which the items' path entries were
++        added to the working set.
++        """
++        seen = {}
++        for item in self.entries:
++            if item not in self.entry_keys:
++                # workaround a cache issue
++                continue
++
++            for key in self.entry_keys[item]:
++                if key not in seen:
++                    seen[key] = 1
++                    yield self.by_key[key]
++
++    def add(self, dist, entry=None, insert=True, replace=False):
++        """Add `dist` to working set, associated with `entry`
++
++        If `entry` is unspecified, it defaults to the ``.location`` of `dist`.
++        On exit from this routine, `entry` is added to the end of the working
++        set's ``.entries`` (if it wasn't already present).
++
++        `dist` is only added to the working set if it's for a project that
++        doesn't already have a distribution in the set, unless `replace=True`.
++        If it's added, any callbacks registered with the ``subscribe()`` method
++        will be called.
++        """
++        if insert:
++            dist.insert_on(self.entries, entry, replace=replace)
++
++        if entry is None:
++            entry = dist.location
++        keys = self.entry_keys.setdefault(entry, [])
++        keys2 = self.entry_keys.setdefault(dist.location, [])
++        if not replace and dist.key in self.by_key:
++            # ignore hidden distros
++            return
++
++        self.by_key[dist.key] = dist
++        if dist.key not in keys:
++            keys.append(dist.key)
++        if dist.key not in keys2:
++            keys2.append(dist.key)
++        self._added_new(dist)
++
++    def resolve(self, requirements, env=None, installer=None,
++                replace_conflicting=False, extras=None):
++        """List all distributions needed to (recursively) meet `requirements`
++
++        `requirements` must be a sequence of ``Requirement`` objects.  `env`,
++        if supplied, should be an ``Environment`` instance.  If
++        not supplied, it defaults to all distributions available within any
++        entry or distribution in the working set.  `installer`, if supplied,
++        will be invoked with each requirement that cannot be met by an
++        already-installed distribution; it should return a ``Distribution`` or
++        ``None``.
++
++        Unless `replace_conflicting=True`, raises a VersionConflict exception
++        if
++        any requirements are found on the path that have the correct name but
++        the wrong version.  Otherwise, if an `installer` is supplied it will be
++        invoked to obtain the correct version of the requirement and activate
++        it.
++
++        `extras` is a list of the extras to be used with these requirements.
++        This is important because extra requirements may look like `my_req;
++        extra = "my_extra"`, which would otherwise be interpreted as a purely
++        optional requirement.  Instead, we want to be able to assert that these
++        requirements are truly required.
++        """
++
++        # set up the stack
++        requirements = list(requirements)[::-1]
++        # set of processed requirements
++        processed = {}
++        # key -> dist
++        best = {}
++        to_activate = []
++
++        req_extras = _ReqExtras()
++
++        # Mapping of requirement to set of distributions that required it;
++        # useful for reporting info about conflicts.
++        required_by = collections.defaultdict(set)
++
++        while requirements:
++            # process dependencies breadth-first
++            req = requirements.pop(0)
++            if req in processed:
++                # Ignore cyclic or redundant dependencies
++                continue
++
++            if not req_extras.markers_pass(req, extras):
++                continue
++
++            dist = best.get(req.key)
++            if dist is None:
++                # Find the best distribution and add it to the map
++                dist = self.by_key.get(req.key)
++                if dist is None or (dist not in req and replace_conflicting):
++                    ws = self
++                    if env is None:
++                        if dist is None:
++                            env = Environment(self.entries)
++                        else:
++                            # Use an empty environment and workingset to avoid
++                            # any further conflicts with the conflicting
++                            # distribution
++                            env = Environment([])
++                            ws = WorkingSet([])
++                    dist = best[req.key] = env.best_match(
++                        req, ws, installer,
++                        replace_conflicting=replace_conflicting
++                    )
++                    if dist is None:
++                        requirers = required_by.get(req, None)
++                        raise DistributionNotFound(req, requirers)
++                to_activate.append(dist)
++            if dist not in req:
++                # Oops, the "best" so far conflicts with a dependency
++                dependent_req = required_by[req]
++                raise VersionConflict(dist, req).with_context(dependent_req)
++
++            # push the new requirements onto the stack
++            new_requirements = dist.requires(req.extras)[::-1]
++            requirements.extend(new_requirements)
++
++            # Register the new requirements needed by req
++            for new_requirement in new_requirements:
++                required_by[new_requirement].add(req.project_name)
++                req_extras[new_requirement] = req.extras
++
++            processed[req] = True
++
++        # return list of distros to activate
++        return to_activate
++
++    def find_plugins(
++            self, plugin_env, full_env=None, installer=None, fallback=True):
++        """Find all activatable distributions in `plugin_env`
++
++        Example usage::
++
++            distributions, errors = working_set.find_plugins(
++                Environment(plugin_dirlist)
++            )
++            # add plugins+libs to sys.path
++            map(working_set.add, distributions)
++            # display errors
++            print('Could not load', errors)
++
++        The `plugin_env` should be an ``Environment`` instance that contains
++        only distributions that are in the project's "plugin directory" or
++        directories. The `full_env`, if supplied, should be an ``Environment``
++        contains all currently-available distributions.  If `full_env` is not
++        supplied, one is created automatically from the ``WorkingSet`` this
++        method is called on, which will typically mean that every directory on
++        ``sys.path`` will be scanned for distributions.
++
++        `installer` is a standard installer callback as used by the
++        ``resolve()`` method. The `fallback` flag indicates whether we should
++        attempt to resolve older versions of a plugin if the newest version
++        cannot be resolved.
++
++        This method returns a 2-tuple: (`distributions`, `error_info`), where
++        `distributions` is a list of the distributions found in `plugin_env`
++        that were loadable, along with any other distributions that are needed
++        to resolve their dependencies.  `error_info` is a dictionary mapping
++        unloadable plugin distributions to an exception instance describing the
++        error that occurred. Usually this will be a ``DistributionNotFound`` or
++        ``VersionConflict`` instance.
++        """
++
++        plugin_projects = list(plugin_env)
++        # scan project names in alphabetic order
++        plugin_projects.sort()
++
++        error_info = {}
++        distributions = {}
++
++        if full_env is None:
++            env = Environment(self.entries)
++            env += plugin_env
++        else:
++            env = full_env + plugin_env
++
++        shadow_set = self.__class__([])
++        # put all our entries in shadow_set
++        list(map(shadow_set.add, self))
++
++        for project_name in plugin_projects:
++
++            for dist in plugin_env[project_name]:
++
++                req = [dist.as_requirement()]
++
++                try:
++                    resolvees = shadow_set.resolve(req, env, installer)
++
++                except ResolutionError as v:
++                    # save error info
++                    error_info[dist] = v
++                    if fallback:
++                        # try the next older version of project
++                        continue
++                    else:
++                        # give up on this project, keep going
++                        break
++
++                else:
++                    list(map(shadow_set.add, resolvees))
++                    distributions.update(dict.fromkeys(resolvees))
++
++                    # success, no need to try any more versions of this project
++                    break
++
++        distributions = list(distributions)
++        distributions.sort()
++
++        return distributions, error_info
++
++    def require(self, *requirements):
++        """Ensure that distributions matching `requirements` are activated
++
++        `requirements` must be a string or a (possibly-nested) sequence
++        thereof, specifying the distributions and versions required.  The
++        return value is a sequence of the distributions that needed to be
++        activated to fulfill the requirements; all relevant distributions are
++        included, even if they were already activated in this working set.
++        """
++        needed = self.resolve(parse_requirements(requirements))
++
++        for dist in needed:
++            self.add(dist)
++
++        return needed
++
++    def subscribe(self, callback, existing=True):
++        """Invoke `callback` for all distributions
++
++        If `existing=True` (default),
++        call on all existing ones, as well.
++        """
++        if callback in self.callbacks:
++            return
++        self.callbacks.append(callback)
++        if not existing:
++            return
++        for dist in self:
++            callback(dist)
++
++    def _added_new(self, dist):
++        for callback in self.callbacks:
++            callback(dist)
++
++    def __getstate__(self):
++        return (
++            self.entries[:], self.entry_keys.copy(), self.by_key.copy(),
++            self.callbacks[:]
++        )
++
++    def __setstate__(self, e_k_b_c):
++        entries, keys, by_key, callbacks = e_k_b_c
++        self.entries = entries[:]
++        self.entry_keys = keys.copy()
++        self.by_key = by_key.copy()
++        self.callbacks = callbacks[:]
++
++
++class _ReqExtras(dict):
++    """
++    Map each requirement to the extras that demanded it.
++    """
++
++    def markers_pass(self, req, extras=None):
++        """
++        Evaluate markers for req against each extra that
++        demanded it.
++
++        Return False if the req has a marker and fails
++        evaluation. Otherwise, return True.
++        """
++        extra_evals = (
++            req.marker.evaluate({'extra': extra})
++            for extra in self.get(req, ()) + (extras or (None,))
++        )
++        return not req.marker or any(extra_evals)
++
++
++class Environment(object):
++    """Searchable snapshot of distributions on a search path"""
++
++    def __init__(
++            self, search_path=None, platform=get_supported_platform(),
++            python=PY_MAJOR):
++        """Snapshot distributions available on a search path
++
++        Any distributions found on `search_path` are added to the environment.
++        `search_path` should be a sequence of ``sys.path`` items.  If not
++        supplied, ``sys.path`` is used.
++
++        `platform` is an optional string specifying the name of the platform
++        that platform-specific distributions must be compatible with.  If
++        unspecified, it defaults to the current platform.  `python` is an
++        optional string naming the desired version of Python (e.g. ``'3.3'``);
++        it defaults to the current version.
++
++        You may explicitly set `platform` (and/or `python`) to ``None`` if you
++        wish to map *all* distributions, not just those compatible with the
++        running platform or Python version.
++        """
++        self._distmap = {}
++        self.platform = platform
++        self.python = python
++        self.scan(search_path)
++
++    def can_add(self, dist):
++        """Is distribution `dist` acceptable for this environment?
++
++        The distribution must match the platform and python version
++        requirements specified when this environment was created, or False
++        is returned.
++        """
++        py_compat = (
++            self.python is None
++            or dist.py_version is None
++            or dist.py_version == self.python
++        )
++        return py_compat and compatible_platforms(dist.platform, self.platform)
++
++    def remove(self, dist):
++        """Remove `dist` from the environment"""
++        self._distmap[dist.key].remove(dist)
++
++    def scan(self, search_path=None):
++        """Scan `search_path` for distributions usable in this environment
++
++        Any distributions found are added to the environment.
++        `search_path` should be a sequence of ``sys.path`` items.  If not
++        supplied, ``sys.path`` is used.  Only distributions conforming to
++        the platform/python version defined at initialization are added.
++        """
++        if search_path is None:
++            search_path = sys.path
++
++        for item in search_path:
++            for dist in find_distributions(item):
++                self.add(dist)
++
++    def __getitem__(self, project_name):
++        """Return a newest-to-oldest list of distributions for `project_name`
++
++        Uses case-insensitive `project_name` comparison, assuming all the
++        project's distributions use their project's name converted to all
++        lowercase as their key.
++
++        """
++        distribution_key = project_name.lower()
++        return self._distmap.get(distribution_key, [])
++
++    def add(self, dist):
++        """Add `dist` if we ``can_add()`` it and it has not already been added
++        """
++        if self.can_add(dist) and dist.has_version():
++            dists = self._distmap.setdefault(dist.key, [])
++            if dist not in dists:
++                dists.append(dist)
++                dists.sort(key=operator.attrgetter('hashcmp'), reverse=True)
++
++    def best_match(
++            self, req, working_set, installer=None, replace_conflicting=False):
++        """Find distribution best matching `req` and usable on `working_set`
++
++        This calls the ``find(req)`` method of the `working_set` to see if a
++        suitable distribution is already active.  (This may raise
++        ``VersionConflict`` if an unsuitable version of the project is already
++        active in the specified `working_set`.)  If a suitable distribution
++        isn't active, this method returns the newest distribution in the
++        environment that meets the ``Requirement`` in `req`.  If no suitable
++        distribution is found, and `installer` is supplied, then the result of
++        calling the environment's ``obtain(req, installer)`` method will be
++        returned.
++        """
++        try:
++            dist = working_set.find(req)
++        except VersionConflict:
++            if not replace_conflicting:
++                raise
++            dist = None
++        if dist is not None:
++            return dist
++        for dist in self[req.key]:
++            if dist in req:
++                return dist
++        # try to download/install
++        return self.obtain(req, installer)
++
++    def obtain(self, requirement, installer=None):
++        """Obtain a distribution matching `requirement` (e.g. via download)
++
++        Obtain a distro that matches requirement (e.g. via download).  In the
++        base ``Environment`` class, this routine just returns
++        ``installer(requirement)``, unless `installer` is None, in which case
++        None is returned instead.  This method is a hook that allows subclasses
++        to attempt other ways of obtaining a distribution before falling back
++        to the `installer` argument."""
++        if installer is not None:
++            return installer(requirement)
++
++    def __iter__(self):
++        """Yield the unique project names of the available distributions"""
++        for key in self._distmap.keys():
++            if self[key]:
++                yield key
++
++    def __iadd__(self, other):
++        """In-place addition of a distribution or environment"""
++        if isinstance(other, Distribution):
++            self.add(other)
++        elif isinstance(other, Environment):
++            for project in other:
++                for dist in other[project]:
++                    self.add(dist)
++        else:
++            raise TypeError("Can't add %r to environment" % (other,))
++        return self
++
++    def __add__(self, other):
++        """Add an environment or distribution to an environment"""
++        new = self.__class__([], platform=None, python=None)
++        for env in self, other:
++            new += env
++        return new
++
++
++# XXX backward compatibility
++AvailableDistributions = Environment
++
++
++class ExtractionError(RuntimeError):
++    """An error occurred extracting a resource
++
++    The following attributes are available from instances of this exception:
++
++    manager
++        The resource manager that raised this exception
++
++    cache_path
++        The base directory for resource extraction
++
++    original_error
++        The exception instance that caused extraction to fail
++    """
++
++
++class ResourceManager:
++    """Manage resource extraction and packages"""
++    extraction_path = None
++
++    def __init__(self):
++        self.cached_files = {}
++
++    def resource_exists(self, package_or_requirement, resource_name):
++        """Does the named resource exist?"""
++        return get_provider(package_or_requirement).has_resource(resource_name)
++
++    def resource_isdir(self, package_or_requirement, resource_name):
++        """Is the named resource an existing directory?"""
++        return get_provider(package_or_requirement).resource_isdir(
++            resource_name
++        )
++
++    def resource_filename(self, package_or_requirement, resource_name):
++        """Return a true filesystem path for specified resource"""
++        return get_provider(package_or_requirement).get_resource_filename(
++            self, resource_name
++        )
++
++    def resource_stream(self, package_or_requirement, resource_name):
++        """Return a readable file-like object for specified resource"""
++        return get_provider(package_or_requirement).get_resource_stream(
++            self, resource_name
++        )
++
++    def resource_string(self, package_or_requirement, resource_name):
++        """Return specified resource as a string"""
++        return get_provider(package_or_requirement).get_resource_string(
++            self, resource_name
++        )
++
++    def resource_listdir(self, package_or_requirement, resource_name):
++        """List the contents of the named resource directory"""
++        return get_provider(package_or_requirement).resource_listdir(
++            resource_name
++        )
++
++    def extraction_error(self):
++        """Give an error message for problems extracting file(s)"""
++
++        old_exc = sys.exc_info()[1]
++        cache_path = self.extraction_path or get_default_cache()
++
++        tmpl = textwrap.dedent("""
++            Can't extract file(s) to egg cache
++
++            The following error occurred while trying to extract file(s)
++            to the Python egg cache:
++
++              {old_exc}
++
++            The Python egg cache directory is currently set to:
++
++              {cache_path}
++
++            Perhaps your account does not have write access to this directory?
++            You can change the cache directory by setting the PYTHON_EGG_CACHE
++            environment variable to point to an accessible directory.
++            """).lstrip()
++        err = ExtractionError(tmpl.format(**locals()))
++        err.manager = self
++        err.cache_path = cache_path
++        err.original_error = old_exc
++        raise err
++
++    def get_cache_path(self, archive_name, names=()):
++        """Return absolute location in cache for `archive_name` and `names`
++
++        The parent directory of the resulting path will be created if it does
++        not already exist.  `archive_name` should be the base filename of the
++        enclosing egg (which may not be the name of the enclosing zipfile!),
++        including its ".egg" extension.  `names`, if provided, should be a
++        sequence of path name parts "under" the egg's extraction location.
++
++        This method should only be called by resource providers that need to
++        obtain an extraction location, and only for names they intend to
++        extract, as it tracks the generated names for possible cleanup later.
++        """
++        extract_path = self.extraction_path or get_default_cache()
++        target_path = os.path.join(extract_path, archive_name + '-tmp', *names)
++        try:
++            _bypass_ensure_directory(target_path)
++        except Exception:
++            self.extraction_error()
++
++        self._warn_unsafe_extraction_path(extract_path)
++
++        self.cached_files[target_path] = 1
++        return target_path
++
++    @staticmethod
++    def _warn_unsafe_extraction_path(path):
++        """
++        If the default extraction path is overridden and set to an insecure
++        location, such as /tmp, it opens up an opportunity for an attacker to
++        replace an extracted file with an unauthorized payload. Warn the user
++        if a known insecure location is used.
++
++        See Distribute #375 for more details.
++        """
++        if os.name == 'nt' and not path.startswith(os.environ['windir']):
++            # On Windows, permissions are generally restrictive by default
++            #  and temp directories are not writable by other users, so
++            #  bypass the warning.
++            return
++        mode = os.stat(path).st_mode
++        if mode & stat.S_IWOTH or mode & stat.S_IWGRP:
++            msg = (
++                "%s is writable by group/others and vulnerable to attack "
++                "when "
++                "used with get_resource_filename. Consider a more secure "
++                "location (set with .set_extraction_path or the "
++                "PYTHON_EGG_CACHE environment variable)." % path
++            )
++            warnings.warn(msg, UserWarning)
++
++    def postprocess(self, tempname, filename):
++        """Perform any platform-specific postprocessing of `tempname`
++
++        This is where Mac header rewrites should be done; other platforms don't
++        have anything special they should do.
++
++        Resource providers should call this method ONLY after successfully
++        extracting a compressed resource.  They must NOT call it on resources
++        that are already in the filesystem.
++
++        `tempname` is the current (temporary) name of the file, and `filename`
++        is the name it will be renamed to by the caller after this routine
++        returns.
++        """
++
++        if os.name == 'posix':
++            # Make the resource executable
++            mode = ((os.stat(tempname).st_mode) | 0o555) & 0o7777
++            os.chmod(tempname, mode)
++
++    def set_extraction_path(self, path):
++        """Set the base path where resources will be extracted to, if needed.
++
++        If you do not call this routine before any extractions take place, the
++        path defaults to the return value of ``get_default_cache()``.  (Which
++        is based on the ``PYTHON_EGG_CACHE`` environment variable, with various
++        platform-specific fallbacks.  See that routine's documentation for more
++        details.)
++
++        Resources are extracted to subdirectories of this path based upon
++        information given by the ``IResourceProvider``.  You may set this to a
++        temporary directory, but then you must call ``cleanup_resources()`` to
++        delete the extracted files when done.  There is no guarantee that
++        ``cleanup_resources()`` will be able to remove all extracted files.
++
++        (Note: you may not change the extraction path for a given resource
++        manager once resources have been extracted, unless you first call
++        ``cleanup_resources()``.)
++        """
++        if self.cached_files:
++            raise ValueError(
++                "Can't change extraction path, files already extracted"
++            )
++
++        self.extraction_path = path
++
++    def cleanup_resources(self, force=False):
++        """
++        Delete all extracted resource files and directories, returning a list
++        of the file and directory names that could not be successfully removed.
++        This function does not have any concurrency protection, so it should
++        generally only be called when the extraction path is a temporary
++        directory exclusive to a single process.  This method is not
++        automatically called; you must call it explicitly or register it as an
++        ``atexit`` function if you wish to ensure cleanup of a temporary
++        directory used for extractions.
++        """
++        # XXX
++
++
++def get_default_cache():
++    """
++    Return the ``PYTHON_EGG_CACHE`` environment variable
++    or a platform-relevant user cache dir for an app
++    named "Python-Eggs".
++    """
++    return (
++        os.environ.get('PYTHON_EGG_CACHE')
++        or appdirs.user_cache_dir(appname='Python-Eggs')
++    )
++
++
++def safe_name(name):
++    """Convert an arbitrary string to a standard distribution name
++
++    Any runs of non-alphanumeric/. characters are replaced with a single '-'.
++    """
++    return re.sub('[^A-Za-z0-9.]+', '-', name)
++
++
++def safe_version(version):
++    """
++    Convert an arbitrary string to a standard version string
++    """
++    try:
++        # normalize the version
++        return str(packaging.version.Version(version))
++    except packaging.version.InvalidVersion:
++        version = version.replace(' ', '.')
++        return re.sub('[^A-Za-z0-9.]+', '-', version)
++
++
++def safe_extra(extra):
++    """Convert an arbitrary string to a standard 'extra' name
++
++    Any runs of non-alphanumeric characters are replaced with a single '_',
++    and the result is always lowercased.
++    """
++    return re.sub('[^A-Za-z0-9.-]+', '_', extra).lower()
++
++
++def to_filename(name):
++    """Convert a project or version name to its filename-escaped form
++
++    Any '-' characters are currently replaced with '_'.
++    """
++    return name.replace('-', '_')
++
++
++def invalid_marker(text):
++    """
++    Validate text as a PEP 508 environment marker; return an exception
++    if invalid or False otherwise.
++    """
++    try:
++        evaluate_marker(text)
++    except SyntaxError as e:
++        e.filename = None
++        e.lineno = None
++        return e
++    return False
++
++
++def evaluate_marker(text, extra=None):
++    """
++    Evaluate a PEP 508 environment marker.
++    Return a boolean indicating the marker result in this environment.
++    Raise SyntaxError if marker is invalid.
++
++    This implementation uses the 'pyparsing' module.
++    """
++    try:
++        marker = packaging.markers.Marker(text)
++        return marker.evaluate()
++    except packaging.markers.InvalidMarker as e:
++        raise SyntaxError(e)
++
++
++class NullProvider:
++    """Try to implement resources and metadata for arbitrary PEP 302 loaders"""
++
++    egg_name = None
++    egg_info = None
++    loader = None
++
++    def __init__(self, module):
++        self.loader = getattr(module, '__loader__', None)
++        self.module_path = os.path.dirname(getattr(module, '__file__', ''))
++
++    def get_resource_filename(self, manager, resource_name):
++        return self._fn(self.module_path, resource_name)
++
++    def get_resource_stream(self, manager, resource_name):
++        return io.BytesIO(self.get_resource_string(manager, resource_name))
++
++    def get_resource_string(self, manager, resource_name):
++        return self._get(self._fn(self.module_path, resource_name))
++
++    def has_resource(self, resource_name):
++        return self._has(self._fn(self.module_path, resource_name))
++
++    def has_metadata(self, name):
++        return self.egg_info and self._has(self._fn(self.egg_info, name))
++
++    def get_metadata(self, name):
++        if not self.egg_info:
++            return ""
++        value = self._get(self._fn(self.egg_info, name))
++        return value.decode('utf-8') if six.PY3 else value
++
++    def get_metadata_lines(self, name):
++        return yield_lines(self.get_metadata(name))
++
++    def resource_isdir(self, resource_name):
++        return self._isdir(self._fn(self.module_path, resource_name))
++
++    def metadata_isdir(self, name):
++        return self.egg_info and self._isdir(self._fn(self.egg_info, name))
++
++    def resource_listdir(self, resource_name):
++        return self._listdir(self._fn(self.module_path, resource_name))
++
++    def metadata_listdir(self, name):
++        if self.egg_info:
++            return self._listdir(self._fn(self.egg_info, name))
++        return []
++
++    def run_script(self, script_name, namespace):
++        script = 'scripts/' + script_name
++        if not self.has_metadata(script):
++            raise ResolutionError(
++                "Script {script!r} not found in metadata at {self.egg_info!r}"
++                .format(**locals()),
++            )
++        script_text = self.get_metadata(script).replace('\r\n', '\n')
++        script_text = script_text.replace('\r', '\n')
++        script_filename = self._fn(self.egg_info, script)
++        namespace['__file__'] = script_filename
++        if os.path.exists(script_filename):
++            source = open(script_filename).read()
++            code = compile(source, script_filename, 'exec')
++            exec(code, namespace, namespace)
++        else:
++            from linecache import cache
++            cache[script_filename] = (
++                len(script_text), 0, script_text.split('\n'), script_filename
++            )
++            script_code = compile(script_text, script_filename, 'exec')
++            exec(script_code, namespace, namespace)
++
++    def _has(self, path):
++        raise NotImplementedError(
++            "Can't perform this operation for unregistered loader type"
++        )
++
++    def _isdir(self, path):
++        raise NotImplementedError(
++            "Can't perform this operation for unregistered loader type"
++        )
++
++    def _listdir(self, path):
++        raise NotImplementedError(
++            "Can't perform this operation for unregistered loader type"
++        )
++
++    def _fn(self, base, resource_name):
++        if resource_name:
++            return os.path.join(base, *resource_name.split('/'))
++        return base
++
++    def _get(self, path):
++        if hasattr(self.loader, 'get_data'):
++            return self.loader.get_data(path)
++        raise NotImplementedError(
++            "Can't perform this operation for loaders without 'get_data()'"
++        )
++
++
++register_loader_type(object, NullProvider)
++
++
++class EggProvider(NullProvider):
++    """Provider based on a virtual filesystem"""
++
++    def __init__(self, module):
++        NullProvider.__init__(self, module)
++        self._setup_prefix()
++
++    def _setup_prefix(self):
++        # we assume here that our metadata may be nested inside a "basket"
++        # of multiple eggs; that's why we use module_path instead of .archive
++        path = self.module_path
++        old = None
++        while path != old:
++            if _is_egg_path(path):
++                self.egg_name = os.path.basename(path)
++                self.egg_info = os.path.join(path, 'EGG-INFO')
++                self.egg_root = path
++                break
++            old = path
++            path, base = os.path.split(path)
++
++
++class DefaultProvider(EggProvider):
++    """Provides access to package resources in the filesystem"""
++
++    def _has(self, path):
++        return os.path.exists(path)
++
++    def _isdir(self, path):
++        return os.path.isdir(path)
++
++    def _listdir(self, path):
++        return os.listdir(path)
++
++    def get_resource_stream(self, manager, resource_name):
++        return open(self._fn(self.module_path, resource_name), 'rb')
++
++    def _get(self, path):
++        with open(path, 'rb') as stream:
++            return stream.read()
++
++    @classmethod
++    def _register(cls):
++        loader_cls = getattr(
++            importlib_machinery,
++            'SourceFileLoader',
++            type(None),
++        )
++        register_loader_type(loader_cls, cls)
++
++
++DefaultProvider._register()
++
++
++class EmptyProvider(NullProvider):
++    """Provider that returns nothing for all requests"""
++
++    module_path = None
++
++    _isdir = _has = lambda self, path: False
++
++    def _get(self, path):
++        return ''
++
++    def _listdir(self, path):
++        return []
++
++    def __init__(self):
++        pass
++
++
++empty_provider = EmptyProvider()
++
++
++class ZipManifests(dict):
++    """
++    zip manifest builder
++    """
++
++    @classmethod
++    def build(cls, path):
++        """
++        Build a dictionary similar to the zipimport directory
++        caches, except instead of tuples, store ZipInfo objects.
++
++        Use a platform-specific path separator (os.sep) for the path keys
++        for compatibility with pypy on Windows.
++        """
++        with zipfile.ZipFile(path) as zfile:
++            items = (
++                (
++                    name.replace('/', os.sep),
++                    zfile.getinfo(name),
++                )
++                for name in zfile.namelist()
++            )
++            return dict(items)
++
++    load = build
++
++
++class MemoizedZipManifests(ZipManifests):
++    """
++    Memoized zipfile manifests.
++    """
++    manifest_mod = collections.namedtuple('manifest_mod', 'manifest mtime')
++
++    def load(self, path):
++        """
++        Load a manifest at path or return a suitable manifest already loaded.
++        """
++        path = os.path.normpath(path)
++        mtime = os.stat(path).st_mtime
++
++        if path not in self or self[path].mtime != mtime:
++            manifest = self.build(path)
++            self[path] = self.manifest_mod(manifest, mtime)
++
++        return self[path].manifest
++
++
++class ZipProvider(EggProvider):
++    """Resource support for zips and eggs"""
++
++    eagers = None
++    _zip_manifests = MemoizedZipManifests()
++
++    def __init__(self, module):
++        EggProvider.__init__(self, module)
++        self.zip_pre = self.loader.archive + os.sep
++
++    def _zipinfo_name(self, fspath):
++        # Convert a virtual filename (full path to file) into a zipfile subpath
++        # usable with the zipimport directory cache for our target archive
++        fspath = fspath.rstrip(os.sep)
++        if fspath == self.loader.archive:
++            return ''
++        if fspath.startswith(self.zip_pre):
++            return fspath[len(self.zip_pre):]
++        raise AssertionError(
++            "%s is not a subpath of %s" % (fspath, self.zip_pre)
++        )
++
++    def _parts(self, zip_path):
++        # Convert a zipfile subpath into an egg-relative path part list.
++        # pseudo-fs path
++        fspath = self.zip_pre + zip_path
++        if fspath.startswith(self.egg_root + os.sep):
++            return fspath[len(self.egg_root) + 1:].split(os.sep)
++        raise AssertionError(
++            "%s is not a subpath of %s" % (fspath, self.egg_root)
++        )
++
++    @property
++    def zipinfo(self):
++        return self._zip_manifests.load(self.loader.archive)
++
++    def get_resource_filename(self, manager, resource_name):
++        if not self.egg_name:
++            raise NotImplementedError(
++                "resource_filename() only supported for .egg, not .zip"
++            )
++        # no need to lock for extraction, since we use temp names
++        zip_path = self._resource_to_zip(resource_name)
++        eagers = self._get_eager_resources()
++        if '/'.join(self._parts(zip_path)) in eagers:
++            for name in eagers:
++                self._extract_resource(manager, self._eager_to_zip(name))
++        return self._extract_resource(manager, zip_path)
++
++    @staticmethod
++    def _get_date_and_size(zip_stat):
++        size = zip_stat.file_size
++        # ymdhms+wday, yday, dst
++        date_time = zip_stat.date_time + (0, 0, -1)
++        # 1980 offset already done
++        timestamp = time.mktime(date_time)
++        return timestamp, size
++
++    def _extract_resource(self, manager, zip_path):
++
++        if zip_path in self._index():
++            for name in self._index()[zip_path]:
++                last = self._extract_resource(
++                    manager, os.path.join(zip_path, name)
++                )
++            # return the extracted directory name
++            return os.path.dirname(last)
++
++        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])
++
++        if not WRITE_SUPPORT:
++            raise IOError('"os.rename" and "os.unlink" are not supported '
++                          'on this platform')
++        try:
++
++            real_path = manager.get_cache_path(
++                self.egg_name, self._parts(zip_path)
++            )
++
++            if self._is_current(real_path, zip_path):
++                return real_path
++
++            outf, tmpnam = _mkstemp(
++                ".$extract",
++                dir=os.path.dirname(real_path),
++            )
++            os.write(outf, self.loader.get_data(zip_path))
++            os.close(outf)
++            utime(tmpnam, (timestamp, timestamp))
++            manager.postprocess(tmpnam, real_path)
++
++            try:
++                rename(tmpnam, real_path)
++
++            except os.error:
++                if os.path.isfile(real_path):
++                    if self._is_current(real_path, zip_path):
++                        # the file became current since it was checked above,
++                        #  so proceed.
++                        return real_path
++                    # Windows, del old file and retry
++                    elif os.name == 'nt':
++                        unlink(real_path)
++                        rename(tmpnam, real_path)
++                        return real_path
++                raise
++
++        except os.error:
++            # report a user-friendly error
++            manager.extraction_error()
++
++        return real_path
++
++    def _is_current(self, file_path, zip_path):
++        """
++        Return True if the file_path is current for this zip_path
++        """
++        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])
++        if not os.path.isfile(file_path):
++            return False
++        stat = os.stat(file_path)
++        if stat.st_size != size or stat.st_mtime != timestamp:
++            return False
++        # check that the contents match
++        zip_contents = self.loader.get_data(zip_path)
++        with open(file_path, 'rb') as f:
++            file_contents = f.read()
++        return zip_contents == file_contents
++
++    def _get_eager_resources(self):
++        if self.eagers is None:
++            eagers = []
++            for name in ('native_libs.txt', 'eager_resources.txt'):
++                if self.has_metadata(name):
++                    eagers.extend(self.get_metadata_lines(name))
++            self.eagers = eagers
++        return self.eagers
++
++    def _index(self):
++        try:
++            return self._dirindex
++        except AttributeError:
++            ind = {}
++            for path in self.zipinfo:
++                parts = path.split(os.sep)
++                while parts:
++                    parent = os.sep.join(parts[:-1])
++                    if parent in ind:
++                        ind[parent].append(parts[-1])
++                        break
++                    else:
++                        ind[parent] = [parts.pop()]
++            self._dirindex = ind
++            return ind
++
++    def _has(self, fspath):
++        zip_path = self._zipinfo_name(fspath)
++        return zip_path in self.zipinfo or zip_path in self._index()
++
++    def _isdir(self, fspath):
++        return self._zipinfo_name(fspath) in self._index()
++
++    def _listdir(self, fspath):
++        return list(self._index().get(self._zipinfo_name(fspath), ()))
++
++    def _eager_to_zip(self, resource_name):
++        return self._zipinfo_name(self._fn(self.egg_root, resource_name))
++
++    def _resource_to_zip(self, resource_name):
++        return self._zipinfo_name(self._fn(self.module_path, resource_name))
++
++
++register_loader_type(zipimport.zipimporter, ZipProvider)
++
++
++class FileMetadata(EmptyProvider):
++    """Metadata handler for standalone PKG-INFO files
++
++    Usage::
++
++        metadata = FileMetadata("/path/to/PKG-INFO")
++
++    This provider rejects all data and metadata requests except for PKG-INFO,
++    which is treated as existing, and will be the contents of the file at
++    the provided location.
++    """
++
++    def __init__(self, path):
++        self.path = path
++
++    def has_metadata(self, name):
++        return name == 'PKG-INFO' and os.path.isfile(self.path)
++
++    def get_metadata(self, name):
++        if name != 'PKG-INFO':
++            raise KeyError("No metadata except PKG-INFO is available")
++
++        with io.open(self.path, encoding='utf-8', errors="replace") as f:
++            metadata = f.read()
++        self._warn_on_replacement(metadata)
++        return metadata
++
++    def _warn_on_replacement(self, metadata):
++        # Python 2.7 compat for: replacement_char = '�'
++        replacement_char = b'\xef\xbf\xbd'.decode('utf-8')
++        if replacement_char in metadata:
++            tmpl = "{self.path} could not be properly decoded in UTF-8"
++            msg = tmpl.format(**locals())
++            warnings.warn(msg)
++
++    def get_metadata_lines(self, name):
++        return yield_lines(self.get_metadata(name))
++
++
++class PathMetadata(DefaultProvider):
++    """Metadata provider for egg directories
++
++    Usage::
++
++        # Development eggs:
++
++        egg_info = "/path/to/PackageName.egg-info"
++        base_dir = os.path.dirname(egg_info)
++        metadata = PathMetadata(base_dir, egg_info)
++        dist_name = os.path.splitext(os.path.basename(egg_info))[0]
++        dist = Distribution(basedir, project_name=dist_name, metadata=metadata)
++
++        # Unpacked egg directories:
++
++        egg_path = "/path/to/PackageName-ver-pyver-etc.egg"
++        metadata = PathMetadata(egg_path, os.path.join(egg_path,'EGG-INFO'))
++        dist = Distribution.from_filename(egg_path, metadata=metadata)
++    """
++
++    def __init__(self, path, egg_info):
++        self.module_path = path
++        self.egg_info = egg_info
++
++
++class EggMetadata(ZipProvider):
++    """Metadata provider for .egg files"""
++
++    def __init__(self, importer):
++        """Create a metadata provider from a zipimporter"""
++
++        self.zip_pre = importer.archive + os.sep
++        self.loader = importer
++        if importer.prefix:
++            self.module_path = os.path.join(importer.archive, importer.prefix)
++        else:
++            self.module_path = importer.archive
++        self._setup_prefix()
++
++
++_declare_state('dict', _distribution_finders={})
++
++
++def register_finder(importer_type, distribution_finder):
++    """Register `distribution_finder` to find distributions in sys.path items
++
++    `importer_type` is the type or class of a PEP 302 "Importer" (sys.path item
++    handler), and `distribution_finder` is a callable that, passed a path
++    item and the importer instance, yields ``Distribution`` instances found on
++    that path item.  See ``pkg_resources.find_on_path`` for an example."""
++    _distribution_finders[importer_type] = distribution_finder
++
++
++def find_distributions(path_item, only=False):
++    """Yield distributions accessible via `path_item`"""
++    importer = get_importer(path_item)
++    finder = _find_adapter(_distribution_finders, importer)
++    return finder(importer, path_item, only)
++
++
++def find_eggs_in_zip(importer, path_item, only=False):
++    """
++    Find eggs in zip files; possibly multiple nested eggs.
++    """
++    if importer.archive.endswith('.whl'):
++        # wheels are not supported with this finder
++        # they don't have PKG-INFO metadata, and won't ever contain eggs
++        return
++    metadata = EggMetadata(importer)
++    if metadata.has_metadata('PKG-INFO'):
++        yield Distribution.from_filename(path_item, metadata=metadata)
++    if only:
++        # don't yield nested distros
++        return
++    for subitem in metadata.resource_listdir('/'):
++        if _is_egg_path(subitem):
++            subpath = os.path.join(path_item, subitem)
++            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)
++            for dist in dists:
++                yield dist
++        elif subitem.lower().endswith('.dist-info'):
++            subpath = os.path.join(path_item, subitem)
++            submeta = EggMetadata(zipimport.zipimporter(subpath))
++            submeta.egg_info = subpath
++            yield Distribution.from_location(path_item, subitem, submeta)
++
++
++register_finder(zipimport.zipimporter, find_eggs_in_zip)
++
++
++def find_nothing(importer, path_item, only=False):
++    return ()
++
++
++register_finder(object, find_nothing)
++
++
++def _by_version_descending(names):
++    """
++    Given a list of filenames, return them in descending order
++    by version number.
++
++    >>> names = 'bar', 'foo', 'Python-2.7.10.egg', 'Python-2.7.2.egg'
++    >>> _by_version_descending(names)
++    ['Python-2.7.10.egg', 'Python-2.7.2.egg', 'foo', 'bar']
++    >>> names = 'Setuptools-1.2.3b1.egg', 'Setuptools-1.2.3.egg'
++    >>> _by_version_descending(names)
++    ['Setuptools-1.2.3.egg', 'Setuptools-1.2.3b1.egg']
++    >>> names = 'Setuptools-1.2.3b1.egg', 'Setuptools-1.2.3.post1.egg'
++    >>> _by_version_descending(names)
++    ['Setuptools-1.2.3.post1.egg', 'Setuptools-1.2.3b1.egg']
++    """
++    def _by_version(name):
++        """
++        Parse each component of the filename
++        """
++        name, ext = os.path.splitext(name)
++        parts = itertools.chain(name.split('-'), [ext])
++        return [packaging.version.parse(part) for part in parts]
++
++    return sorted(names, key=_by_version, reverse=True)
++
++
++def find_on_path(importer, path_item, only=False):
++    """Yield distributions accessible on a sys.path directory"""
++    path_item = _normalize_cached(path_item)
++
++    if _is_unpacked_egg(path_item):
++        yield Distribution.from_filename(
++            path_item, metadata=PathMetadata(
++                path_item, os.path.join(path_item, 'EGG-INFO')
++            )
++        )
++        return
++
++    entries = safe_listdir(path_item)
++
++    # for performance, before sorting by version,
++    # screen entries for only those that will yield
++    # distributions
++    filtered = (
++        entry
++        for entry in entries
++        if dist_factory(path_item, entry, only)
++    )
++
++    # scan for .egg and .egg-info in directory
++    path_item_entries = _by_version_descending(filtered)
++    for entry in path_item_entries:
++        fullpath = os.path.join(path_item, entry)
++        factory = dist_factory(path_item, entry, only)
++        for dist in factory(fullpath):
++            yield dist
++
++
++def dist_factory(path_item, entry, only):
++    """
++    Return a dist_factory for a path_item and entry
++    """
++    lower = entry.lower()
++    is_meta = any(map(lower.endswith, ('.egg-info', '.dist-info')))
++    return (
++        distributions_from_metadata
++        if is_meta else
++        find_distributions
++        if not only and _is_egg_path(entry) else
++        resolve_egg_link
++        if not only and lower.endswith('.egg-link') else
++        NoDists()
++    )
++
++
++class NoDists:
++    """
++    >>> bool(NoDists())
++    False
++
++    >>> list(NoDists()('anything'))
++    []
++    """
++    def __bool__(self):
++        return False
++    if six.PY2:
++        __nonzero__ = __bool__
++
++    def __call__(self, fullpath):
++        return iter(())
++
++
++def safe_listdir(path):
++    """
++    Attempt to list contents of path, but suppress some exceptions.
++    """
++    try:
++        return os.listdir(path)
++    except (PermissionError, NotADirectoryError):
++        pass
++    except OSError as e:
++        # Ignore the directory if does not exist, not a directory or
++        # permission denied
++        ignorable = (
++            e.errno in (errno.ENOTDIR, errno.EACCES, errno.ENOENT)
++            # Python 2 on Windows needs to be handled this way :(
++            or getattr(e, "winerror", None) == 267
++        )
++        if not ignorable:
++            raise
++    return ()
++
++
++def distributions_from_metadata(path):
++    root = os.path.dirname(path)
++    if os.path.isdir(path):
++        if len(os.listdir(path)) == 0:
++            # empty metadata dir; skip
++            return
++        metadata = PathMetadata(root, path)
++    else:
++        metadata = FileMetadata(path)
++    entry = os.path.basename(path)
++    yield Distribution.from_location(
++        root, entry, metadata, precedence=DEVELOP_DIST,
++    )
++
++
++def non_empty_lines(path):
++    """
++    Yield non-empty lines from file at path
++    """
++    with open(path) as f:
++        for line in f:
++            line = line.strip()
++            if line:
++                yield line
++
++
++def resolve_egg_link(path):
++    """
++    Given a path to an .egg-link, resolve distributions
++    present in the referenced path.
++    """
++    referenced_paths = non_empty_lines(path)
++    resolved_paths = (
++        os.path.join(os.path.dirname(path), ref)
++        for ref in referenced_paths
++    )
++    dist_groups = map(find_distributions, resolved_paths)
++    return next(dist_groups, ())
++
++
++register_finder(pkgutil.ImpImporter, find_on_path)
++
++if hasattr(importlib_machinery, 'FileFinder'):
++    register_finder(importlib_machinery.FileFinder, find_on_path)
++
++_declare_state('dict', _namespace_handlers={})
++_declare_state('dict', _namespace_packages={})
++
++
++def register_namespace_handler(importer_type, namespace_handler):
++    """Register `namespace_handler` to declare namespace packages
++
++    `importer_type` is the type or class of a PEP 302 "Importer" (sys.path item
++    handler), and `namespace_handler` is a callable like this::
++
++        def namespace_handler(importer, path_entry, moduleName, module):
++            # return a path_entry to use for child packages
++
++    Namespace handlers are only called if the importer object has already
++    agreed that it can handle the relevant path item, and they should only
++    return a subpath if the module __path__ does not already contain an
++    equivalent subpath.  For an example namespace handler, see
++    ``pkg_resources.file_ns_handler``.
++    """
++    _namespace_handlers[importer_type] = namespace_handler
++
++
++def _handle_ns(packageName, path_item):
++    """Ensure that named package includes a subpath of path_item (if needed)"""
++
++    importer = get_importer(path_item)
++    if importer is None:
++        return None
++    loader = importer.find_module(packageName)
++    if loader is None:
++        return None
++    module = sys.modules.get(packageName)
++    if module is None:
++        module = sys.modules[packageName] = types.ModuleType(packageName)
++        module.__path__ = []
++        _set_parent_ns(packageName)
++    elif not hasattr(module, '__path__'):
++        raise TypeError("Not a package:", packageName)
++    handler = _find_adapter(_namespace_handlers, importer)
++    subpath = handler(importer, path_item, packageName, module)
++    if subpath is not None:
++        path = module.__path__
++        path.append(subpath)
++        loader.load_module(packageName)
++        _rebuild_mod_path(path, packageName, module)
++    return subpath
++
++
++def _rebuild_mod_path(orig_path, package_name, module):
++    """
++    Rebuild module.__path__ ensuring that all entries are ordered
++    corresponding to their sys.path order
++    """
++    sys_path = [_normalize_cached(p) for p in sys.path]
++
++    def safe_sys_path_index(entry):
++        """
++        Workaround for #520 and #513.
++        """
++        try:
++            return sys_path.index(entry)
++        except ValueError:
++            return float('inf')
++
++    def position_in_sys_path(path):
++        """
++        Return the ordinal of the path based on its position in sys.path
++        """
++        path_parts = path.split(os.sep)
++        module_parts = package_name.count('.') + 1
++        parts = path_parts[:-module_parts]
++        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))
++
++    if not isinstance(orig_path, list):
++        # Is this behavior useful when module.__path__ is not a list?
++        return
++
++    orig_path.sort(key=position_in_sys_path)
++    module.__path__[:] = [_normalize_cached(p) for p in orig_path]
++
++
++def declare_namespace(packageName):
++    """Declare that package 'packageName' is a namespace package"""
++
++    _imp.acquire_lock()
++    try:
++        if packageName in _namespace_packages:
++            return
++
++        path, parent = sys.path, None
++        if '.' in packageName:
++            parent = '.'.join(packageName.split('.')[:-1])
++            declare_namespace(parent)
++            if parent not in _namespace_packages:
++                __import__(parent)
++            try:
++                path = sys.modules[parent].__path__
++            except AttributeError:
++                raise TypeError("Not a package:", parent)
++
++        # Track what packages are namespaces, so when new path items are added,
++        # they can be updated
++        _namespace_packages.setdefault(parent, []).append(packageName)
++        _namespace_packages.setdefault(packageName, [])
++
++        for path_item in path:
++            # Ensure all the parent's path items are reflected in the child,
++            # if they apply
++            _handle_ns(packageName, path_item)
++
++    finally:
++        _imp.release_lock()
++
++
++def fixup_namespace_packages(path_item, parent=None):
++    """Ensure that previously-declared namespace packages include path_item"""
++    _imp.acquire_lock()
++    try:
++        for package in _namespace_packages.get(parent, ()):
++            subpath = _handle_ns(package, path_item)
++            if subpath:
++                fixup_namespace_packages(subpath, package)
++    finally:
++        _imp.release_lock()
++
++
++def file_ns_handler(importer, path_item, packageName, module):
++    """Compute an ns-package subpath for a filesystem or zipfile importer"""
++
++    subpath = os.path.join(path_item, packageName.split('.')[-1])
++    normalized = _normalize_cached(subpath)
++    for item in module.__path__:
++        if _normalize_cached(item) == normalized:
++            break
++    else:
++        # Only return the path if it's not already there
++        return subpath
++
++
++register_namespace_handler(pkgutil.ImpImporter, file_ns_handler)
++register_namespace_handler(zipimport.zipimporter, file_ns_handler)
++
++if hasattr(importlib_machinery, 'FileFinder'):
++    register_namespace_handler(importlib_machinery.FileFinder, file_ns_handler)
++
++
++def null_ns_handler(importer, path_item, packageName, module):
++    return None
++
++
++register_namespace_handler(object, null_ns_handler)
++
++
++def normalize_path(filename):
++    """Normalize a file/dir name for comparison purposes"""
++    return os.path.normcase(os.path.realpath(filename))
++
++
++def _normalize_cached(filename, _cache={}):
++    try:
++        return _cache[filename]
++    except KeyError:
++        _cache[filename] = result = normalize_path(filename)
++        return result
++
++
++def _is_egg_path(path):
++    """
++    Determine if given path appears to be an egg.
++    """
++    return path.lower().endswith('.egg')
++
++
++def _is_unpacked_egg(path):
++    """
++    Determine if given path appears to be an unpacked egg.
++    """
++    return (
++        _is_egg_path(path) and
++        os.path.isfile(os.path.join(path, 'EGG-INFO', 'PKG-INFO'))
++    )
++
++
++def _set_parent_ns(packageName):
++    parts = packageName.split('.')
++    name = parts.pop()
++    if parts:
++        parent = '.'.join(parts)
++        setattr(sys.modules[parent], name, sys.modules[packageName])
++
++
++def yield_lines(strs):
++    """Yield non-empty/non-comment lines of a string or sequence"""
++    if isinstance(strs, six.string_types):
++        for s in strs.splitlines():
++            s = s.strip()
++            # skip blank lines/comments
++            if s and not s.startswith('#'):
++                yield s
++    else:
++        for ss in strs:
++            for s in yield_lines(ss):
++                yield s
++
++
++MODULE = re.compile(r"\w+(\.\w+)*$").match
++EGG_NAME = re.compile(
++    r"""
++    (?P<name>[^-]+) (
++        -(?P<ver>[^-]+) (
++            -py(?P<pyver>[^-]+) (
++                -(?P<plat>.+)
++            )?
++        )?
++    )?
++    """,
++    re.VERBOSE | re.IGNORECASE,
++).match
++
++
++class EntryPoint(object):
++    """Object representing an advertised importable object"""
++
++    def __init__(self, name, module_name, attrs=(), extras=(), dist=None):
++        if not MODULE(module_name):
++            raise ValueError("Invalid module name", module_name)
++        self.name = name
++        self.module_name = module_name
++        self.attrs = tuple(attrs)
++        self.extras = tuple(extras)
++        self.dist = dist
++
++    def __str__(self):
++        s = "%s = %s" % (self.name, self.module_name)
++        if self.attrs:
++            s += ':' + '.'.join(self.attrs)
++        if self.extras:
++            s += ' [%s]' % ','.join(self.extras)
++        return s
++
++    def __repr__(self):
++        return "EntryPoint.parse(%r)" % str(self)
++
++    def load(self, require=True, *args, **kwargs):
++        """
++        Require packages for this EntryPoint, then resolve it.
++        """
++        if not require or args or kwargs:
++            warnings.warn(
++                "Parameters to load are deprecated.  Call .resolve and "
++                ".require separately.",
++                DeprecationWarning,
++                stacklevel=2,
++            )
++        if require:
++            self.require(*args, **kwargs)
++        return self.resolve()
++
++    def resolve(self):
++        """
++        Resolve the entry point from its module and attrs.
++        """
++        module = __import__(self.module_name, fromlist=['__name__'], level=0)
++        try:
++            return functools.reduce(getattr, self.attrs, module)
++        except AttributeError as exc:
++            raise ImportError(str(exc))
++
++    def require(self, env=None, installer=None):
++        if self.extras and not self.dist:
++            raise UnknownExtra("Can't require() without a distribution", self)
++
++        # Get the requirements for this entry point with all its extras and
++        # then resolve them. We have to pass `extras` along when resolving so
++        # that the working set knows what extras we want. Otherwise, for
++        # dist-info distributions, the working set will assume that the
++        # requirements for that extra are purely optional and skip over them.
++        reqs = self.dist.requires(self.extras)
++        items = working_set.resolve(reqs, env, installer, extras=self.extras)
++        list(map(working_set.add, items))
++
++    pattern = re.compile(
++        r'\s*'
++        r'(?P<name>.+?)\s*'
++        r'=\s*'
++        r'(?P<module>[\w.]+)\s*'
++        r'(:\s*(?P<attr>[\w.]+))?\s*'
++        r'(?P<extras>\[.*\])?\s*$'
++    )
++
++    @classmethod
++    def parse(cls, src, dist=None):
++        """Parse a single entry point from string `src`
++
++        Entry point syntax follows the form::
++
++            name = some.module:some.attr [extra1, extra2]
++
++        The entry name and module name are required, but the ``:attrs`` and
++        ``[extras]`` parts are optional
++        """
++        m = cls.pattern.match(src)
++        if not m:
++            msg = "EntryPoint must be in 'name=module:attrs [extras]' format"
++            raise ValueError(msg, src)
++        res = m.groupdict()
++        extras = cls._parse_extras(res['extras'])
++        attrs = res['attr'].split('.') if res['attr'] else ()
++        return cls(res['name'], res['module'], attrs, extras, dist)
++
++    @classmethod
++    def _parse_extras(cls, extras_spec):
++        if not extras_spec:
++            return ()
++        req = Requirement.parse('x' + extras_spec)
++        if req.specs:
++            raise ValueError()
++        return req.extras
++
++    @classmethod
++    def parse_group(cls, group, lines, dist=None):
++        """Parse an entry point group"""
++        if not MODULE(group):
++            raise ValueError("Invalid group name", group)
++        this = {}
++        for line in yield_lines(lines):
++            ep = cls.parse(line, dist)
++            if ep.name in this:
++                raise ValueError("Duplicate entry point", group, ep.name)
++            this[ep.name] = ep
++        return this
++
++    @classmethod
++    def parse_map(cls, data, dist=None):
++        """Parse a map of entry point groups"""
++        if isinstance(data, dict):
++            data = data.items()
++        else:
++            data = split_sections(data)
++        maps = {}
++        for group, lines in data:
++            if group is None:
++                if not lines:
++                    continue
++                raise ValueError("Entry points must be listed in groups")
++            group = group.strip()
++            if group in maps:
++                raise ValueError("Duplicate group name", group)
++            maps[group] = cls.parse_group(group, lines, dist)
++        return maps
++
++
++def _remove_md5_fragment(location):
++    if not location:
++        return ''
++    parsed = urllib.parse.urlparse(location)
++    if parsed[-1].startswith('md5='):
++        return urllib.parse.urlunparse(parsed[:-1] + ('',))
++    return location
++
++
++def _version_from_file(lines):
++    """
++    Given an iterable of lines from a Metadata file, return
++    the value of the Version field, if present, or None otherwise.
++    """
++    def is_version_line(line):
++        return line.lower().startswith('version:')
++    version_lines = filter(is_version_line, lines)
++    line = next(iter(version_lines), '')
++    _, _, value = line.partition(':')
++    return safe_version(value.strip()) or None
++
++
++class Distribution(object):
++    """Wrap an actual or potential sys.path entry w/metadata"""
++    PKG_INFO = 'PKG-INFO'
++
++    def __init__(
++            self, location=None, metadata=None, project_name=None,
++            version=None, py_version=PY_MAJOR, platform=None,
++            precedence=EGG_DIST):
++        self.project_name = safe_name(project_name or 'Unknown')
++        if version is not None:
++            self._version = safe_version(version)
++        self.py_version = py_version
++        self.platform = platform
++        self.location = location
++        self.precedence = precedence
++        self._provider = metadata or empty_provider
++
++    @classmethod
++    def from_location(cls, location, basename, metadata=None, **kw):
++        project_name, version, py_version, platform = [None] * 4
++        basename, ext = os.path.splitext(basename)
++        if ext.lower() in _distributionImpl:
++            cls = _distributionImpl[ext.lower()]
++
++            match = EGG_NAME(basename)
++            if match:
++                project_name, version, py_version, platform = match.group(
++                    'name', 'ver', 'pyver', 'plat'
++                )
++        return cls(
++            location, metadata, project_name=project_name, version=version,
++            py_version=py_version, platform=platform, **kw
++        )._reload_version()
++
++    def _reload_version(self):
++        return self
++
++    @property
++    def hashcmp(self):
++        return (
++            self.parsed_version,
++            self.precedence,
++            self.key,
++            _remove_md5_fragment(self.location),
++            self.py_version or '',
++            self.platform or '',
++        )
++
++    def __hash__(self):
++        return hash(self.hashcmp)
++
++    def __lt__(self, other):
++        return self.hashcmp < other.hashcmp
++
++    def __le__(self, other):
++        return self.hashcmp <= other.hashcmp
++
++    def __gt__(self, other):
++        return self.hashcmp > other.hashcmp
++
++    def __ge__(self, other):
++        return self.hashcmp >= other.hashcmp
++
++    def __eq__(self, other):
++        if not isinstance(other, self.__class__):
++            # It's not a Distribution, so they are not equal
++            return False
++        return self.hashcmp == other.hashcmp
++
++    def __ne__(self, other):
++        return not self == other
++
++    # These properties have to be lazy so that we don't have to load any
++    # metadata until/unless it's actually needed.  (i.e., some distributions
++    # may not know their name or version without loading PKG-INFO)
++
++    @property
++    def key(self):
++        try:
++            return self._key
++        except AttributeError:
++            self._key = key = self.project_name.lower()
++            return key
++
++    @property
++    def parsed_version(self):
++        if not hasattr(self, "_parsed_version"):
++            self._parsed_version = parse_version(self.version)
++
++        return self._parsed_version
++
++    def _warn_legacy_version(self):
++        LV = packaging.version.LegacyVersion
++        is_legacy = isinstance(self._parsed_version, LV)
++        if not is_legacy:
++            return
++
++        # While an empty version is technically a legacy version and
++        # is not a valid PEP 440 version, it's also unlikely to
++        # actually come from someone and instead it is more likely that
++        # it comes from setuptools attempting to parse a filename and
++        # including it in the list. So for that we'll gate this warning
++        # on if the version is anything at all or not.
++        if not self.version:
++            return
++
++        tmpl = textwrap.dedent("""
++            '{project_name} ({version})' is being parsed as a legacy,
++            non PEP 440,
++            version. You may find odd behavior and sort order.
++            In particular it will be sorted as less than 0.0. It
++            is recommended to migrate to PEP 440 compatible
++            versions.
++            """).strip().replace('\n', ' ')
++
++        warnings.warn(tmpl.format(**vars(self)), PEP440Warning)
++
++    @property
++    def version(self):
++        try:
++            return self._version
++        except AttributeError:
++            version = _version_from_file(self._get_metadata(self.PKG_INFO))
++            if version is None:
++                tmpl = "Missing 'Version:' header and/or %s file"
++                raise ValueError(tmpl % self.PKG_INFO, self)
++            return version
++
++    @property
++    def _dep_map(self):
++        """
++        A map of extra to its list of (direct) requirements
++        for this distribution, including the null extra.
++        """
++        try:
++            return self.__dep_map
++        except AttributeError:
++            self.__dep_map = self._filter_extras(self._build_dep_map())
++        return self.__dep_map
++
++    @staticmethod
++    def _filter_extras(dm):
++        """
++        Given a mapping of extras to dependencies, strip off
++        environment markers and filter out any dependencies
++        not matching the markers.
++        """
++        for extra in list(filter(None, dm)):
++            new_extra = extra
++            reqs = dm.pop(extra)
++            new_extra, _, marker = extra.partition(':')
++            fails_marker = marker and (
++                invalid_marker(marker)
++                or not evaluate_marker(marker)
++            )
++            if fails_marker:
++                reqs = []
++            new_extra = safe_extra(new_extra) or None
++
++            dm.setdefault(new_extra, []).extend(reqs)
++        return dm
++
++    def _build_dep_map(self):
++        dm = {}
++        for name in 'requires.txt', 'depends.txt':
++            for extra, reqs in split_sections(self._get_metadata(name)):
++                dm.setdefault(extra, []).extend(parse_requirements(reqs))
++        return dm
++
++    def requires(self, extras=()):
++        """List of Requirements needed for this distro if `extras` are used"""
++        dm = self._dep_map
++        deps = []
++        deps.extend(dm.get(None, ()))
++        for ext in extras:
++            try:
++                deps.extend(dm[safe_extra(ext)])
++            except KeyError:
++                raise UnknownExtra(
++                    "%s has no such extra feature %r" % (self, ext)
++                )
++        return deps
++
++    def _get_metadata(self, name):
++        if self.has_metadata(name):
++            for line in self.get_metadata_lines(name):
++                yield line
++
++    def activate(self, path=None, replace=False):
++        """Ensure distribution is importable on `path` (default=sys.path)"""
++        if path is None:
++            path = sys.path
++        self.insert_on(path, replace=replace)
++        if path is sys.path:
++            fixup_namespace_packages(self.location)
++            for pkg in self._get_metadata('namespace_packages.txt'):
++                if pkg in sys.modules:
++                    declare_namespace(pkg)
++
++    def egg_name(self):
++        """Return what this distribution's standard .egg filename should be"""
++        filename = "%s-%s-py%s" % (
++            to_filename(self.project_name), to_filename(self.version),
++            self.py_version or PY_MAJOR
++        )
++
++        if self.platform:
++            filename += '-' + self.platform
++        return filename
++
++    def __repr__(self):
++        if self.location:
++            return "%s (%s)" % (self, self.location)
++        else:
++            return str(self)
++
++    def __str__(self):
++        try:
++            version = getattr(self, 'version', None)
++        except ValueError:
++            version = None
++        version = version or "[unknown version]"
++        return "%s %s" % (self.project_name, version)
++
++    def __getattr__(self, attr):
++        """Delegate all unrecognized public attributes to .metadata provider"""
++        if attr.startswith('_'):
++            raise AttributeError(attr)
++        return getattr(self._provider, attr)
++
++    @classmethod
++    def from_filename(cls, filename, metadata=None, **kw):
++        return cls.from_location(
++            _normalize_cached(filename), os.path.basename(filename), metadata,
++            **kw
++        )
++
++    def as_requirement(self):
++        """Return a ``Requirement`` that matches this distribution exactly"""
++        if isinstance(self.parsed_version, packaging.version.Version):
++            spec = "%s==%s" % (self.project_name, self.parsed_version)
++        else:
++            spec = "%s===%s" % (self.project_name, self.parsed_version)
++
++        return Requirement.parse(spec)
++
++    def load_entry_point(self, group, name):
++        """Return the `name` entry point of `group` or raise ImportError"""
++        ep = self.get_entry_info(group, name)
++        if ep is None:
++            raise ImportError("Entry point %r not found" % ((group, name),))
++        return ep.load()
++
++    def get_entry_map(self, group=None):
++        """Return the entry point map for `group`, or the full entry map"""
++        try:
++            ep_map = self._ep_map
++        except AttributeError:
++            ep_map = self._ep_map = EntryPoint.parse_map(
++                self._get_metadata('entry_points.txt'), self
++            )
++        if group is not None:
++            return ep_map.get(group, {})
++        return ep_map
++
++    def get_entry_info(self, group, name):
++        """Return the EntryPoint object for `group`+`name`, or ``None``"""
++        return self.get_entry_map(group).get(name)
++
++    def insert_on(self, path, loc=None, replace=False):
++        """Ensure self.location is on path
++
++        If replace=False (default):
++            - If location is already in path anywhere, do nothing.
++            - Else:
++              - If it's an egg and its parent directory is on path,
++                insert just ahead of the parent.
++              - Else: add to the end of path.
++        If replace=True:
++            - If location is already on path anywhere (not eggs)
++              or higher priority than its parent (eggs)
++              do nothing.
++            - Else:
++              - If it's an egg and its parent directory is on path,
++                insert just ahead of the parent,
++                removing any lower-priority entries.
++              - Else: add it to the front of path.
++        """
++
++        loc = loc or self.location
++        if not loc:
++            return
++
++        nloc = _normalize_cached(loc)
++        bdir = os.path.dirname(nloc)
++        npath = [(p and _normalize_cached(p) or p) for p in path]
++
++        for p, item in enumerate(npath):
++            if item == nloc:
++                if replace:
++                    break
++                else:
++                    # don't modify path (even removing duplicates) if
++                    # found and not replace
++                    return
++            elif item == bdir and self.precedence == EGG_DIST:
++                # if it's an .egg, give it precedence over its directory
++                # UNLESS it's already been added to sys.path and replace=False
++                if (not replace) and nloc in npath[p:]:
++                    return
++                if path is sys.path:
++                    self.check_version_conflict()
++                path.insert(p, loc)
++                npath.insert(p, nloc)
++                break
++        else:
++            if path is sys.path:
++                self.check_version_conflict()
++            if replace:
++                path.insert(0, loc)
++            else:
++                path.append(loc)
++            return
++
++        # p is the spot where we found or inserted loc; now remove duplicates
++        while True:
++            try:
++                np = npath.index(nloc, p + 1)
++            except ValueError:
++                break
++            else:
++                del npath[np], path[np]
++                # ha!
++                p = np
++
++        return
++
++    def check_version_conflict(self):
++        if self.key == 'setuptools':
++            # ignore the inevitable setuptools self-conflicts  :(
++            return
++
++        nsp = dict.fromkeys(self._get_metadata('namespace_packages.txt'))
++        loc = normalize_path(self.location)
++        for modname in self._get_metadata('top_level.txt'):
++            if (modname not in sys.modules or modname in nsp
++                    or modname in _namespace_packages):
++                continue
++            if modname in ('pkg_resources', 'setuptools', 'site'):
++                continue
++            fn = getattr(sys.modules[modname], '__file__', None)
++            if fn and (normalize_path(fn).startswith(loc) or
++                       fn.startswith(self.location)):
++                continue
++            issue_warning(
++                "Module %s was already imported from %s, but %s is being added"
++                " to sys.path" % (modname, fn, self.location),
++            )
++
++    def has_version(self):
++        try:
++            self.version
++        except ValueError:
++            issue_warning("Unbuilt egg for " + repr(self))
++            return False
++        return True
++
++    def clone(self, **kw):
++        """Copy this distribution, substituting in any changed keyword args"""
++        names = 'project_name version py_version platform location precedence'
++        for attr in names.split():
++            kw.setdefault(attr, getattr(self, attr, None))
++        kw.setdefault('metadata', self._provider)
++        return self.__class__(**kw)
++
++    @property
++    def extras(self):
++        return [dep for dep in self._dep_map if dep]
++
++
++class EggInfoDistribution(Distribution):
++    def _reload_version(self):
++        """
++        Packages installed by distutils (e.g. numpy or scipy),
++        which uses an old safe_version, and so
++        their version numbers can get mangled when
++        converted to filenames (e.g., 1.11.0.dev0+2329eae to
++        1.11.0.dev0_2329eae). These distributions will not be
++        parsed properly
++        downstream by Distribution and safe_version, so
++        take an extra step and try to get the version number from
++        the metadata file itself instead of the filename.
++        """
++        md_version = _version_from_file(self._get_metadata(self.PKG_INFO))
++        if md_version:
++            self._version = md_version
++        return self
++
++
++class DistInfoDistribution(Distribution):
++    """
++    Wrap an actual or potential sys.path entry
++    w/metadata, .dist-info style.
++    """
++    PKG_INFO = 'METADATA'
++    EQEQ = re.compile(r"([\(,])\s*(\d.*?)\s*([,\)])")
++
++    @property
++    def _parsed_pkg_info(self):
++        """Parse and cache metadata"""
++        try:
++            return self._pkg_info
++        except AttributeError:
++            metadata = self.get_metadata(self.PKG_INFO)
++            self._pkg_info = email.parser.Parser().parsestr(metadata)
++            return self._pkg_info
++
++    @property
++    def _dep_map(self):
++        try:
++            return self.__dep_map
++        except AttributeError:
++            self.__dep_map = self._compute_dependencies()
++            return self.__dep_map
++
++    def _compute_dependencies(self):
++        """Recompute this distribution's dependencies."""
++        dm = self.__dep_map = {None: []}
++
++        reqs = []
++        # Including any condition expressions
++        for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:
++            reqs.extend(parse_requirements(req))
++
++        def reqs_for_extra(extra):
++            for req in reqs:
++                if not req.marker or req.marker.evaluate({'extra': extra}):
++                    yield req
++
++        common = frozenset(reqs_for_extra(None))
++        dm[None].extend(common)
++
++        for extra in self._parsed_pkg_info.get_all('Provides-Extra') or []:
++            s_extra = safe_extra(extra.strip())
++            dm[s_extra] = list(frozenset(reqs_for_extra(extra)) - common)
++
++        return dm
++
++
++_distributionImpl = {
++    '.egg': Distribution,
++    '.egg-info': EggInfoDistribution,
++    '.dist-info': DistInfoDistribution,
++}
++
++
++def issue_warning(*args, **kw):
++    level = 1
++    g = globals()
++    try:
++        # find the first stack frame that is *not* code in
++        # the pkg_resources module, to use for the warning
++        while sys._getframe(level).f_globals is g:
++            level += 1
++    except ValueError:
++        pass
++    warnings.warn(stacklevel=level + 1, *args, **kw)
++
++
++class RequirementParseError(ValueError):
++    def __str__(self):
++        return ' '.join(self.args)
++
++
++def parse_requirements(strs):
++    """Yield ``Requirement`` objects for each specification in `strs`
++
++    `strs` must be a string, or a (possibly-nested) iterable thereof.
++    """
++    # create a steppable iterator, so we can handle \-continuations
++    lines = iter(yield_lines(strs))
++
++    for line in lines:
++        # Drop comments -- a hash without a space may be in a URL.
++        if ' #' in line:
++            line = line[:line.find(' #')]
++        # If there is a line continuation, drop it, and append the next line.
++        if line.endswith('\\'):
++            line = line[:-2].strip()
++            try:
++                line += next(lines)
++            except StopIteration:
++                return
++        yield Requirement(line)
++
++
++class Requirement(packaging.requirements.Requirement):
++    def __init__(self, requirement_string):
++        """DO NOT CALL THIS UNDOCUMENTED METHOD; use Requirement.parse()!"""
++        try:
++            super(Requirement, self).__init__(requirement_string)
++        except packaging.requirements.InvalidRequirement as e:
++            raise RequirementParseError(str(e))
++        self.unsafe_name = self.name
++        project_name = safe_name(self.name)
++        self.project_name, self.key = project_name, project_name.lower()
++        self.specs = [
++            (spec.operator, spec.version) for spec in self.specifier]
++        self.extras = tuple(map(safe_extra, self.extras))
++        self.hashCmp = (
++            self.key,
++            self.specifier,
++            frozenset(self.extras),
++            str(self.marker) if self.marker else None,
++        )
++        self.__hash = hash(self.hashCmp)
++
++    def __eq__(self, other):
++        return (
++            isinstance(other, Requirement) and
++            self.hashCmp == other.hashCmp
++        )
++
++    def __ne__(self, other):
++        return not self == other
++
++    def __contains__(self, item):
++        if isinstance(item, Distribution):
++            if item.key != self.key:
++                return False
++
++            item = item.version
++
++        # Allow prereleases always in order to match the previous behavior of
++        # this method. In the future this should be smarter and follow PEP 440
++        # more accurately.
++        return self.specifier.contains(item, prereleases=True)
++
++    def __hash__(self):
++        return self.__hash
++
++    def __repr__(self):
++        return "Requirement.parse(%r)" % str(self)
++
++    @staticmethod
++    def parse(s):
++        req, = parse_requirements(s)
++        return req
++
++
++def _always_object(classes):
++    """
++    Ensure object appears in the mro even
++    for old-style classes.
++    """
++    if object not in classes:
++        return classes + (object,)
++    return classes
++
++
++def _find_adapter(registry, ob):
++    """Return an adapter factory for `ob` from `registry`"""
++    types = _always_object(inspect.getmro(getattr(ob, '__class__', type(ob))))
++    for t in types:
++        if t in registry:
++            return registry[t]
++
++
++def ensure_directory(path):
++    """Ensure that the parent directory of `path` exists"""
++    dirname = os.path.dirname(path)
++    py31compat.makedirs(dirname, exist_ok=True)
++
++
++def _bypass_ensure_directory(path):
++    """Sandbox-bypassing version of ensure_directory()"""
++    if not WRITE_SUPPORT:
++        raise IOError('"os.mkdir" not supported on this platform.')
++    dirname, filename = split(path)
++    if dirname and filename and not isdir(dirname):
++        _bypass_ensure_directory(dirname)
++        mkdir(dirname, 0o755)
++
++
++def split_sections(s):
++    """Split a string or iterable thereof into (section, content) pairs
++
++    Each ``section`` is a stripped version of the section header ("[section]")
++    and each ``content`` is a list of stripped lines excluding blank lines and
++    comment-only lines.  If there are any such lines before the first section
++    header, they're returned in a first ``section`` of ``None``.
++    """
++    section = None
++    content = []
++    for line in yield_lines(s):
++        if line.startswith("["):
++            if line.endswith("]"):
++                if section or content:
++                    yield section, content
++                section = line[1:-1].strip()
++                content = []
++            else:
++                raise ValueError("Invalid section heading", line)
++        else:
++            content.append(line)
++
++    # wrap up last segment
++    yield section, content
++
++
++def _mkstemp(*args, **kw):
++    old_open = os.open
++    try:
++        # temporarily bypass sandboxing
++        os.open = os_open
++        return tempfile.mkstemp(*args, **kw)
++    finally:
++        # and then put it back
++        os.open = old_open
++
++
++# Silence the PEP440Warning by default, so that end users don't get hit by it
++# randomly just because they use pkg_resources. We want to append the rule
++# because we want earlier uses of filterwarnings to take precedence over this
++# one.
++warnings.filterwarnings("ignore", category=PEP440Warning, append=True)
++
++
++# from jaraco.functools 1.3
++def _call_aside(f, *args, **kwargs):
++    f(*args, **kwargs)
++    return f
++
++
++@_call_aside
++def _initialize(g=globals()):
++    "Set up global resource manager (deliberately not state-saved)"
++    manager = ResourceManager()
++    g['_manager'] = manager
++    g.update(
++        (name, getattr(manager, name))
++        for name in dir(manager)
++        if not name.startswith('_')
++    )
++
++
++@_call_aside
++def _initialize_master_working_set():
++    """
++    Prepare the master working set and make the ``require()``
++    API available.
++
++    This function has explicit effects on the global state
++    of pkg_resources. It is intended to be invoked once at
++    the initialization of this module.
++
++    Invocation by other packages is unsupported and done
++    at their own risk.
++    """
++    working_set = WorkingSet._build_master()
++    _declare_state('object', working_set=working_set)
++
++    require = working_set.require
++    iter_entry_points = working_set.iter_entry_points
++    add_activation_listener = working_set.subscribe
++    run_script = working_set.run_script
++    # backward compatibility
++    run_main = run_script
++    # Activate all distributions already on sys.path with replace=False and
++    # ensure that all distributions added to the working set in the future
++    # (e.g. by calling ``require()``) will get activated as well,
++    # with higher priority (replace=True).
++    tuple(
++        dist.activate(replace=False)
++        for dist in working_set
++    )
++    add_activation_listener(
++        lambda dist: dist.activate(replace=True),
++        existing=False,
++    )
++    working_set.entries = []
++    # match order
++    list(map(working_set.add_entry, sys.path))
++    globals().update(locals())
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/download.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/download.py	(date 1573549699976)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/download.py	(date 1573549699976)
+@@ -0,0 +1,922 @@
++from __future__ import absolute_import
++
++import cgi
++import email.utils
++import getpass
++import json
++import logging
++import mimetypes
++import os
++import platform
++import re
++import shutil
++import sys
++
++from pip._vendor import requests, six, urllib3
++from pip._vendor.cachecontrol import CacheControlAdapter
++from pip._vendor.cachecontrol.caches import FileCache
++from pip._vendor.lockfile import LockError
++from pip._vendor.requests.adapters import BaseAdapter, HTTPAdapter
++from pip._vendor.requests.auth import AuthBase, HTTPBasicAuth
++from pip._vendor.requests.models import CONTENT_CHUNK_SIZE, Response
++from pip._vendor.requests.structures import CaseInsensitiveDict
++from pip._vendor.requests.utils import get_netrc_auth
++# NOTE: XMLRPC Client is not annotated in typeshed as on 2017-07-17, which is
++#       why we ignore the type on this import
++from pip._vendor.six.moves import xmlrpc_client  # type: ignore
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++from pip._vendor.six.moves.urllib import request as urllib_request
++from pip._vendor.six.moves.urllib.parse import unquote as urllib_unquote
++from pip._vendor.urllib3.util import IS_PYOPENSSL
++
++import pip
++from pip._internal.compat import WINDOWS
++from pip._internal.exceptions import HashMismatch, InstallationError
++from pip._internal.locations import write_delete_marker_file
++from pip._internal.models import PyPI
++from pip._internal.utils.encoding import auto_decode
++from pip._internal.utils.filesystem import check_path_owner
++from pip._internal.utils.glibc import libc_ver
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import (
++    ARCHIVE_EXTENSIONS, ask_path_exists, backup_dir, call_subprocess, consume,
++    display_path, format_size, get_installed_version, rmtree, splitext,
++    unpack_file,
++)
++from pip._internal.utils.setuptools_build import SETUPTOOLS_SHIM
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.utils.ui import DownloadProgressProvider
++from pip._internal.vcs import vcs
++
++try:
++    import ssl  # noqa
++except ImportError:
++    ssl = None
++
++HAS_TLS = (ssl is not None) or IS_PYOPENSSL
++
++__all__ = ['get_file_content',
++           'is_url', 'url_to_path', 'path_to_url',
++           'is_archive_file', 'unpack_vcs_link',
++           'unpack_file_url', 'is_vcs_url', 'is_file_url',
++           'unpack_http_url', 'unpack_url']
++
++
++logger = logging.getLogger(__name__)
++
++
++def user_agent():
++    """
++    Return a string representing the user agent.
++    """
++    data = {
++        "installer": {"name": "pip", "version": pip.__version__},
++        "python": platform.python_version(),
++        "implementation": {
++            "name": platform.python_implementation(),
++        },
++    }
++
++    if data["implementation"]["name"] == 'CPython':
++        data["implementation"]["version"] = platform.python_version()
++    elif data["implementation"]["name"] == 'PyPy':
++        if sys.pypy_version_info.releaselevel == 'final':
++            pypy_version_info = sys.pypy_version_info[:3]
++        else:
++            pypy_version_info = sys.pypy_version_info
++        data["implementation"]["version"] = ".".join(
++            [str(x) for x in pypy_version_info]
++        )
++    elif data["implementation"]["name"] == 'Jython':
++        # Complete Guess
++        data["implementation"]["version"] = platform.python_version()
++    elif data["implementation"]["name"] == 'IronPython':
++        # Complete Guess
++        data["implementation"]["version"] = platform.python_version()
++
++    if sys.platform.startswith("linux"):
++        from pip._vendor import distro
++        distro_infos = dict(filter(
++            lambda x: x[1],
++            zip(["name", "version", "id"], distro.linux_distribution()),
++        ))
++        libc = dict(filter(
++            lambda x: x[1],
++            zip(["lib", "version"], libc_ver()),
++        ))
++        if libc:
++            distro_infos["libc"] = libc
++        if distro_infos:
++            data["distro"] = distro_infos
++
++    if sys.platform.startswith("darwin") and platform.mac_ver()[0]:
++        data["distro"] = {"name": "macOS", "version": platform.mac_ver()[0]}
++
++    if platform.system():
++        data.setdefault("system", {})["name"] = platform.system()
++
++    if platform.release():
++        data.setdefault("system", {})["release"] = platform.release()
++
++    if platform.machine():
++        data["cpu"] = platform.machine()
++
++    if HAS_TLS:
++        data["openssl_version"] = ssl.OPENSSL_VERSION
++
++    setuptools_version = get_installed_version("setuptools")
++    if setuptools_version is not None:
++        data["setuptools_version"] = setuptools_version
++
++    return "{data[installer][name]}/{data[installer][version]} {json}".format(
++        data=data,
++        json=json.dumps(data, separators=(",", ":"), sort_keys=True),
++    )
++
++
++class MultiDomainBasicAuth(AuthBase):
++
++    def __init__(self, prompting=True):
++        self.prompting = prompting
++        self.passwords = {}
++
++    def __call__(self, req):
++        parsed = urllib_parse.urlparse(req.url)
++
++        # Get the netloc without any embedded credentials
++        netloc = parsed.netloc.rsplit("@", 1)[-1]
++
++        # Set the url of the request to the url without any credentials
++        req.url = urllib_parse.urlunparse(parsed[:1] + (netloc,) + parsed[2:])
++
++        # Use any stored credentials that we have for this netloc
++        username, password = self.passwords.get(netloc, (None, None))
++
++        # Extract credentials embedded in the url if we have none stored
++        if username is None:
++            username, password = self.parse_credentials(parsed.netloc)
++
++        # Get creds from netrc if we still don't have them
++        if username is None and password is None:
++            netrc_auth = get_netrc_auth(req.url)
++            username, password = netrc_auth if netrc_auth else (None, None)
++
++        if username or password:
++            # Store the username and password
++            self.passwords[netloc] = (username, password)
++
++            # Send the basic auth with this request
++            req = HTTPBasicAuth(username or "", password or "")(req)
++
++        # Attach a hook to handle 401 responses
++        req.register_hook("response", self.handle_401)
++
++        return req
++
++    def handle_401(self, resp, **kwargs):
++        # We only care about 401 responses, anything else we want to just
++        #   pass through the actual response
++        if resp.status_code != 401:
++            return resp
++
++        # We are not able to prompt the user so simply return the response
++        if not self.prompting:
++            return resp
++
++        parsed = urllib_parse.urlparse(resp.url)
++
++        # Prompt the user for a new username and password
++        username = six.moves.input("User for %s: " % parsed.netloc)
++        password = getpass.getpass("Password: ")
++
++        # Store the new username and password to use for future requests
++        if username or password:
++            self.passwords[parsed.netloc] = (username, password)
++
++        # Consume content and release the original connection to allow our new
++        #   request to reuse the same one.
++        resp.content
++        resp.raw.release_conn()
++
++        # Add our new username and password to the request
++        req = HTTPBasicAuth(username or "", password or "")(resp.request)
++
++        # Send our new request
++        new_resp = resp.connection.send(req, **kwargs)
++        new_resp.history.append(resp)
++
++        return new_resp
++
++    def parse_credentials(self, netloc):
++        if "@" in netloc:
++            userinfo = netloc.rsplit("@", 1)[0]
++            if ":" in userinfo:
++                user, pwd = userinfo.split(":", 1)
++                return (urllib_unquote(user), urllib_unquote(pwd))
++            return urllib_unquote(userinfo), None
++        return None, None
++
++
++class LocalFSAdapter(BaseAdapter):
++
++    def send(self, request, stream=None, timeout=None, verify=None, cert=None,
++             proxies=None):
++        pathname = url_to_path(request.url)
++
++        resp = Response()
++        resp.status_code = 200
++        resp.url = request.url
++
++        try:
++            stats = os.stat(pathname)
++        except OSError as exc:
++            resp.status_code = 404
++            resp.raw = exc
++        else:
++            modified = email.utils.formatdate(stats.st_mtime, usegmt=True)
++            content_type = mimetypes.guess_type(pathname)[0] or "text/plain"
++            resp.headers = CaseInsensitiveDict({
++                "Content-Type": content_type,
++                "Content-Length": stats.st_size,
++                "Last-Modified": modified,
++            })
++
++            resp.raw = open(pathname, "rb")
++            resp.close = resp.raw.close
++
++        return resp
++
++    def close(self):
++        pass
++
++
++class SafeFileCache(FileCache):
++    """
++    A file based cache which is safe to use even when the target directory may
++    not be accessible or writable.
++    """
++
++    def __init__(self, *args, **kwargs):
++        super(SafeFileCache, self).__init__(*args, **kwargs)
++
++        # Check to ensure that the directory containing our cache directory
++        # is owned by the user current executing pip. If it does not exist
++        # we will check the parent directory until we find one that does exist.
++        # If it is not owned by the user executing pip then we will disable
++        # the cache and log a warning.
++        if not check_path_owner(self.directory):
++            logger.warning(
++                "The directory '%s' or its parent directory is not owned by "
++                "the current user and the cache has been disabled. Please "
++                "check the permissions and owner of that directory. If "
++                "executing pip with sudo, you may want sudo's -H flag.",
++                self.directory,
++            )
++
++            # Set our directory to None to disable the Cache
++            self.directory = None
++
++    def get(self, *args, **kwargs):
++        # If we don't have a directory, then the cache should be a no-op.
++        if self.directory is None:
++            return
++
++        try:
++            return super(SafeFileCache, self).get(*args, **kwargs)
++        except (LockError, OSError, IOError):
++            # We intentionally silence this error, if we can't access the cache
++            # then we can just skip caching and process the request as if
++            # caching wasn't enabled.
++            pass
++
++    def set(self, *args, **kwargs):
++        # If we don't have a directory, then the cache should be a no-op.
++        if self.directory is None:
++            return
++
++        try:
++            return super(SafeFileCache, self).set(*args, **kwargs)
++        except (LockError, OSError, IOError):
++            # We intentionally silence this error, if we can't access the cache
++            # then we can just skip caching and process the request as if
++            # caching wasn't enabled.
++            pass
++
++    def delete(self, *args, **kwargs):
++        # If we don't have a directory, then the cache should be a no-op.
++        if self.directory is None:
++            return
++
++        try:
++            return super(SafeFileCache, self).delete(*args, **kwargs)
++        except (LockError, OSError, IOError):
++            # We intentionally silence this error, if we can't access the cache
++            # then we can just skip caching and process the request as if
++            # caching wasn't enabled.
++            pass
++
++
++class InsecureHTTPAdapter(HTTPAdapter):
++
++    def cert_verify(self, conn, url, verify, cert):
++        conn.cert_reqs = 'CERT_NONE'
++        conn.ca_certs = None
++
++
++class PipSession(requests.Session):
++
++    timeout = None
++
++    def __init__(self, *args, **kwargs):
++        retries = kwargs.pop("retries", 0)
++        cache = kwargs.pop("cache", None)
++        insecure_hosts = kwargs.pop("insecure_hosts", [])
++
++        super(PipSession, self).__init__(*args, **kwargs)
++
++        # Attach our User Agent to the request
++        self.headers["User-Agent"] = user_agent()
++
++        # Attach our Authentication handler to the session
++        self.auth = MultiDomainBasicAuth()
++
++        # Create our urllib3.Retry instance which will allow us to customize
++        # how we handle retries.
++        retries = urllib3.Retry(
++            # Set the total number of retries that a particular request can
++            # have.
++            total=retries,
++
++            # A 503 error from PyPI typically means that the Fastly -> Origin
++            # connection got interrupted in some way. A 503 error in general
++            # is typically considered a transient error so we'll go ahead and
++            # retry it.
++            # A 500 may indicate transient error in Amazon S3
++            # A 520 or 527 - may indicate transient error in CloudFlare
++            status_forcelist=[500, 503, 520, 527],
++
++            # Add a small amount of back off between failed requests in
++            # order to prevent hammering the service.
++            backoff_factor=0.25,
++        )
++
++        # We want to _only_ cache responses on securely fetched origins. We do
++        # this because we can't validate the response of an insecurely fetched
++        # origin, and we don't want someone to be able to poison the cache and
++        # require manual eviction from the cache to fix it.
++        if cache:
++            secure_adapter = CacheControlAdapter(
++                cache=SafeFileCache(cache, use_dir_lock=True),
++                max_retries=retries,
++            )
++        else:
++            secure_adapter = HTTPAdapter(max_retries=retries)
++
++        # Our Insecure HTTPAdapter disables HTTPS validation. It does not
++        # support caching (see above) so we'll use it for all http:// URLs as
++        # well as any https:// host that we've marked as ignoring TLS errors
++        # for.
++        insecure_adapter = InsecureHTTPAdapter(max_retries=retries)
++
++        self.mount("https://", secure_adapter)
++        self.mount("http://", insecure_adapter)
++
++        # Enable file:// urls
++        self.mount("file://", LocalFSAdapter())
++
++        # We want to use a non-validating adapter for any requests which are
++        # deemed insecure.
++        for host in insecure_hosts:
++            self.mount("https://{}/".format(host), insecure_adapter)
++
++    def request(self, method, url, *args, **kwargs):
++        # Allow setting a default timeout on a session
++        kwargs.setdefault("timeout", self.timeout)
++
++        # Dispatch the actual request
++        return super(PipSession, self).request(method, url, *args, **kwargs)
++
++
++def get_file_content(url, comes_from=None, session=None):
++    """Gets the content of a file; it may be a filename, file: URL, or
++    http: URL.  Returns (location, content).  Content is unicode.
++
++    :param url:         File path or url.
++    :param comes_from:  Origin description of requirements.
++    :param session:     Instance of pip.download.PipSession.
++    """
++    if session is None:
++        raise TypeError(
++            "get_file_content() missing 1 required keyword argument: 'session'"
++        )
++
++    match = _scheme_re.search(url)
++    if match:
++        scheme = match.group(1).lower()
++        if (scheme == 'file' and comes_from and
++                comes_from.startswith('http')):
++            raise InstallationError(
++                'Requirements file %s references URL %s, which is local'
++                % (comes_from, url))
++        if scheme == 'file':
++            path = url.split(':', 1)[1]
++            path = path.replace('\\', '/')
++            match = _url_slash_drive_re.match(path)
++            if match:
++                path = match.group(1) + ':' + path.split('|', 1)[1]
++            path = urllib_parse.unquote(path)
++            if path.startswith('/'):
++                path = '/' + path.lstrip('/')
++            url = path
++        else:
++            # FIXME: catch some errors
++            resp = session.get(url)
++            resp.raise_for_status()
++            return resp.url, resp.text
++    try:
++        with open(url, 'rb') as f:
++            content = auto_decode(f.read())
++    except IOError as exc:
++        raise InstallationError(
++            'Could not open requirements file: %s' % str(exc)
++        )
++    return url, content
++
++
++_scheme_re = re.compile(r'^(http|https|file):', re.I)
++_url_slash_drive_re = re.compile(r'/*([a-z])\|', re.I)
++
++
++def is_url(name):
++    """Returns true if the name looks like a URL"""
++    if ':' not in name:
++        return False
++    scheme = name.split(':', 1)[0].lower()
++    return scheme in ['http', 'https', 'file', 'ftp'] + vcs.all_schemes
++
++
++def url_to_path(url):
++    """
++    Convert a file: URL to a path.
++    """
++    assert url.startswith('file:'), (
++        "You can only turn file: urls into filenames (not %r)" % url)
++
++    _, netloc, path, _, _ = urllib_parse.urlsplit(url)
++
++    # if we have a UNC path, prepend UNC share notation
++    if netloc:
++        netloc = '\\\\' + netloc
++
++    path = urllib_request.url2pathname(netloc + path)
++    return path
++
++
++def path_to_url(path):
++    """
++    Convert a path to a file: URL.  The path will be made absolute and have
++    quoted path parts.
++    """
++    path = os.path.normpath(os.path.abspath(path))
++    url = urllib_parse.urljoin('file:', urllib_request.pathname2url(path))
++    return url
++
++
++def is_archive_file(name):
++    """Return True if `name` is a considered as an archive file."""
++    ext = splitext(name)[1].lower()
++    if ext in ARCHIVE_EXTENSIONS:
++        return True
++    return False
++
++
++def unpack_vcs_link(link, location):
++    vcs_backend = _get_used_vcs_backend(link)
++    vcs_backend.unpack(location)
++
++
++def _get_used_vcs_backend(link):
++    for backend in vcs.backends:
++        if link.scheme in backend.schemes:
++            vcs_backend = backend(link.url)
++            return vcs_backend
++
++
++def is_vcs_url(link):
++    return bool(_get_used_vcs_backend(link))
++
++
++def is_file_url(link):
++    return link.url.lower().startswith('file:')
++
++
++def is_dir_url(link):
++    """Return whether a file:// Link points to a directory.
++
++    ``link`` must not have any other scheme but file://. Call is_file_url()
++    first.
++
++    """
++    link_path = url_to_path(link.url_without_fragment)
++    return os.path.isdir(link_path)
++
++
++def _progress_indicator(iterable, *args, **kwargs):
++    return iterable
++
++
++def _download_url(resp, link, content_file, hashes, progress_bar):
++    try:
++        total_length = int(resp.headers['content-length'])
++    except (ValueError, KeyError, TypeError):
++        total_length = 0
++
++    cached_resp = getattr(resp, "from_cache", False)
++    if logger.getEffectiveLevel() > logging.INFO:
++        show_progress = False
++    elif cached_resp:
++        show_progress = False
++    elif total_length > (40 * 1000):
++        show_progress = True
++    elif not total_length:
++        show_progress = True
++    else:
++        show_progress = False
++
++    show_url = link.show_url
++
++    def resp_read(chunk_size):
++        try:
++            # Special case for urllib3.
++            for chunk in resp.raw.stream(
++                    chunk_size,
++                    # We use decode_content=False here because we don't
++                    # want urllib3 to mess with the raw bytes we get
++                    # from the server. If we decompress inside of
++                    # urllib3 then we cannot verify the checksum
++                    # because the checksum will be of the compressed
++                    # file. This breakage will only occur if the
++                    # server adds a Content-Encoding header, which
++                    # depends on how the server was configured:
++                    # - Some servers will notice that the file isn't a
++                    #   compressible file and will leave the file alone
++                    #   and with an empty Content-Encoding
++                    # - Some servers will notice that the file is
++                    #   already compressed and will leave the file
++                    #   alone and will add a Content-Encoding: gzip
++                    #   header
++                    # - Some servers won't notice anything at all and
++                    #   will take a file that's already been compressed
++                    #   and compress it again and set the
++                    #   Content-Encoding: gzip header
++                    #
++                    # By setting this not to decode automatically we
++                    # hope to eliminate problems with the second case.
++                    decode_content=False):
++                yield chunk
++        except AttributeError:
++            # Standard file-like object.
++            while True:
++                chunk = resp.raw.read(chunk_size)
++                if not chunk:
++                    break
++                yield chunk
++
++    def written_chunks(chunks):
++        for chunk in chunks:
++            content_file.write(chunk)
++            yield chunk
++
++    progress_indicator = _progress_indicator
++
++    if link.netloc == PyPI.netloc:
++        url = show_url
++    else:
++        url = link.url_without_fragment
++
++    if show_progress:  # We don't show progress on cached responses
++        progress_indicator = DownloadProgressProvider(progress_bar,
++                                                      max=total_length)
++        if total_length:
++            logger.info("Downloading %s (%s)", url, format_size(total_length))
++        else:
++            logger.info("Downloading %s", url)
++    elif cached_resp:
++        logger.info("Using cached %s", url)
++    else:
++        logger.info("Downloading %s", url)
++
++    logger.debug('Downloading from URL %s', link)
++
++    downloaded_chunks = written_chunks(
++        progress_indicator(
++            resp_read(CONTENT_CHUNK_SIZE),
++            CONTENT_CHUNK_SIZE
++        )
++    )
++    if hashes:
++        hashes.check_against_chunks(downloaded_chunks)
++    else:
++        consume(downloaded_chunks)
++
++
++def _copy_file(filename, location, link):
++    copy = True
++    download_location = os.path.join(location, link.filename)
++    if os.path.exists(download_location):
++        response = ask_path_exists(
++            'The file %s exists. (i)gnore, (w)ipe, (b)ackup, (a)abort' %
++            display_path(download_location), ('i', 'w', 'b', 'a'))
++        if response == 'i':
++            copy = False
++        elif response == 'w':
++            logger.warning('Deleting %s', display_path(download_location))
++            os.remove(download_location)
++        elif response == 'b':
++            dest_file = backup_dir(download_location)
++            logger.warning(
++                'Backing up %s to %s',
++                display_path(download_location),
++                display_path(dest_file),
++            )
++            shutil.move(download_location, dest_file)
++        elif response == 'a':
++            sys.exit(-1)
++    if copy:
++        shutil.copy(filename, download_location)
++        logger.info('Saved %s', display_path(download_location))
++
++
++def unpack_http_url(link, location, download_dir=None,
++                    session=None, hashes=None, progress_bar="on"):
++    if session is None:
++        raise TypeError(
++            "unpack_http_url() missing 1 required keyword argument: 'session'"
++        )
++
++    with TempDirectory(kind="unpack") as temp_dir:
++        # If a download dir is specified, is the file already downloaded there?
++        already_downloaded_path = None
++        if download_dir:
++            already_downloaded_path = _check_download_dir(link,
++                                                          download_dir,
++                                                          hashes)
++
++        if already_downloaded_path:
++            from_path = already_downloaded_path
++            content_type = mimetypes.guess_type(from_path)[0]
++        else:
++            # let's download to a tmp dir
++            from_path, content_type = _download_http_url(link,
++                                                         session,
++                                                         temp_dir.path,
++                                                         hashes,
++                                                         progress_bar)
++
++        # unpack the archive to the build dir location. even when only
++        # downloading archives, they have to be unpacked to parse dependencies
++        unpack_file(from_path, location, content_type, link)
++
++        # a download dir is specified; let's copy the archive there
++        if download_dir and not already_downloaded_path:
++            _copy_file(from_path, download_dir, link)
++
++        if not already_downloaded_path:
++            os.unlink(from_path)
++
++
++def unpack_file_url(link, location, download_dir=None, hashes=None):
++    """Unpack link into location.
++
++    If download_dir is provided and link points to a file, make a copy
++    of the link file inside download_dir.
++    """
++    link_path = url_to_path(link.url_without_fragment)
++
++    # If it's a url to a local directory
++    if is_dir_url(link):
++        if os.path.isdir(location):
++            rmtree(location)
++        shutil.copytree(link_path, location, symlinks=True)
++        if download_dir:
++            logger.info('Link is a directory, ignoring download_dir')
++        return
++
++    # If --require-hashes is off, `hashes` is either empty, the
++    # link's embedded hash, or MissingHashes; it is required to
++    # match. If --require-hashes is on, we are satisfied by any
++    # hash in `hashes` matching: a URL-based or an option-based
++    # one; no internet-sourced hash will be in `hashes`.
++    if hashes:
++        hashes.check_against_path(link_path)
++
++    # If a download dir is specified, is the file already there and valid?
++    already_downloaded_path = None
++    if download_dir:
++        already_downloaded_path = _check_download_dir(link,
++                                                      download_dir,
++                                                      hashes)
++
++    if already_downloaded_path:
++        from_path = already_downloaded_path
++    else:
++        from_path = link_path
++
++    content_type = mimetypes.guess_type(from_path)[0]
++
++    # unpack the archive to the build dir location. even when only downloading
++    # archives, they have to be unpacked to parse dependencies
++    unpack_file(from_path, location, content_type, link)
++
++    # a download dir is specified and not already downloaded
++    if download_dir and not already_downloaded_path:
++        _copy_file(from_path, download_dir, link)
++
++
++def _copy_dist_from_dir(link_path, location):
++    """Copy distribution files in `link_path` to `location`.
++
++    Invoked when user requests to install a local directory. E.g.:
++
++        pip install .
++        pip install ~/dev/git-repos/python-prompt-toolkit
++
++    """
++
++    # Note: This is currently VERY SLOW if you have a lot of data in the
++    # directory, because it copies everything with `shutil.copytree`.
++    # What it should really do is build an sdist and install that.
++    # See https://github.com/pypa/pip/issues/2195
++
++    if os.path.isdir(location):
++        rmtree(location)
++
++    # build an sdist
++    setup_py = 'setup.py'
++    sdist_args = [sys.executable]
++    sdist_args.append('-c')
++    sdist_args.append(SETUPTOOLS_SHIM % setup_py)
++    sdist_args.append('sdist')
++    sdist_args += ['--dist-dir', location]
++    logger.info('Running setup.py sdist for %s', link_path)
++
++    with indent_log():
++        call_subprocess(sdist_args, cwd=link_path, show_stdout=False)
++
++    # unpack sdist into `location`
++    sdist = os.path.join(location, os.listdir(location)[0])
++    logger.info('Unpacking sdist %s into %s', sdist, location)
++    unpack_file(sdist, location, content_type=None, link=None)
++
++
++class PipXmlrpcTransport(xmlrpc_client.Transport):
++    """Provide a `xmlrpclib.Transport` implementation via a `PipSession`
++    object.
++    """
++
++    def __init__(self, index_url, session, use_datetime=False):
++        xmlrpc_client.Transport.__init__(self, use_datetime)
++        index_parts = urllib_parse.urlparse(index_url)
++        self._scheme = index_parts.scheme
++        self._session = session
++
++    def request(self, host, handler, request_body, verbose=False):
++        parts = (self._scheme, host, handler, None, None, None)
++        url = urllib_parse.urlunparse(parts)
++        try:
++            headers = {'Content-Type': 'text/xml'}
++            response = self._session.post(url, data=request_body,
++                                          headers=headers, stream=True)
++            response.raise_for_status()
++            self.verbose = verbose
++            return self.parse_response(response.raw)
++        except requests.HTTPError as exc:
++            logger.critical(
++                "HTTP error %s while getting %s",
++                exc.response.status_code, url,
++            )
++            raise
++
++
++def unpack_url(link, location, download_dir=None,
++               only_download=False, session=None, hashes=None,
++               progress_bar="on"):
++    """Unpack link.
++       If link is a VCS link:
++         if only_download, export into download_dir and ignore location
++          else unpack into location
++       for other types of link:
++         - unpack into location
++         - if download_dir, copy the file into download_dir
++         - if only_download, mark location for deletion
++
++    :param hashes: A Hashes object, one of whose embedded hashes must match,
++        or HashMismatch will be raised. If the Hashes is empty, no matches are
++        required, and unhashable types of requirements (like VCS ones, which
++        would ordinarily raise HashUnsupported) are allowed.
++    """
++    # non-editable vcs urls
++    if is_vcs_url(link):
++        unpack_vcs_link(link, location)
++
++    # file urls
++    elif is_file_url(link):
++        unpack_file_url(link, location, download_dir, hashes=hashes)
++
++    # http urls
++    else:
++        if session is None:
++            session = PipSession()
++
++        unpack_http_url(
++            link,
++            location,
++            download_dir,
++            session,
++            hashes=hashes,
++            progress_bar=progress_bar
++        )
++    if only_download:
++        write_delete_marker_file(location)
++
++
++def _download_http_url(link, session, temp_dir, hashes, progress_bar):
++    """Download link url into temp_dir using provided session"""
++    target_url = link.url.split('#', 1)[0]
++    try:
++        resp = session.get(
++            target_url,
++            # We use Accept-Encoding: identity here because requests
++            # defaults to accepting compressed responses. This breaks in
++            # a variety of ways depending on how the server is configured.
++            # - Some servers will notice that the file isn't a compressible
++            #   file and will leave the file alone and with an empty
++            #   Content-Encoding
++            # - Some servers will notice that the file is already
++            #   compressed and will leave the file alone and will add a
++            #   Content-Encoding: gzip header
++            # - Some servers won't notice anything at all and will take
++            #   a file that's already been compressed and compress it again
++            #   and set the Content-Encoding: gzip header
++            # By setting this to request only the identity encoding We're
++            # hoping to eliminate the third case. Hopefully there does not
++            # exist a server which when given a file will notice it is
++            # already compressed and that you're not asking for a
++            # compressed file and will then decompress it before sending
++            # because if that's the case I don't think it'll ever be
++            # possible to make this work.
++            headers={"Accept-Encoding": "identity"},
++            stream=True,
++        )
++        resp.raise_for_status()
++    except requests.HTTPError as exc:
++        logger.critical(
++            "HTTP error %s while getting %s", exc.response.status_code, link,
++        )
++        raise
++
++    content_type = resp.headers.get('content-type', '')
++    filename = link.filename  # fallback
++    # Have a look at the Content-Disposition header for a better guess
++    content_disposition = resp.headers.get('content-disposition')
++    if content_disposition:
++        type, params = cgi.parse_header(content_disposition)
++        # We use ``or`` here because we don't want to use an "empty" value
++        # from the filename param.
++        filename = params.get('filename') or filename
++    ext = splitext(filename)[1]
++    if not ext:
++        ext = mimetypes.guess_extension(content_type)
++        if ext:
++            filename += ext
++    if not ext and link.url != resp.url:
++        ext = os.path.splitext(resp.url)[1]
++        if ext:
++            filename += ext
++    file_path = os.path.join(temp_dir, filename)
++    with open(file_path, 'wb') as content_file:
++        _download_url(resp, link, content_file, hashes, progress_bar)
++    return file_path, content_type
++
++
++def _check_download_dir(link, download_dir, hashes):
++    """ Check download_dir for previously downloaded file with correct hash
++        If a correct file is found return its path else None
++    """
++    download_path = os.path.join(download_dir, link.filename)
++    if os.path.exists(download_path):
++        # If already downloaded, does its hash match?
++        logger.info('File was already downloaded %s', download_path)
++        if hashes:
++            try:
++                hashes.check_against_path(download_path)
++            except HashMismatch:
++                logger.warning(
++                    'Previously-downloaded file %s has bad hash. '
++                    'Re-downloading.',
++                    download_path
++                )
++                os.unlink(download_path)
++                return None
++        return download_path
++    return None
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/cmdoptions.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/cmdoptions.py	(date 1573549699949)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/cmdoptions.py	(date 1573549699949)
+@@ -0,0 +1,609 @@
++"""
++shared options and groups
++
++The principle here is to define options once, but *not* instantiate them
++globally. One reason being that options with action='append' can carry state
++between parses. pip parses general options twice internally, and shouldn't
++pass on state. To be consistent, all options will follow this design.
++
++"""
++from __future__ import absolute_import
++
++import warnings
++from functools import partial
++from optparse import SUPPRESS_HELP, Option, OptionGroup
++
++from pip._internal.index import (
++    FormatControl, fmt_ctl_handle_mutual_exclude, fmt_ctl_no_binary,
++)
++from pip._internal.locations import USER_CACHE_DIR, src_prefix
++from pip._internal.models import PyPI
++from pip._internal.utils.hashes import STRONG_HASHES
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++from pip._internal.utils.ui import BAR_TYPES
++
++if MYPY_CHECK_RUNNING:
++    from typing import Any
++
++
++def make_option_group(group, parser):
++    """
++    Return an OptionGroup object
++    group  -- assumed to be dict with 'name' and 'options' keys
++    parser -- an optparse Parser
++    """
++    option_group = OptionGroup(parser, group['name'])
++    for option in group['options']:
++        option_group.add_option(option())
++    return option_group
++
++
++def check_install_build_global(options, check_options=None):
++    """Disable wheels if per-setup.py call options are set.
++
++    :param options: The OptionParser options to update.
++    :param check_options: The options to check, if not supplied defaults to
++        options.
++    """
++    if check_options is None:
++        check_options = options
++
++    def getname(n):
++        return getattr(check_options, n, None)
++    names = ["build_options", "global_options", "install_options"]
++    if any(map(getname, names)):
++        control = options.format_control
++        fmt_ctl_no_binary(control)
++        warnings.warn(
++            'Disabling all use of wheels due to the use of --build-options '
++            '/ --global-options / --install-options.', stacklevel=2,
++        )
++
++
++###########
++# options #
++###########
++
++help_ = partial(
++    Option,
++    '-h', '--help',
++    dest='help',
++    action='help',
++    help='Show help.',
++)  # type: Any
++
++isolated_mode = partial(
++    Option,
++    "--isolated",
++    dest="isolated_mode",
++    action="store_true",
++    default=False,
++    help=(
++        "Run pip in an isolated mode, ignoring environment variables and user "
++        "configuration."
++    ),
++)
++
++require_virtualenv = partial(
++    Option,
++    # Run only if inside a virtualenv, bail if not.
++    '--require-virtualenv', '--require-venv',
++    dest='require_venv',
++    action='store_true',
++    default=False,
++    help=SUPPRESS_HELP
++)  # type: Any
++
++verbose = partial(
++    Option,
++    '-v', '--verbose',
++    dest='verbose',
++    action='count',
++    default=0,
++    help='Give more output. Option is additive, and can be used up to 3 times.'
++)
++
++no_color = partial(
++    Option,
++    '--no-color',
++    dest='no_color',
++    action='store_true',
++    default=False,
++    help="Suppress colored output",
++)
++
++version = partial(
++    Option,
++    '-V', '--version',
++    dest='version',
++    action='store_true',
++    help='Show version and exit.',
++)  # type: Any
++
++quiet = partial(
++    Option,
++    '-q', '--quiet',
++    dest='quiet',
++    action='count',
++    default=0,
++    help=(
++        'Give less output. Option is additive, and can be used up to 3'
++        ' times (corresponding to WARNING, ERROR, and CRITICAL logging'
++        ' levels).'
++    ),
++)  # type: Any
++
++progress_bar = partial(
++    Option,
++    '--progress-bar',
++    dest='progress_bar',
++    type='choice',
++    choices=list(BAR_TYPES.keys()),
++    default='on',
++    help=(
++        'Specify type of progress to be displayed [' +
++        '|'.join(BAR_TYPES.keys()) + '] (default: %default)'
++    ),
++)  # type: Any
++
++log = partial(
++    Option,
++    "--log", "--log-file", "--local-log",
++    dest="log",
++    metavar="path",
++    help="Path to a verbose appending log."
++)  # type: Any
++
++no_input = partial(
++    Option,
++    # Don't ask for input
++    '--no-input',
++    dest='no_input',
++    action='store_true',
++    default=False,
++    help=SUPPRESS_HELP
++)  # type: Any
++
++proxy = partial(
++    Option,
++    '--proxy',
++    dest='proxy',
++    type='str',
++    default='',
++    help="Specify a proxy in the form [user:passwd@]proxy.server:port."
++)  # type: Any
++
++retries = partial(
++    Option,
++    '--retries',
++    dest='retries',
++    type='int',
++    default=5,
++    help="Maximum number of retries each connection should attempt "
++         "(default %default times).",
++)  # type: Any
++
++timeout = partial(
++    Option,
++    '--timeout', '--default-timeout',
++    metavar='sec',
++    dest='timeout',
++    type='float',
++    default=15,
++    help='Set the socket timeout (default %default seconds).',
++)  # type: Any
++
++skip_requirements_regex = partial(
++    Option,
++    # A regex to be used to skip requirements
++    '--skip-requirements-regex',
++    dest='skip_requirements_regex',
++    type='str',
++    default='',
++    help=SUPPRESS_HELP,
++)  # type: Any
++
++
++def exists_action():
++    return Option(
++        # Option when path already exist
++        '--exists-action',
++        dest='exists_action',
++        type='choice',
++        choices=['s', 'i', 'w', 'b', 'a'],
++        default=[],
++        action='append',
++        metavar='action',
++        help="Default action when a path already exists: "
++             "(s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort).",
++    )
++
++
++cert = partial(
++    Option,
++    '--cert',
++    dest='cert',
++    type='str',
++    metavar='path',
++    help="Path to alternate CA bundle.",
++)  # type: Any
++
++client_cert = partial(
++    Option,
++    '--client-cert',
++    dest='client_cert',
++    type='str',
++    default=None,
++    metavar='path',
++    help="Path to SSL client certificate, a single file containing the "
++         "private key and the certificate in PEM format.",
++)  # type: Any
++
++index_url = partial(
++    Option,
++    '-i', '--index-url', '--pypi-url',
++    dest='index_url',
++    metavar='URL',
++    default=PyPI.simple_url,
++    help="Base URL of Python Package Index (default %default). "
++         "This should point to a repository compliant with PEP 503 "
++         "(the simple repository API) or a local directory laid out "
++         "in the same format.",
++)  # type: Any
++
++
++def extra_index_url():
++    return Option(
++        '--extra-index-url',
++        dest='extra_index_urls',
++        metavar='URL',
++        action='append',
++        default=[],
++        help="Extra URLs of package indexes to use in addition to "
++             "--index-url. Should follow the same rules as "
++             "--index-url.",
++    )
++
++
++no_index = partial(
++    Option,
++    '--no-index',
++    dest='no_index',
++    action='store_true',
++    default=False,
++    help='Ignore package index (only looking at --find-links URLs instead).',
++)  # type: Any
++
++
++def find_links():
++    return Option(
++        '-f', '--find-links',
++        dest='find_links',
++        action='append',
++        default=[],
++        metavar='url',
++        help="If a url or path to an html file, then parse for links to "
++             "archives. If a local path or file:// url that's a directory, "
++             "then look for archives in the directory listing.",
++    )
++
++
++def trusted_host():
++    return Option(
++        "--trusted-host",
++        dest="trusted_hosts",
++        action="append",
++        metavar="HOSTNAME",
++        default=[],
++        help="Mark this host as trusted, even though it does not have valid "
++             "or any HTTPS.",
++    )
++
++
++# Remove after 1.5
++process_dependency_links = partial(
++    Option,
++    "--process-dependency-links",
++    dest="process_dependency_links",
++    action="store_true",
++    default=False,
++    help="Enable the processing of dependency links.",
++)  # type: Any
++
++
++def constraints():
++    return Option(
++        '-c', '--constraint',
++        dest='constraints',
++        action='append',
++        default=[],
++        metavar='file',
++        help='Constrain versions using the given constraints file. '
++        'This option can be used multiple times.'
++    )
++
++
++def requirements():
++    return Option(
++        '-r', '--requirement',
++        dest='requirements',
++        action='append',
++        default=[],
++        metavar='file',
++        help='Install from the given requirements file. '
++        'This option can be used multiple times.'
++    )
++
++
++def editable():
++    return Option(
++        '-e', '--editable',
++        dest='editables',
++        action='append',
++        default=[],
++        metavar='path/url',
++        help=('Install a project in editable mode (i.e. setuptools '
++              '"develop mode") from a local project path or a VCS url.'),
++    )
++
++
++src = partial(
++    Option,
++    '--src', '--source', '--source-dir', '--source-directory',
++    dest='src_dir',
++    metavar='dir',
++    default=src_prefix,
++    help='Directory to check out editable projects into. '
++    'The default in a virtualenv is "<venv path>/src". '
++    'The default for global installs is "<current dir>/src".'
++)  # type: Any
++
++
++def _get_format_control(values, option):
++    """Get a format_control object."""
++    return getattr(values, option.dest)
++
++
++def _handle_no_binary(option, opt_str, value, parser):
++    existing = getattr(parser.values, option.dest)
++    fmt_ctl_handle_mutual_exclude(
++        value, existing.no_binary, existing.only_binary,
++    )
++
++
++def _handle_only_binary(option, opt_str, value, parser):
++    existing = getattr(parser.values, option.dest)
++    fmt_ctl_handle_mutual_exclude(
++        value, existing.only_binary, existing.no_binary,
++    )
++
++
++def no_binary():
++    return Option(
++        "--no-binary", dest="format_control", action="callback",
++        callback=_handle_no_binary, type="str",
++        default=FormatControl(set(), set()),
++        help="Do not use binary packages. Can be supplied multiple times, and "
++             "each time adds to the existing value. Accepts either :all: to "
++             "disable all binary packages, :none: to empty the set, or one or "
++             "more package names with commas between them. Note that some "
++             "packages are tricky to compile and may fail to install when "
++             "this option is used on them.",
++    )
++
++
++def only_binary():
++    return Option(
++        "--only-binary", dest="format_control", action="callback",
++        callback=_handle_only_binary, type="str",
++        default=FormatControl(set(), set()),
++        help="Do not use source packages. Can be supplied multiple times, and "
++             "each time adds to the existing value. Accepts either :all: to "
++             "disable all source packages, :none: to empty the set, or one or "
++             "more package names with commas between them. Packages without "
++             "binary distributions will fail to install when this option is "
++             "used on them.",
++    )
++
++
++cache_dir = partial(
++    Option,
++    "--cache-dir",
++    dest="cache_dir",
++    default=USER_CACHE_DIR,
++    metavar="dir",
++    help="Store the cache data in <dir>."
++)
++
++no_cache = partial(
++    Option,
++    "--no-cache-dir",
++    dest="cache_dir",
++    action="store_false",
++    help="Disable the cache.",
++)
++
++no_deps = partial(
++    Option,
++    '--no-deps', '--no-dependencies',
++    dest='ignore_dependencies',
++    action='store_true',
++    default=False,
++    help="Don't install package dependencies.",
++)  # type: Any
++
++build_dir = partial(
++    Option,
++    '-b', '--build', '--build-dir', '--build-directory',
++    dest='build_dir',
++    metavar='dir',
++    help='Directory to unpack packages into and build in. Note that '
++         'an initial build still takes place in a temporary directory. '
++         'The location of temporary directories can be controlled by setting '
++         'the TMPDIR environment variable (TEMP on Windows) appropriately. '
++         'When passed, build directories are not cleaned in case of failures.'
++)  # type: Any
++
++ignore_requires_python = partial(
++    Option,
++    '--ignore-requires-python',
++    dest='ignore_requires_python',
++    action='store_true',
++    help='Ignore the Requires-Python information.'
++)  # type: Any
++
++no_build_isolation = partial(
++    Option,
++    '--no-build-isolation',
++    dest='build_isolation',
++    action='store_false',
++    default=True,
++    help='Disable isolation when building a modern source distribution. '
++         'Build dependencies specified by PEP 518 must be already installed '
++         'if this option is used.'
++)  # type: Any
++
++install_options = partial(
++    Option,
++    '--install-option',
++    dest='install_options',
++    action='append',
++    metavar='options',
++    help="Extra arguments to be supplied to the setup.py install "
++         "command (use like --install-option=\"--install-scripts=/usr/local/"
++         "bin\"). Use multiple --install-option options to pass multiple "
++         "options to setup.py install. If you are using an option with a "
++         "directory path, be sure to use absolute path.",
++)  # type: Any
++
++global_options = partial(
++    Option,
++    '--global-option',
++    dest='global_options',
++    action='append',
++    metavar='options',
++    help="Extra global options to be supplied to the setup.py "
++         "call before the install command.",
++)  # type: Any
++
++no_clean = partial(
++    Option,
++    '--no-clean',
++    action='store_true',
++    default=False,
++    help="Don't clean up build directories)."
++)  # type: Any
++
++pre = partial(
++    Option,
++    '--pre',
++    action='store_true',
++    default=False,
++    help="Include pre-release and development versions. By default, "
++         "pip only finds stable versions.",
++)  # type: Any
++
++disable_pip_version_check = partial(
++    Option,
++    "--disable-pip-version-check",
++    dest="disable_pip_version_check",
++    action="store_true",
++    default=False,
++    help="Don't periodically check PyPI to determine whether a new version "
++         "of pip is available for download. Implied with --no-index.",
++)  # type: Any
++
++
++# Deprecated, Remove later
++always_unzip = partial(
++    Option,
++    '-Z', '--always-unzip',
++    dest='always_unzip',
++    action='store_true',
++    help=SUPPRESS_HELP,
++)  # type: Any
++
++
++def _merge_hash(option, opt_str, value, parser):
++    """Given a value spelled "algo:digest", append the digest to a list
++    pointed to in a dict by the algo name."""
++    if not parser.values.hashes:
++        parser.values.hashes = {}
++    try:
++        algo, digest = value.split(':', 1)
++    except ValueError:
++        parser.error('Arguments to %s must be a hash name '
++                     'followed by a value, like --hash=sha256:abcde...' %
++                     opt_str)
++    if algo not in STRONG_HASHES:
++        parser.error('Allowed hash algorithms for %s are %s.' %
++                     (opt_str, ', '.join(STRONG_HASHES)))
++    parser.values.hashes.setdefault(algo, []).append(digest)
++
++
++hash = partial(
++    Option,
++    '--hash',
++    # Hash values eventually end up in InstallRequirement.hashes due to
++    # __dict__ copying in process_line().
++    dest='hashes',
++    action='callback',
++    callback=_merge_hash,
++    type='string',
++    help="Verify that the package's archive matches this "
++         'hash before installing. Example: --hash=sha256:abcdef...',
++)  # type: Any
++
++
++require_hashes = partial(
++    Option,
++    '--require-hashes',
++    dest='require_hashes',
++    action='store_true',
++    default=False,
++    help='Require a hash to check each requirement against, for '
++         'repeatable installs. This option is implied when any package in a '
++         'requirements file has a --hash option.',
++)  # type: Any
++
++
++##########
++# groups #
++##########
++
++general_group = {
++    'name': 'General Options',
++    'options': [
++        help_,
++        isolated_mode,
++        require_virtualenv,
++        verbose,
++        version,
++        quiet,
++        log,
++        no_input,
++        proxy,
++        retries,
++        timeout,
++        skip_requirements_regex,
++        exists_action,
++        trusted_host,
++        cert,
++        client_cert,
++        cache_dir,
++        no_cache,
++        disable_pip_version_check,
++        no_color,
++    ]
++}
++
++index_group = {
++    'name': 'Package Index Options',
++    'options': [
++        index_url,
++        extra_index_url,
++        no_index,
++        find_links,
++        process_dependency_links,
++    ]
++}
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/exceptions.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/exceptions.py	(date 1573549699983)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/exceptions.py	(date 1573549699983)
+@@ -0,0 +1,249 @@
++"""Exceptions used throughout package"""
++from __future__ import absolute_import
++
++from itertools import chain, groupby, repeat
++
++from pip._vendor.six import iteritems
++
++
++class PipError(Exception):
++    """Base pip exception"""
++
++
++class ConfigurationError(PipError):
++    """General exception in configuration"""
++
++
++class InstallationError(PipError):
++    """General exception during installation"""
++
++
++class UninstallationError(PipError):
++    """General exception during uninstallation"""
++
++
++class DistributionNotFound(InstallationError):
++    """Raised when a distribution cannot be found to satisfy a requirement"""
++
++
++class RequirementsFileParseError(InstallationError):
++    """Raised when a general error occurs parsing a requirements file line."""
++
++
++class BestVersionAlreadyInstalled(PipError):
++    """Raised when the most up-to-date version of a package is already
++    installed."""
++
++
++class BadCommand(PipError):
++    """Raised when virtualenv or a command is not found"""
++
++
++class CommandError(PipError):
++    """Raised when there is an error in command-line arguments"""
++
++
++class PreviousBuildDirError(PipError):
++    """Raised when there's a previous conflicting build directory"""
++
++
++class InvalidWheelFilename(InstallationError):
++    """Invalid wheel filename."""
++
++
++class UnsupportedWheel(InstallationError):
++    """Unsupported wheel."""
++
++
++class HashErrors(InstallationError):
++    """Multiple HashError instances rolled into one for reporting"""
++
++    def __init__(self):
++        self.errors = []
++
++    def append(self, error):
++        self.errors.append(error)
++
++    def __str__(self):
++        lines = []
++        self.errors.sort(key=lambda e: e.order)
++        for cls, errors_of_cls in groupby(self.errors, lambda e: e.__class__):
++            lines.append(cls.head)
++            lines.extend(e.body() for e in errors_of_cls)
++        if lines:
++            return '\n'.join(lines)
++
++    def __nonzero__(self):
++        return bool(self.errors)
++
++    def __bool__(self):
++        return self.__nonzero__()
++
++
++class HashError(InstallationError):
++    """
++    A failure to verify a package against known-good hashes
++
++    :cvar order: An int sorting hash exception classes by difficulty of
++        recovery (lower being harder), so the user doesn't bother fretting
++        about unpinned packages when he has deeper issues, like VCS
++        dependencies, to deal with. Also keeps error reports in a
++        deterministic order.
++    :cvar head: A section heading for display above potentially many
++        exceptions of this kind
++    :ivar req: The InstallRequirement that triggered this error. This is
++        pasted on after the exception is instantiated, because it's not
++        typically available earlier.
++
++    """
++    req = None
++    head = ''
++
++    def body(self):
++        """Return a summary of me for display under the heading.
++
++        This default implementation simply prints a description of the
++        triggering requirement.
++
++        :param req: The InstallRequirement that provoked this error, with
++            populate_link() having already been called
++
++        """
++        return '    %s' % self._requirement_name()
++
++    def __str__(self):
++        return '%s\n%s' % (self.head, self.body())
++
++    def _requirement_name(self):
++        """Return a description of the requirement that triggered me.
++
++        This default implementation returns long description of the req, with
++        line numbers
++
++        """
++        return str(self.req) if self.req else 'unknown package'
++
++
++class VcsHashUnsupported(HashError):
++    """A hash was provided for a version-control-system-based requirement, but
++    we don't have a method for hashing those."""
++
++    order = 0
++    head = ("Can't verify hashes for these requirements because we don't "
++            "have a way to hash version control repositories:")
++
++
++class DirectoryUrlHashUnsupported(HashError):
++    """A hash was provided for a version-control-system-based requirement, but
++    we don't have a method for hashing those."""
++
++    order = 1
++    head = ("Can't verify hashes for these file:// requirements because they "
++            "point to directories:")
++
++
++class HashMissing(HashError):
++    """A hash was needed for a requirement but is absent."""
++
++    order = 2
++    head = ('Hashes are required in --require-hashes mode, but they are '
++            'missing from some requirements. Here is a list of those '
++            'requirements along with the hashes their downloaded archives '
++            'actually had. Add lines like these to your requirements files to '
++            'prevent tampering. (If you did not enable --require-hashes '
++            'manually, note that it turns on automatically when any package '
++            'has a hash.)')
++
++    def __init__(self, gotten_hash):
++        """
++        :param gotten_hash: The hash of the (possibly malicious) archive we
++            just downloaded
++        """
++        self.gotten_hash = gotten_hash
++
++    def body(self):
++        # Dodge circular import.
++        from pip._internal.utils.hashes import FAVORITE_HASH
++
++        package = None
++        if self.req:
++            # In the case of URL-based requirements, display the original URL
++            # seen in the requirements file rather than the package name,
++            # so the output can be directly copied into the requirements file.
++            package = (self.req.original_link if self.req.original_link
++                       # In case someone feeds something downright stupid
++                       # to InstallRequirement's constructor.
++                       else getattr(self.req, 'req', None))
++        return '    %s --hash=%s:%s' % (package or 'unknown package',
++                                        FAVORITE_HASH,
++                                        self.gotten_hash)
++
++
++class HashUnpinned(HashError):
++    """A requirement had a hash specified but was not pinned to a specific
++    version."""
++
++    order = 3
++    head = ('In --require-hashes mode, all requirements must have their '
++            'versions pinned with ==. These do not:')
++
++
++class HashMismatch(HashError):
++    """
++    Distribution file hash values don't match.
++
++    :ivar package_name: The name of the package that triggered the hash
++        mismatch. Feel free to write to this after the exception is raise to
++        improve its error message.
++
++    """
++    order = 4
++    head = ('THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS '
++            'FILE. If you have updated the package versions, please update '
++            'the hashes. Otherwise, examine the package contents carefully; '
++            'someone may have tampered with them.')
++
++    def __init__(self, allowed, gots):
++        """
++        :param allowed: A dict of algorithm names pointing to lists of allowed
++            hex digests
++        :param gots: A dict of algorithm names pointing to hashes we
++            actually got from the files under suspicion
++        """
++        self.allowed = allowed
++        self.gots = gots
++
++    def body(self):
++        return '    %s:\n%s' % (self._requirement_name(),
++                                self._hash_comparison())
++
++    def _hash_comparison(self):
++        """
++        Return a comparison of actual and expected hash values.
++
++        Example::
++
++               Expected sha256 abcdeabcdeabcdeabcdeabcdeabcdeabcdeabcdeabcde
++                            or 123451234512345123451234512345123451234512345
++                    Got        bcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdefbcdef
++
++        """
++        def hash_then_or(hash_name):
++            # For now, all the decent hashes have 6-char names, so we can get
++            # away with hard-coding space literals.
++            return chain([hash_name], repeat('    or'))
++
++        lines = []
++        for hash_name, expecteds in iteritems(self.allowed):
++            prefix = hash_then_or(hash_name)
++            lines.extend(('        Expected %s %s' % (next(prefix), e))
++                         for e in expecteds)
++            lines.append('             Got        %s\n' %
++                         self.gots[hash_name].hexdigest())
++            prefix = '    or'
++        return '\n'.join(lines)
++
++
++class UnsupportedPythonVersion(InstallationError):
++    """Unsupported python version according to Requires-Python package
++    metadata."""
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/locations.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/locations.py	(date 1573549700000)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/locations.py	(date 1573549700000)
+@@ -0,0 +1,194 @@
++"""Locations where we look for configs, install stuff, etc"""
++from __future__ import absolute_import
++
++import os
++import os.path
++import platform
++import site
++import sys
++import sysconfig
++from distutils import sysconfig as distutils_sysconfig
++from distutils.command.install import SCHEME_KEYS, install  # type: ignore
++
++from pip._internal.compat import WINDOWS, expanduser
++from pip._internal.utils import appdirs
++
++# Application Directories
++USER_CACHE_DIR = appdirs.user_cache_dir("pip")
++
++
++DELETE_MARKER_MESSAGE = '''\
++This file is placed here by pip to indicate the source was put
++here by pip.
++
++Once this package is successfully installed this source code will be
++deleted (unless you remove this file).
++'''
++PIP_DELETE_MARKER_FILENAME = 'pip-delete-this-directory.txt'
++
++
++def write_delete_marker_file(directory):
++    """
++    Write the pip delete marker file into this directory.
++    """
++    filepath = os.path.join(directory, PIP_DELETE_MARKER_FILENAME)
++    with open(filepath, 'w') as marker_fp:
++        marker_fp.write(DELETE_MARKER_MESSAGE)
++
++
++def running_under_virtualenv():
++    """
++    Return True if we're running inside a virtualenv, False otherwise.
++
++    """
++    if hasattr(sys, 'real_prefix'):
++        return True
++    elif sys.prefix != getattr(sys, "base_prefix", sys.prefix):
++        return True
++
++    return False
++
++
++def virtualenv_no_global():
++    """
++    Return True if in a venv and no system site packages.
++    """
++    # this mirrors the logic in virtualenv.py for locating the
++    # no-global-site-packages.txt file
++    site_mod_dir = os.path.dirname(os.path.abspath(site.__file__))
++    no_global_file = os.path.join(site_mod_dir, 'no-global-site-packages.txt')
++    if running_under_virtualenv() and os.path.isfile(no_global_file):
++        return True
++
++
++if running_under_virtualenv():
++    src_prefix = os.path.join(sys.prefix, 'src')
++else:
++    # FIXME: keep src in cwd for now (it is not a temporary folder)
++    try:
++        src_prefix = os.path.join(os.getcwd(), 'src')
++    except OSError:
++        # In case the current working directory has been renamed or deleted
++        sys.exit(
++            "The folder you are executing pip from can no longer be found."
++        )
++
++# under macOS + virtualenv sys.prefix is not properly resolved
++# it is something like /path/to/python/bin/..
++# Note: using realpath due to tmp dirs on OSX being symlinks
++src_prefix = os.path.abspath(src_prefix)
++
++# FIXME doesn't account for venv linked to global site-packages
++
++site_packages = sysconfig.get_path("purelib")
++# This is because of a bug in PyPy's sysconfig module, see
++# https://bitbucket.org/pypy/pypy/issues/2506/sysconfig-returns-incorrect-paths
++# for more information.
++if platform.python_implementation().lower() == "pypy":
++    site_packages = distutils_sysconfig.get_python_lib()
++try:
++    # Use getusersitepackages if this is present, as it ensures that the
++    # value is initialised properly.
++    user_site = site.getusersitepackages()
++except AttributeError:
++    user_site = site.USER_SITE
++user_dir = expanduser('~')
++if WINDOWS:
++    bin_py = os.path.join(sys.prefix, 'Scripts')
++    bin_user = os.path.join(user_site, 'Scripts')
++    # buildout uses 'bin' on Windows too?
++    if not os.path.exists(bin_py):
++        bin_py = os.path.join(sys.prefix, 'bin')
++        bin_user = os.path.join(user_site, 'bin')
++
++    config_basename = 'pip.ini'
++
++    legacy_storage_dir = os.path.join(user_dir, 'pip')
++    legacy_config_file = os.path.join(
++        legacy_storage_dir,
++        config_basename,
++    )
++else:
++    bin_py = os.path.join(sys.prefix, 'bin')
++    bin_user = os.path.join(user_site, 'bin')
++
++    config_basename = 'pip.conf'
++
++    legacy_storage_dir = os.path.join(user_dir, '.pip')
++    legacy_config_file = os.path.join(
++        legacy_storage_dir,
++        config_basename,
++    )
++    # Forcing to use /usr/local/bin for standard macOS framework installs
++    # Also log to ~/Library/Logs/ for use with the Console.app log viewer
++    if sys.platform[:6] == 'darwin' and sys.prefix[:16] == '/System/Library/':
++        bin_py = '/usr/local/bin'
++
++site_config_files = [
++    os.path.join(path, config_basename)
++    for path in appdirs.site_config_dirs('pip')
++]
++
++venv_config_file = os.path.join(sys.prefix, config_basename)
++new_config_file = os.path.join(appdirs.user_config_dir("pip"), config_basename)
++
++
++def distutils_scheme(dist_name, user=False, home=None, root=None,
++                     isolated=False, prefix=None):
++    """
++    Return a distutils install scheme
++    """
++    from distutils.dist import Distribution
++
++    scheme = {}
++
++    if isolated:
++        extra_dist_args = {"script_args": ["--no-user-cfg"]}
++    else:
++        extra_dist_args = {}
++    dist_args = {'name': dist_name}
++    dist_args.update(extra_dist_args)
++
++    d = Distribution(dist_args)
++    d.parse_config_files()
++    i = d.get_command_obj('install', create=True)
++    # NOTE: setting user or home has the side-effect of creating the home dir
++    # or user base for installations during finalize_options()
++    # ideally, we'd prefer a scheme class that has no side-effects.
++    assert not (user and prefix), "user={} prefix={}".format(user, prefix)
++    i.user = user or i.user
++    if user:
++        i.prefix = ""
++    i.prefix = prefix or i.prefix
++    i.home = home or i.home
++    i.root = root or i.root
++    i.finalize_options()
++    for key in SCHEME_KEYS:
++        scheme[key] = getattr(i, 'install_' + key)
++
++    # install_lib specified in setup.cfg should install *everything*
++    # into there (i.e. it takes precedence over both purelib and
++    # platlib).  Note, i.install_lib is *always* set after
++    # finalize_options(); we only want to override here if the user
++    # has explicitly requested it hence going back to the config
++    if 'install_lib' in d.get_option_dict('install'):
++        scheme.update(dict(purelib=i.install_lib, platlib=i.install_lib))
++
++    if running_under_virtualenv():
++        scheme['headers'] = os.path.join(
++            sys.prefix,
++            'include',
++            'site',
++            'python' + sys.version[:3],
++            dist_name,
++        )
++
++        if root is not None:
++            path_no_drive = os.path.splitdrive(
++                os.path.abspath(scheme["headers"]))[1]
++            scheme["headers"] = os.path.join(
++                root,
++                path_no_drive[1:],
++            )
++
++    return scheme
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/basecommand.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/basecommand.py	(date 1573549699919)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/basecommand.py	(date 1573549699919)
+@@ -0,0 +1,373 @@
++"""Base Command class, and related routines"""
++from __future__ import absolute_import
++
++import logging
++import logging.config
++import optparse
++import os
++import sys
++import warnings
++
++from pip._internal import cmdoptions
++from pip._internal.baseparser import (
++    ConfigOptionParser, UpdatingDefaultsHelpFormatter,
++)
++from pip._internal.compat import WINDOWS
++from pip._internal.download import PipSession
++from pip._internal.exceptions import (
++    BadCommand, CommandError, InstallationError, PreviousBuildDirError,
++    UninstallationError,
++)
++from pip._internal.index import PackageFinder
++from pip._internal.locations import running_under_virtualenv
++from pip._internal.req.req_file import parse_requirements
++from pip._internal.req.req_install import InstallRequirement
++from pip._internal.status_codes import (
++    ERROR, PREVIOUS_BUILD_DIR_ERROR, SUCCESS, UNKNOWN_ERROR,
++    VIRTUALENV_NOT_FOUND,
++)
++from pip._internal.utils import deprecation
++from pip._internal.utils.logging import IndentingFormatter
++from pip._internal.utils.misc import get_prog, normalize_path
++from pip._internal.utils.outdated import pip_version_check
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++
++if MYPY_CHECK_RUNNING:
++    from typing import Optional
++
++__all__ = ['Command']
++
++logger = logging.getLogger(__name__)
++
++
++class Command(object):
++    name = None  # type: Optional[str]
++    usage = None  # type: Optional[str]
++    hidden = False  # type: bool
++    ignore_require_venv = False  # type: bool
++    log_streams = ("ext://sys.stdout", "ext://sys.stderr")
++
++    def __init__(self, isolated=False):
++        parser_kw = {
++            'usage': self.usage,
++            'prog': '%s %s' % (get_prog(), self.name),
++            'formatter': UpdatingDefaultsHelpFormatter(),
++            'add_help_option': False,
++            'name': self.name,
++            'description': self.__doc__,
++            'isolated': isolated,
++        }
++
++        self.parser = ConfigOptionParser(**parser_kw)
++
++        # Commands should add options to this option group
++        optgroup_name = '%s Options' % self.name.capitalize()
++        self.cmd_opts = optparse.OptionGroup(self.parser, optgroup_name)
++
++        # Add the general options
++        gen_opts = cmdoptions.make_option_group(
++            cmdoptions.general_group,
++            self.parser,
++        )
++        self.parser.add_option_group(gen_opts)
++
++    def _build_session(self, options, retries=None, timeout=None):
++        session = PipSession(
++            cache=(
++                normalize_path(os.path.join(options.cache_dir, "http"))
++                if options.cache_dir else None
++            ),
++            retries=retries if retries is not None else options.retries,
++            insecure_hosts=options.trusted_hosts,
++        )
++
++        # Handle custom ca-bundles from the user
++        if options.cert:
++            session.verify = options.cert
++
++        # Handle SSL client certificate
++        if options.client_cert:
++            session.cert = options.client_cert
++
++        # Handle timeouts
++        if options.timeout or timeout:
++            session.timeout = (
++                timeout if timeout is not None else options.timeout
++            )
++
++        # Handle configured proxies
++        if options.proxy:
++            session.proxies = {
++                "http": options.proxy,
++                "https": options.proxy,
++            }
++
++        # Determine if we can prompt the user for authentication or not
++        session.auth.prompting = not options.no_input
++
++        return session
++
++    def parse_args(self, args):
++        # factored out for testability
++        return self.parser.parse_args(args)
++
++    def main(self, args):
++        options, args = self.parse_args(args)
++
++        # Set verbosity so that it can be used elsewhere.
++        self.verbosity = options.verbose - options.quiet
++
++        if self.verbosity >= 1:
++            level = "DEBUG"
++        elif self.verbosity == -1:
++            level = "WARNING"
++        elif self.verbosity == -2:
++            level = "ERROR"
++        elif self.verbosity <= -3:
++            level = "CRITICAL"
++        else:
++            level = "INFO"
++
++        # The root logger should match the "console" level *unless* we
++        # specified "--log" to send debug logs to a file.
++        root_level = level
++        if options.log:
++            root_level = "DEBUG"
++
++        logger_class = "pip._internal.utils.logging.ColorizedStreamHandler"
++        handler_class = "pip._internal.utils.logging.BetterRotatingFileHandler"
++
++        logging.config.dictConfig({
++            "version": 1,
++            "disable_existing_loggers": False,
++            "filters": {
++                "exclude_warnings": {
++                    "()": "pip._internal.utils.logging.MaxLevelFilter",
++                    "level": logging.WARNING,
++                },
++            },
++            "formatters": {
++                "indent": {
++                    "()": IndentingFormatter,
++                    "format": "%(message)s",
++                },
++            },
++            "handlers": {
++                "console": {
++                    "level": level,
++                    "class": logger_class,
++                    "no_color": options.no_color,
++                    "stream": self.log_streams[0],
++                    "filters": ["exclude_warnings"],
++                    "formatter": "indent",
++                },
++                "console_errors": {
++                    "level": "WARNING",
++                    "class": logger_class,
++                    "no_color": options.no_color,
++                    "stream": self.log_streams[1],
++                    "formatter": "indent",
++                },
++                "user_log": {
++                    "level": "DEBUG",
++                    "class": handler_class,
++                    "filename": options.log or "/dev/null",
++                    "delay": True,
++                    "formatter": "indent",
++                },
++            },
++            "root": {
++                "level": root_level,
++                "handlers": list(filter(None, [
++                    "console",
++                    "console_errors",
++                    "user_log" if options.log else None,
++                ])),
++            },
++            # Disable any logging besides WARNING unless we have DEBUG level
++            # logging enabled. These use both pip._vendor and the bare names
++            # for the case where someone unbundles our libraries.
++            "loggers": {
++                name: {
++                    "level": (
++                        "WARNING" if level in ["INFO", "ERROR"] else "DEBUG"
++                    )
++                } for name in [
++                    "pip._vendor", "distlib", "requests", "urllib3"
++                ]
++            },
++        })
++
++        if sys.version_info[:2] == (3, 3):
++            warnings.warn(
++                "Python 3.3 supported has been deprecated and support for it "
++                "will be dropped in the future. Please upgrade your Python.",
++                deprecation.RemovedInPip11Warning,
++            )
++
++        # TODO: try to get these passing down from the command?
++        #      without resorting to os.environ to hold these.
++
++        if options.no_input:
++            os.environ['PIP_NO_INPUT'] = '1'
++
++        if options.exists_action:
++            os.environ['PIP_EXISTS_ACTION'] = ' '.join(options.exists_action)
++
++        if options.require_venv and not self.ignore_require_venv:
++            # If a venv is required check if it can really be found
++            if not running_under_virtualenv():
++                logger.critical(
++                    'Could not find an activated virtualenv (required).'
++                )
++                sys.exit(VIRTUALENV_NOT_FOUND)
++
++        original_root_handlers = set(logging.root.handlers)
++
++        try:
++            status = self.run(options, args)
++            # FIXME: all commands should return an exit status
++            # and when it is done, isinstance is not needed anymore
++            if isinstance(status, int):
++                return status
++        except PreviousBuildDirError as exc:
++            logger.critical(str(exc))
++            logger.debug('Exception information:', exc_info=True)
++
++            return PREVIOUS_BUILD_DIR_ERROR
++        except (InstallationError, UninstallationError, BadCommand) as exc:
++            logger.critical(str(exc))
++            logger.debug('Exception information:', exc_info=True)
++
++            return ERROR
++        except CommandError as exc:
++            logger.critical('ERROR: %s', exc)
++            logger.debug('Exception information:', exc_info=True)
++
++            return ERROR
++        except KeyboardInterrupt:
++            logger.critical('Operation cancelled by user')
++            logger.debug('Exception information:', exc_info=True)
++
++            return ERROR
++        except:
++            logger.critical('Exception:', exc_info=True)
++
++            return UNKNOWN_ERROR
++        finally:
++            # Check if we're using the latest version of pip available
++            if (not options.disable_pip_version_check and not
++                    getattr(options, "no_index", False)):
++                with self._build_session(
++                        options,
++                        retries=0,
++                        timeout=min(5, options.timeout)) as session:
++                    pip_version_check(session, options)
++            # Avoid leaking loggers
++            for handler in set(logging.root.handlers) - original_root_handlers:
++                # this method benefit from the Logger class internal lock
++                logging.root.removeHandler(handler)
++
++        return SUCCESS
++
++
++class RequirementCommand(Command):
++
++    @staticmethod
++    def populate_requirement_set(requirement_set, args, options, finder,
++                                 session, name, wheel_cache):
++        """
++        Marshal cmd line args into a requirement set.
++        """
++        # NOTE: As a side-effect, options.require_hashes and
++        #       requirement_set.require_hashes may be updated
++
++        for filename in options.constraints:
++            for req_to_add in parse_requirements(
++                    filename,
++                    constraint=True, finder=finder, options=options,
++                    session=session, wheel_cache=wheel_cache):
++                req_to_add.is_direct = True
++                requirement_set.add_requirement(req_to_add)
++
++        for req in args:
++            req_to_add = InstallRequirement.from_line(
++                req, None, isolated=options.isolated_mode,
++                wheel_cache=wheel_cache
++            )
++            req_to_add.is_direct = True
++            requirement_set.add_requirement(req_to_add)
++
++        for req in options.editables:
++            req_to_add = InstallRequirement.from_editable(
++                req,
++                isolated=options.isolated_mode,
++                wheel_cache=wheel_cache
++            )
++            req_to_add.is_direct = True
++            requirement_set.add_requirement(req_to_add)
++
++        for filename in options.requirements:
++            for req_to_add in parse_requirements(
++                    filename,
++                    finder=finder, options=options, session=session,
++                    wheel_cache=wheel_cache):
++                req_to_add.is_direct = True
++                requirement_set.add_requirement(req_to_add)
++        # If --require-hashes was a line in a requirements file, tell
++        # RequirementSet about it:
++        requirement_set.require_hashes = options.require_hashes
++
++        if not (args or options.editables or options.requirements):
++            opts = {'name': name}
++            if options.find_links:
++                raise CommandError(
++                    'You must give at least one requirement to %(name)s '
++                    '(maybe you meant "pip %(name)s %(links)s"?)' %
++                    dict(opts, links=' '.join(options.find_links)))
++            else:
++                raise CommandError(
++                    'You must give at least one requirement to %(name)s '
++                    '(see "pip help %(name)s")' % opts)
++
++        # On Windows, any operation modifying pip should be run as:
++        #     python -m pip ...
++        # See https://github.com/pypa/pip/issues/1299 for more discussion
++        should_show_use_python_msg = (
++            WINDOWS and
++            requirement_set.has_requirement("pip") and
++            os.path.basename(sys.argv[0]).startswith("pip")
++        )
++        if should_show_use_python_msg:
++            new_command = [
++                sys.executable, "-m", "pip"
++            ] + sys.argv[1:]
++            raise CommandError(
++                'To modify pip, please run the following command:\n{}'
++                .format(" ".join(new_command))
++            )
++
++    def _build_package_finder(self, options, session,
++                              platform=None, python_versions=None,
++                              abi=None, implementation=None):
++        """
++        Create a package finder appropriate to this requirement command.
++        """
++        index_urls = [options.index_url] + options.extra_index_urls
++        if options.no_index:
++            logger.debug('Ignoring indexes: %s', ','.join(index_urls))
++            index_urls = []
++
++        return PackageFinder(
++            find_links=options.find_links,
++            format_control=options.format_control,
++            index_urls=index_urls,
++            trusted_hosts=options.trusted_hosts,
++            allow_all_prereleases=options.pre,
++            process_dependency_links=options.process_dependency_links,
++            session=session,
++            platform=platform,
++            versions=python_versions,
++            abi=abi,
++            implementation=implementation,
++        )
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/resolve.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/resolve.py	(date 1573549700016)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/resolve.py	(date 1573549700016)
+@@ -0,0 +1,354 @@
++"""Dependency Resolution
++
++The dependency resolution in pip is performed as follows:
++
++for top-level requirements:
++    a. only one spec allowed per project, regardless of conflicts or not.
++       otherwise a "double requirement" exception is raised
++    b. they override sub-dependency requirements.
++for sub-dependencies
++    a. "first found, wins" (where the order is breadth first)
++"""
++
++import logging
++from collections import defaultdict
++from itertools import chain
++
++from pip._internal.exceptions import (
++    BestVersionAlreadyInstalled, DistributionNotFound, HashError, HashErrors,
++    UnsupportedPythonVersion,
++)
++
++from pip._internal.req.req_install import InstallRequirement
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import dist_in_usersite, ensure_dir
++from pip._internal.utils.packaging import check_dist_requires_python
++
++logger = logging.getLogger(__name__)
++
++
++class Resolver(object):
++    """Resolves which packages need to be installed/uninstalled to perform \
++    the requested operation without breaking the requirements of any package.
++    """
++
++    _allowed_strategies = {"eager", "only-if-needed", "to-satisfy-only"}
++
++    def __init__(self, preparer, session, finder, wheel_cache, use_user_site,
++                 ignore_dependencies, ignore_installed, ignore_requires_python,
++                 force_reinstall, isolated, upgrade_strategy):
++        super(Resolver, self).__init__()
++        assert upgrade_strategy in self._allowed_strategies
++
++        self.preparer = preparer
++        self.finder = finder
++        self.session = session
++
++        # NOTE: This would eventually be replaced with a cache that can give
++        #       information about both sdist and wheels transparently.
++        self.wheel_cache = wheel_cache
++
++        self.require_hashes = None  # This is set in resolve
++
++        self.upgrade_strategy = upgrade_strategy
++        self.force_reinstall = force_reinstall
++        self.isolated = isolated
++        self.ignore_dependencies = ignore_dependencies
++        self.ignore_installed = ignore_installed
++        self.ignore_requires_python = ignore_requires_python
++        self.use_user_site = use_user_site
++
++        self._discovered_dependencies = defaultdict(list)
++
++    def resolve(self, requirement_set):
++        """Resolve what operations need to be done
++
++        As a side-effect of this method, the packages (and their dependencies)
++        are downloaded, unpacked and prepared for installation. This
++        preparation is done by ``pip.operations.prepare``.
++
++        Once PyPI has static dependency metadata available, it would be
++        possible to move the preparation to become a step separated from
++        dependency resolution.
++        """
++        # make the wheelhouse
++        if self.preparer.wheel_download_dir:
++            ensure_dir(self.preparer.wheel_download_dir)
++
++        # If any top-level requirement has a hash specified, enter
++        # hash-checking mode, which requires hashes from all.
++        root_reqs = (
++            requirement_set.unnamed_requirements +
++            list(requirement_set.requirements.values())
++        )
++        self.require_hashes = (
++            requirement_set.require_hashes or
++            any(req.has_hash_options for req in root_reqs)
++        )
++
++        # Display where finder is looking for packages
++        locations = self.finder.get_formatted_locations()
++        if locations:
++            logger.info(locations)
++
++        # Actually prepare the files, and collect any exceptions. Most hash
++        # exceptions cannot be checked ahead of time, because
++        # req.populate_link() needs to be called before we can make decisions
++        # based on link type.
++        discovered_reqs = []
++        hash_errors = HashErrors()
++        for req in chain(root_reqs, discovered_reqs):
++            try:
++                discovered_reqs.extend(
++                    self._resolve_one(requirement_set, req)
++                )
++            except HashError as exc:
++                exc.req = req
++                hash_errors.append(exc)
++
++        if hash_errors:
++            raise hash_errors
++
++    def _is_upgrade_allowed(self, req):
++        if self.upgrade_strategy == "to-satisfy-only":
++            return False
++        elif self.upgrade_strategy == "eager":
++            return True
++        else:
++            assert self.upgrade_strategy == "only-if-needed"
++            return req.is_direct
++
++    def _set_req_to_reinstall(self, req):
++        """
++        Set a requirement to be installed.
++        """
++        # Don't uninstall the conflict if doing a user install and the
++        # conflict is not a user install.
++        if not self.use_user_site or dist_in_usersite(req.satisfied_by):
++            req.conflicts_with = req.satisfied_by
++        req.satisfied_by = None
++
++    # XXX: Stop passing requirement_set for options
++    def _check_skip_installed(self, req_to_install):
++        """Check if req_to_install should be skipped.
++
++        This will check if the req is installed, and whether we should upgrade
++        or reinstall it, taking into account all the relevant user options.
++
++        After calling this req_to_install will only have satisfied_by set to
++        None if the req_to_install is to be upgraded/reinstalled etc. Any
++        other value will be a dist recording the current thing installed that
++        satisfies the requirement.
++
++        Note that for vcs urls and the like we can't assess skipping in this
++        routine - we simply identify that we need to pull the thing down,
++        then later on it is pulled down and introspected to assess upgrade/
++        reinstalls etc.
++
++        :return: A text reason for why it was skipped, or None.
++        """
++        if self.ignore_installed:
++            return None
++
++        req_to_install.check_if_exists(self.use_user_site)
++        if not req_to_install.satisfied_by:
++            return None
++
++        if self.force_reinstall:
++            self._set_req_to_reinstall(req_to_install)
++            return None
++
++        if not self._is_upgrade_allowed(req_to_install):
++            if self.upgrade_strategy == "only-if-needed":
++                return 'not upgraded as not directly required'
++            return 'already satisfied'
++
++        # Check for the possibility of an upgrade.  For link-based
++        # requirements we have to pull the tree down and inspect to assess
++        # the version #, so it's handled way down.
++        if not req_to_install.link:
++            try:
++                self.finder.find_requirement(req_to_install, upgrade=True)
++            except BestVersionAlreadyInstalled:
++                # Then the best version is installed.
++                return 'already up-to-date'
++            except DistributionNotFound:
++                # No distribution found, so we squash the error.  It will
++                # be raised later when we re-try later to do the install.
++                # Why don't we just raise here?
++                pass
++
++        self._set_req_to_reinstall(req_to_install)
++        return None
++
++    def _get_abstract_dist_for(self, req):
++        """Takes a InstallRequirement and returns a single AbstractDist \
++        representing a prepared variant of the same.
++        """
++        assert self.require_hashes is not None, (
++            "require_hashes should have been set in Resolver.resolve()"
++        )
++
++        if req.editable:
++            return self.preparer.prepare_editable_requirement(
++                req, self.require_hashes, self.use_user_site, self.finder,
++            )
++
++        # satisfied_by is only evaluated by calling _check_skip_installed,
++        # so it must be None here.
++        assert req.satisfied_by is None
++        skip_reason = self._check_skip_installed(req)
++
++        if req.satisfied_by:
++            return self.preparer.prepare_installed_requirement(
++                req, self.require_hashes, skip_reason
++            )
++
++        upgrade_allowed = self._is_upgrade_allowed(req)
++        abstract_dist = self.preparer.prepare_linked_requirement(
++            req, self.session, self.finder, upgrade_allowed,
++            self.require_hashes
++        )
++
++        # NOTE
++        # The following portion is for determining if a certain package is
++        # going to be re-installed/upgraded or not and reporting to the user.
++        # This should probably get cleaned up in a future refactor.
++
++        # req.req is only avail after unpack for URL
++        # pkgs repeat check_if_exists to uninstall-on-upgrade
++        # (#14)
++        if not self.ignore_installed:
++            req.check_if_exists(self.use_user_site)
++
++        if req.satisfied_by:
++            should_modify = (
++                self.upgrade_strategy != "to-satisfy-only" or
++                self.force_reinstall or
++                self.ignore_installed or
++                req.link.scheme == 'file'
++            )
++            if should_modify:
++                self._set_req_to_reinstall(req)
++            else:
++                logger.info(
++                    'Requirement already satisfied (use --upgrade to upgrade):'
++                    ' %s', req,
++                )
++
++        return abstract_dist
++
++    def _resolve_one(self, requirement_set, req_to_install):
++        """Prepare a single requirements file.
++
++        :return: A list of additional InstallRequirements to also install.
++        """
++        # Tell user what we are doing for this requirement:
++        # obtain (editable), skipping, processing (local url), collecting
++        # (remote url or package name)
++        if req_to_install.constraint or req_to_install.prepared:
++            return []
++
++        req_to_install.prepared = True
++
++        # register tmp src for cleanup in case something goes wrong
++        requirement_set.reqs_to_cleanup.append(req_to_install)
++
++        abstract_dist = self._get_abstract_dist_for(req_to_install)
++
++        # Parse and return dependencies
++        dist = abstract_dist.dist(self.finder)
++        try:
++            check_dist_requires_python(dist)
++        except UnsupportedPythonVersion as err:
++            if self.ignore_requires_python:
++                logger.warning(err.args[0])
++            else:
++                raise
++
++        more_reqs = []
++
++        def add_req(subreq, extras_requested):
++            sub_install_req = InstallRequirement.from_req(
++                str(subreq),
++                req_to_install,
++                isolated=self.isolated,
++                wheel_cache=self.wheel_cache,
++            )
++            parent_req_name = req_to_install.name
++            to_scan_again, add_to_parent = requirement_set.add_requirement(
++                sub_install_req,
++                parent_req_name=parent_req_name,
++                extras_requested=extras_requested,
++            )
++            if parent_req_name and add_to_parent:
++                self._discovered_dependencies[parent_req_name].append(
++                    add_to_parent
++                )
++            more_reqs.extend(to_scan_again)
++
++        with indent_log():
++            # We add req_to_install before its dependencies, so that we
++            # can refer to it when adding dependencies.
++            if not requirement_set.has_requirement(req_to_install.name):
++                # 'unnamed' requirements will get added here
++                req_to_install.is_direct = True
++                requirement_set.add_requirement(
++                    req_to_install, parent_req_name=None,
++                )
++
++            if not self.ignore_dependencies:
++                if req_to_install.extras:
++                    logger.debug(
++                        "Installing extra requirements: %r",
++                        ','.join(req_to_install.extras),
++                    )
++                missing_requested = sorted(
++                    set(req_to_install.extras) - set(dist.extras)
++                )
++                for missing in missing_requested:
++                    logger.warning(
++                        '%s does not provide the extra \'%s\'',
++                        dist, missing
++                    )
++
++                available_requested = sorted(
++                    set(dist.extras) & set(req_to_install.extras)
++                )
++                for subreq in dist.requires(available_requested):
++                    add_req(subreq, extras_requested=available_requested)
++
++            if not req_to_install.editable and not req_to_install.satisfied_by:
++                # XXX: --no-install leads this to report 'Successfully
++                # downloaded' for only non-editable reqs, even though we took
++                # action on them.
++                requirement_set.successfully_downloaded.append(req_to_install)
++
++        return more_reqs
++
++    def get_installation_order(self, req_set):
++        """Create the installation order.
++
++        The installation order is topological - requirements are installed
++        before the requiring thing. We break cycles at an arbitrary point,
++        and make no other guarantees.
++        """
++        # The current implementation, which we may change at any point
++        # installs the user specified things in the order given, except when
++        # dependencies must come earlier to achieve topological order.
++        order = []
++        ordered_reqs = set()
++
++        def schedule(req):
++            if req.satisfied_by or req in ordered_reqs:
++                return
++            if req.constraint:
++                return
++            ordered_reqs.add(req)
++            for dep in self._discovered_dependencies[req.name]:
++                schedule(dep)
++            order.append(req)
++
++        for install_req in req_set.requirements.values():
++            schedule(install_req)
++        return order
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/status_codes.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/status_codes.py	(date 1573549700025)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/status_codes.py	(date 1573549700025)
+@@ -0,0 +1,8 @@
++from __future__ import absolute_import
++
++SUCCESS = 0
++ERROR = 1
++UNKNOWN_ERROR = 2
++VIRTUALENV_NOT_FOUND = 3
++PREVIOUS_BUILD_DIR_ERROR = 4
++NO_MATCHES_FOUND = 23
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/index.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/index.py	(date 1573549699992)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/index.py	(date 1573549699992)
+@@ -0,0 +1,1117 @@
++"""Routines related to PyPI, indexes"""
++from __future__ import absolute_import
++
++import cgi
++import itertools
++import logging
++import mimetypes
++import os
++import posixpath
++import re
++import sys
++import warnings
++from collections import namedtuple
++
++from pip._vendor import html5lib, requests, six
++from pip._vendor.distlib.compat import unescape
++from pip._vendor.packaging import specifiers
++from pip._vendor.packaging.utils import canonicalize_name
++from pip._vendor.packaging.version import parse as parse_version
++from pip._vendor.requests.exceptions import SSLError
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++from pip._vendor.six.moves.urllib import request as urllib_request
++
++from pip._internal.compat import ipaddress
++from pip._internal.download import HAS_TLS, is_url, path_to_url, url_to_path
++from pip._internal.exceptions import (
++    BestVersionAlreadyInstalled, DistributionNotFound, InvalidWheelFilename,
++    UnsupportedWheel,
++)
++from pip._internal.models import PyPI
++from pip._internal.pep425tags import get_supported
++from pip._internal.utils.deprecation import RemovedInPip11Warning
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import (
++    ARCHIVE_EXTENSIONS, SUPPORTED_EXTENSIONS, cached_property, normalize_path,
++    splitext,
++)
++from pip._internal.utils.packaging import check_requires_python
++from pip._internal.wheel import Wheel, wheel_ext
++
++__all__ = ['FormatControl', 'fmt_ctl_handle_mutual_exclude', 'PackageFinder']
++
++
++SECURE_ORIGINS = [
++    # protocol, hostname, port
++    # Taken from Chrome's list of secure origins (See: http://bit.ly/1qrySKC)
++    ("https", "*", "*"),
++    ("*", "localhost", "*"),
++    ("*", "127.0.0.0/8", "*"),
++    ("*", "::1/128", "*"),
++    ("file", "*", None),
++    # ssh is always secure.
++    ("ssh", "*", "*"),
++]
++
++
++logger = logging.getLogger(__name__)
++
++
++class InstallationCandidate(object):
++
++    def __init__(self, project, version, location):
++        self.project = project
++        self.version = parse_version(version)
++        self.location = location
++        self._key = (self.project, self.version, self.location)
++
++    def __repr__(self):
++        return "<InstallationCandidate({!r}, {!r}, {!r})>".format(
++            self.project, self.version, self.location,
++        )
++
++    def __hash__(self):
++        return hash(self._key)
++
++    def __lt__(self, other):
++        return self._compare(other, lambda s, o: s < o)
++
++    def __le__(self, other):
++        return self._compare(other, lambda s, o: s <= o)
++
++    def __eq__(self, other):
++        return self._compare(other, lambda s, o: s == o)
++
++    def __ge__(self, other):
++        return self._compare(other, lambda s, o: s >= o)
++
++    def __gt__(self, other):
++        return self._compare(other, lambda s, o: s > o)
++
++    def __ne__(self, other):
++        return self._compare(other, lambda s, o: s != o)
++
++    def _compare(self, other, method):
++        if not isinstance(other, InstallationCandidate):
++            return NotImplemented
++
++        return method(self._key, other._key)
++
++
++class PackageFinder(object):
++    """This finds packages.
++
++    This is meant to match easy_install's technique for looking for
++    packages, by reading pages and looking for appropriate links.
++    """
++
++    def __init__(self, find_links, index_urls, allow_all_prereleases=False,
++                 trusted_hosts=None, process_dependency_links=False,
++                 session=None, format_control=None, platform=None,
++                 versions=None, abi=None, implementation=None):
++        """Create a PackageFinder.
++
++        :param format_control: A FormatControl object or None. Used to control
++            the selection of source packages / binary packages when consulting
++            the index and links.
++        :param platform: A string or None. If None, searches for packages
++            that are supported by the current system. Otherwise, will find
++            packages that can be built on the platform passed in. These
++            packages will only be downloaded for distribution: they will
++            not be built locally.
++        :param versions: A list of strings or None. This is passed directly
++            to pep425tags.py in the get_supported() method.
++        :param abi: A string or None. This is passed directly
++            to pep425tags.py in the get_supported() method.
++        :param implementation: A string or None. This is passed directly
++            to pep425tags.py in the get_supported() method.
++        """
++        if session is None:
++            raise TypeError(
++                "PackageFinder() missing 1 required keyword argument: "
++                "'session'"
++            )
++
++        # Build find_links. If an argument starts with ~, it may be
++        # a local file relative to a home directory. So try normalizing
++        # it and if it exists, use the normalized version.
++        # This is deliberately conservative - it might be fine just to
++        # blindly normalize anything starting with a ~...
++        self.find_links = []
++        for link in find_links:
++            if link.startswith('~'):
++                new_link = normalize_path(link)
++                if os.path.exists(new_link):
++                    link = new_link
++            self.find_links.append(link)
++
++        self.index_urls = index_urls
++        self.dependency_links = []
++
++        # These are boring links that have already been logged somehow:
++        self.logged_links = set()
++
++        self.format_control = format_control or FormatControl(set(), set())
++
++        # Domains that we won't emit warnings for when not using HTTPS
++        self.secure_origins = [
++            ("*", host, "*")
++            for host in (trusted_hosts if trusted_hosts else [])
++        ]
++
++        # Do we want to allow _all_ pre-releases?
++        self.allow_all_prereleases = allow_all_prereleases
++
++        # Do we process dependency links?
++        self.process_dependency_links = process_dependency_links
++
++        # The Session we'll use to make requests
++        self.session = session
++
++        # The valid tags to check potential found wheel candidates against
++        self.valid_tags = get_supported(
++            versions=versions,
++            platform=platform,
++            abi=abi,
++            impl=implementation,
++        )
++
++        # If we don't have TLS enabled, then WARN if anyplace we're looking
++        # relies on TLS.
++        if not HAS_TLS:
++            for link in itertools.chain(self.index_urls, self.find_links):
++                parsed = urllib_parse.urlparse(link)
++                if parsed.scheme == "https":
++                    logger.warning(
++                        "pip is configured with locations that require "
++                        "TLS/SSL, however the ssl module in Python is not "
++                        "available."
++                    )
++                    break
++
++    def get_formatted_locations(self):
++        lines = []
++        if self.index_urls and self.index_urls != [PyPI.simple_url]:
++            lines.append(
++                "Looking in indexes: {}".format(", ".join(self.index_urls))
++            )
++        if self.find_links:
++            lines.append(
++                "Looking in links: {}".format(", ".join(self.find_links))
++            )
++        return "\n".join(lines)
++
++    def add_dependency_links(self, links):
++        # # FIXME: this shouldn't be global list this, it should only
++        # # apply to requirements of the package that specifies the
++        # # dependency_links value
++        # # FIXME: also, we should track comes_from (i.e., use Link)
++        if self.process_dependency_links:
++            warnings.warn(
++                "Dependency Links processing has been deprecated and will be "
++                "removed in a future release.",
++                RemovedInPip11Warning,
++            )
++            self.dependency_links.extend(links)
++
++    @staticmethod
++    def _sort_locations(locations, expand_dir=False):
++        """
++        Sort locations into "files" (archives) and "urls", and return
++        a pair of lists (files,urls)
++        """
++        files = []
++        urls = []
++
++        # puts the url for the given file path into the appropriate list
++        def sort_path(path):
++            url = path_to_url(path)
++            if mimetypes.guess_type(url, strict=False)[0] == 'text/html':
++                urls.append(url)
++            else:
++                files.append(url)
++
++        for url in locations:
++
++            is_local_path = os.path.exists(url)
++            is_file_url = url.startswith('file:')
++
++            if is_local_path or is_file_url:
++                if is_local_path:
++                    path = url
++                else:
++                    path = url_to_path(url)
++                if os.path.isdir(path):
++                    if expand_dir:
++                        path = os.path.realpath(path)
++                        for item in os.listdir(path):
++                            sort_path(os.path.join(path, item))
++                    elif is_file_url:
++                        urls.append(url)
++                elif os.path.isfile(path):
++                    sort_path(path)
++                else:
++                    logger.warning(
++                        "Url '%s' is ignored: it is neither a file "
++                        "nor a directory.", url,
++                    )
++            elif is_url(url):
++                # Only add url with clear scheme
++                urls.append(url)
++            else:
++                logger.warning(
++                    "Url '%s' is ignored. It is either a non-existing "
++                    "path or lacks a specific scheme.", url,
++                )
++
++        return files, urls
++
++    def _candidate_sort_key(self, candidate):
++        """
++        Function used to generate link sort key for link tuples.
++        The greater the return value, the more preferred it is.
++        If not finding wheels, then sorted by version only.
++        If finding wheels, then the sort order is by version, then:
++          1. existing installs
++          2. wheels ordered via Wheel.support_index_min(self.valid_tags)
++          3. source archives
++        Note: it was considered to embed this logic into the Link
++              comparison operators, but then different sdist links
++              with the same version, would have to be considered equal
++        """
++        support_num = len(self.valid_tags)
++        build_tag = tuple()
++        if candidate.location.is_wheel:
++            # can raise InvalidWheelFilename
++            wheel = Wheel(candidate.location.filename)
++            if not wheel.supported(self.valid_tags):
++                raise UnsupportedWheel(
++                    "%s is not a supported wheel for this platform. It "
++                    "can't be sorted." % wheel.filename
++                )
++            pri = -(wheel.support_index_min(self.valid_tags))
++            if wheel.build_tag is not None:
++                match = re.match(r'^(\d+)(.*)$', wheel.build_tag)
++                build_tag_groups = match.groups()
++                build_tag = (int(build_tag_groups[0]), build_tag_groups[1])
++        else:  # sdist
++            pri = -(support_num)
++        return (candidate.version, build_tag, pri)
++
++    def _validate_secure_origin(self, logger, location):
++        # Determine if this url used a secure transport mechanism
++        parsed = urllib_parse.urlparse(str(location))
++        origin = (parsed.scheme, parsed.hostname, parsed.port)
++
++        # The protocol to use to see if the protocol matches.
++        # Don't count the repository type as part of the protocol: in
++        # cases such as "git+ssh", only use "ssh". (I.e., Only verify against
++        # the last scheme.)
++        protocol = origin[0].rsplit('+', 1)[-1]
++
++        # Determine if our origin is a secure origin by looking through our
++        # hardcoded list of secure origins, as well as any additional ones
++        # configured on this PackageFinder instance.
++        for secure_origin in (SECURE_ORIGINS + self.secure_origins):
++            if protocol != secure_origin[0] and secure_origin[0] != "*":
++                continue
++
++            try:
++                # We need to do this decode dance to ensure that we have a
++                # unicode object, even on Python 2.x.
++                addr = ipaddress.ip_address(
++                    origin[1]
++                    if (
++                        isinstance(origin[1], six.text_type) or
++                        origin[1] is None
++                    )
++                    else origin[1].decode("utf8")
++                )
++                network = ipaddress.ip_network(
++                    secure_origin[1]
++                    if isinstance(secure_origin[1], six.text_type)
++                    else secure_origin[1].decode("utf8")
++                )
++            except ValueError:
++                # We don't have both a valid address or a valid network, so
++                # we'll check this origin against hostnames.
++                if (origin[1] and
++                        origin[1].lower() != secure_origin[1].lower() and
++                        secure_origin[1] != "*"):
++                    continue
++            else:
++                # We have a valid address and network, so see if the address
++                # is contained within the network.
++                if addr not in network:
++                    continue
++
++            # Check to see if the port patches
++            if (origin[2] != secure_origin[2] and
++                    secure_origin[2] != "*" and
++                    secure_origin[2] is not None):
++                continue
++
++            # If we've gotten here, then this origin matches the current
++            # secure origin and we should return True
++            return True
++
++        # If we've gotten to this point, then the origin isn't secure and we
++        # will not accept it as a valid location to search. We will however
++        # log a warning that we are ignoring it.
++        logger.warning(
++            "The repository located at %s is not a trusted or secure host and "
++            "is being ignored. If this repository is available via HTTPS we "
++            "recommend you use HTTPS instead, otherwise you may silence "
++            "this warning and allow it anyway with '--trusted-host %s'.",
++            parsed.hostname,
++            parsed.hostname,
++        )
++
++        return False
++
++    def _get_index_urls_locations(self, project_name):
++        """Returns the locations found via self.index_urls
++
++        Checks the url_name on the main (first in the list) index and
++        use this url_name to produce all locations
++        """
++
++        def mkurl_pypi_url(url):
++            loc = posixpath.join(
++                url,
++                urllib_parse.quote(canonicalize_name(project_name)))
++            # For maximum compatibility with easy_install, ensure the path
++            # ends in a trailing slash.  Although this isn't in the spec
++            # (and PyPI can handle it without the slash) some other index
++            # implementations might break if they relied on easy_install's
++            # behavior.
++            if not loc.endswith('/'):
++                loc = loc + '/'
++            return loc
++
++        return [mkurl_pypi_url(url) for url in self.index_urls]
++
++    def find_all_candidates(self, project_name):
++        """Find all available InstallationCandidate for project_name
++
++        This checks index_urls, find_links and dependency_links.
++        All versions found are returned as an InstallationCandidate list.
++
++        See _link_package_versions for details on which files are accepted
++        """
++        index_locations = self._get_index_urls_locations(project_name)
++        index_file_loc, index_url_loc = self._sort_locations(index_locations)
++        fl_file_loc, fl_url_loc = self._sort_locations(
++            self.find_links, expand_dir=True,
++        )
++        dep_file_loc, dep_url_loc = self._sort_locations(self.dependency_links)
++
++        file_locations = (Link(url) for url in itertools.chain(
++            index_file_loc, fl_file_loc, dep_file_loc,
++        ))
++
++        # We trust every url that the user has given us whether it was given
++        #   via --index-url or --find-links
++        # We explicitly do not trust links that came from dependency_links
++        # We want to filter out any thing which does not have a secure origin.
++        url_locations = [
++            link for link in itertools.chain(
++                (Link(url) for url in index_url_loc),
++                (Link(url) for url in fl_url_loc),
++                (Link(url) for url in dep_url_loc),
++            )
++            if self._validate_secure_origin(logger, link)
++        ]
++
++        logger.debug('%d location(s) to search for versions of %s:',
++                     len(url_locations), project_name)
++
++        for location in url_locations:
++            logger.debug('* %s', location)
++
++        canonical_name = canonicalize_name(project_name)
++        formats = fmt_ctl_formats(self.format_control, canonical_name)
++        search = Search(project_name, canonical_name, formats)
++        find_links_versions = self._package_versions(
++            # We trust every directly linked archive in find_links
++            (Link(url, '-f') for url in self.find_links),
++            search
++        )
++
++        page_versions = []
++        for page in self._get_pages(url_locations, project_name):
++            logger.debug('Analyzing links from page %s', page.url)
++            with indent_log():
++                page_versions.extend(
++                    self._package_versions(page.links, search)
++                )
++
++        dependency_versions = self._package_versions(
++            (Link(url) for url in self.dependency_links), search
++        )
++        if dependency_versions:
++            logger.debug(
++                'dependency_links found: %s',
++                ', '.join([
++                    version.location.url for version in dependency_versions
++                ])
++            )
++
++        file_versions = self._package_versions(file_locations, search)
++        if file_versions:
++            file_versions.sort(reverse=True)
++            logger.debug(
++                'Local files found: %s',
++                ', '.join([
++                    url_to_path(candidate.location.url)
++                    for candidate in file_versions
++                ])
++            )
++
++        # This is an intentional priority ordering
++        return (
++            file_versions + find_links_versions + page_versions +
++            dependency_versions
++        )
++
++    def find_requirement(self, req, upgrade):
++        """Try to find a Link matching req
++
++        Expects req, an InstallRequirement and upgrade, a boolean
++        Returns a Link if found,
++        Raises DistributionNotFound or BestVersionAlreadyInstalled otherwise
++        """
++        all_candidates = self.find_all_candidates(req.name)
++
++        # Filter out anything which doesn't match our specifier
++        compatible_versions = set(
++            req.specifier.filter(
++                # We turn the version object into a str here because otherwise
++                # when we're debundled but setuptools isn't, Python will see
++                # packaging.version.Version and
++                # pkg_resources._vendor.packaging.version.Version as different
++                # types. This way we'll use a str as a common data interchange
++                # format. If we stop using the pkg_resources provided specifier
++                # and start using our own, we can drop the cast to str().
++                [str(c.version) for c in all_candidates],
++                prereleases=(
++                    self.allow_all_prereleases
++                    if self.allow_all_prereleases else None
++                ),
++            )
++        )
++        applicable_candidates = [
++            # Again, converting to str to deal with debundling.
++            c for c in all_candidates if str(c.version) in compatible_versions
++        ]
++
++        if applicable_candidates:
++            best_candidate = max(applicable_candidates,
++                                 key=self._candidate_sort_key)
++        else:
++            best_candidate = None
++
++        if req.satisfied_by is not None:
++            installed_version = parse_version(req.satisfied_by.version)
++        else:
++            installed_version = None
++
++        if installed_version is None and best_candidate is None:
++            logger.critical(
++                'Could not find a version that satisfies the requirement %s '
++                '(from versions: %s)',
++                req,
++                ', '.join(
++                    sorted(
++                        {str(c.version) for c in all_candidates},
++                        key=parse_version,
++                    )
++                )
++            )
++
++            raise DistributionNotFound(
++                'No matching distribution found for %s' % req
++            )
++
++        best_installed = False
++        if installed_version and (
++                best_candidate is None or
++                best_candidate.version <= installed_version):
++            best_installed = True
++
++        if not upgrade and installed_version is not None:
++            if best_installed:
++                logger.debug(
++                    'Existing installed version (%s) is most up-to-date and '
++                    'satisfies requirement',
++                    installed_version,
++                )
++            else:
++                logger.debug(
++                    'Existing installed version (%s) satisfies requirement '
++                    '(most up-to-date version is %s)',
++                    installed_version,
++                    best_candidate.version,
++                )
++            return None
++
++        if best_installed:
++            # We have an existing version, and its the best version
++            logger.debug(
++                'Installed version (%s) is most up-to-date (past versions: '
++                '%s)',
++                installed_version,
++                ', '.join(sorted(compatible_versions, key=parse_version)) or
++                "none",
++            )
++            raise BestVersionAlreadyInstalled
++
++        logger.debug(
++            'Using version %s (newest of versions: %s)',
++            best_candidate.version,
++            ', '.join(sorted(compatible_versions, key=parse_version))
++        )
++        return best_candidate.location
++
++    def _get_pages(self, locations, project_name):
++        """
++        Yields (page, page_url) from the given locations, skipping
++        locations that have errors.
++        """
++        seen = set()
++        for location in locations:
++            if location in seen:
++                continue
++            seen.add(location)
++
++            page = self._get_page(location)
++            if page is None:
++                continue
++
++            yield page
++
++    _py_version_re = re.compile(r'-py([123]\.?[0-9]?)$')
++
++    def _sort_links(self, links):
++        """
++        Returns elements of links in order, non-egg links first, egg links
++        second, while eliminating duplicates
++        """
++        eggs, no_eggs = [], []
++        seen = set()
++        for link in links:
++            if link not in seen:
++                seen.add(link)
++                if link.egg_fragment:
++                    eggs.append(link)
++                else:
++                    no_eggs.append(link)
++        return no_eggs + eggs
++
++    def _package_versions(self, links, search):
++        result = []
++        for link in self._sort_links(links):
++            v = self._link_package_versions(link, search)
++            if v is not None:
++                result.append(v)
++        return result
++
++    def _log_skipped_link(self, link, reason):
++        if link not in self.logged_links:
++            logger.debug('Skipping link %s; %s', link, reason)
++            self.logged_links.add(link)
++
++    def _link_package_versions(self, link, search):
++        """Return an InstallationCandidate or None"""
++        version = None
++        if link.egg_fragment:
++            egg_info = link.egg_fragment
++            ext = link.ext
++        else:
++            egg_info, ext = link.splitext()
++            if not ext:
++                self._log_skipped_link(link, 'not a file')
++                return
++            if ext not in SUPPORTED_EXTENSIONS:
++                self._log_skipped_link(
++                    link, 'unsupported archive format: %s' % ext,
++                )
++                return
++            if "binary" not in search.formats and ext == wheel_ext:
++                self._log_skipped_link(
++                    link, 'No binaries permitted for %s' % search.supplied,
++                )
++                return
++            if "macosx10" in link.path and ext == '.zip':
++                self._log_skipped_link(link, 'macosx10 one')
++                return
++            if ext == wheel_ext:
++                try:
++                    wheel = Wheel(link.filename)
++                except InvalidWheelFilename:
++                    self._log_skipped_link(link, 'invalid wheel filename')
++                    return
++                if canonicalize_name(wheel.name) != search.canonical:
++                    self._log_skipped_link(
++                        link, 'wrong project name (not %s)' % search.supplied)
++                    return
++
++                if not wheel.supported(self.valid_tags):
++                    self._log_skipped_link(
++                        link, 'it is not compatible with this Python')
++                    return
++
++                version = wheel.version
++
++        # This should be up by the search.ok_binary check, but see issue 2700.
++        if "source" not in search.formats and ext != wheel_ext:
++            self._log_skipped_link(
++                link, 'No sources permitted for %s' % search.supplied,
++            )
++            return
++
++        if not version:
++            version = egg_info_matches(egg_info, search.supplied, link)
++        if version is None:
++            self._log_skipped_link(
++                link, 'wrong project name (not %s)' % search.supplied)
++            return
++
++        match = self._py_version_re.search(version)
++        if match:
++            version = version[:match.start()]
++            py_version = match.group(1)
++            if py_version != sys.version[:3]:
++                self._log_skipped_link(
++                    link, 'Python version is incorrect')
++                return
++        try:
++            support_this_python = check_requires_python(link.requires_python)
++        except specifiers.InvalidSpecifier:
++            logger.debug("Package %s has an invalid Requires-Python entry: %s",
++                         link.filename, link.requires_python)
++            support_this_python = True
++
++        if not support_this_python:
++            logger.debug("The package %s is incompatible with the python"
++                         "version in use. Acceptable python versions are:%s",
++                         link, link.requires_python)
++            return
++        logger.debug('Found link %s, version: %s', link, version)
++
++        return InstallationCandidate(search.supplied, version, link)
++
++    def _get_page(self, link):
++        return HTMLPage.get_page(link, session=self.session)
++
++
++def egg_info_matches(
++        egg_info, search_name, link,
++        _egg_info_re=re.compile(r'([a-z0-9_.]+)-([a-z0-9_.!+-]+)', re.I)):
++    """Pull the version part out of a string.
++
++    :param egg_info: The string to parse. E.g. foo-2.1
++    :param search_name: The name of the package this belongs to. None to
++        infer the name. Note that this cannot unambiguously parse strings
++        like foo-2-2 which might be foo, 2-2 or foo-2, 2.
++    :param link: The link the string came from, for logging on failure.
++    """
++    match = _egg_info_re.search(egg_info)
++    if not match:
++        logger.debug('Could not parse version from link: %s', link)
++        return None
++    if search_name is None:
++        full_match = match.group(0)
++        return full_match[full_match.index('-'):]
++    name = match.group(0).lower()
++    # To match the "safe" name that pkg_resources creates:
++    name = name.replace('_', '-')
++    # project name and version must be separated by a dash
++    look_for = search_name.lower() + "-"
++    if name.startswith(look_for):
++        return match.group(0)[len(look_for):]
++    else:
++        return None
++
++
++class HTMLPage(object):
++    """Represents one page, along with its URL"""
++
++    def __init__(self, content, url, headers=None):
++        # Determine if we have any encoding information in our headers
++        encoding = None
++        if headers and "Content-Type" in headers:
++            content_type, params = cgi.parse_header(headers["Content-Type"])
++
++            if "charset" in params:
++                encoding = params['charset']
++
++        self.content = content
++        self.parsed = html5lib.parse(
++            self.content,
++            transport_encoding=encoding,
++            namespaceHTMLElements=False,
++        )
++        self.url = url
++        self.headers = headers
++
++    def __str__(self):
++        return self.url
++
++    @classmethod
++    def get_page(cls, link, skip_archives=True, session=None):
++        if session is None:
++            raise TypeError(
++                "get_page() missing 1 required keyword argument: 'session'"
++            )
++
++        url = link.url
++        url = url.split('#', 1)[0]
++
++        # Check for VCS schemes that do not support lookup as web pages.
++        from pip._internal.vcs import VcsSupport
++        for scheme in VcsSupport.schemes:
++            if url.lower().startswith(scheme) and url[len(scheme)] in '+:':
++                logger.debug('Cannot look at %s URL %s', scheme, link)
++                return None
++
++        try:
++            if skip_archives:
++                filename = link.filename
++                for bad_ext in ARCHIVE_EXTENSIONS:
++                    if filename.endswith(bad_ext):
++                        content_type = cls._get_content_type(
++                            url, session=session,
++                        )
++                        if content_type.lower().startswith('text/html'):
++                            break
++                        else:
++                            logger.debug(
++                                'Skipping page %s because of Content-Type: %s',
++                                link,
++                                content_type,
++                            )
++                            return
++
++            logger.debug('Getting page %s', url)
++
++            # Tack index.html onto file:// URLs that point to directories
++            (scheme, netloc, path, params, query, fragment) = \
++                urllib_parse.urlparse(url)
++            if (scheme == 'file' and
++                    os.path.isdir(urllib_request.url2pathname(path))):
++                # add trailing slash if not present so urljoin doesn't trim
++                # final segment
++                if not url.endswith('/'):
++                    url += '/'
++                url = urllib_parse.urljoin(url, 'index.html')
++                logger.debug(' file: URL is directory, getting %s', url)
++
++            resp = session.get(
++                url,
++                headers={
++                    "Accept": "text/html",
++                    "Cache-Control": "max-age=600",
++                },
++            )
++            resp.raise_for_status()
++
++            # The check for archives above only works if the url ends with
++            # something that looks like an archive. However that is not a
++            # requirement of an url. Unless we issue a HEAD request on every
++            # url we cannot know ahead of time for sure if something is HTML
++            # or not. However we can check after we've downloaded it.
++            content_type = resp.headers.get('Content-Type', 'unknown')
++            if not content_type.lower().startswith("text/html"):
++                logger.debug(
++                    'Skipping page %s because of Content-Type: %s',
++                    link,
++                    content_type,
++                )
++                return
++
++            inst = cls(resp.content, resp.url, resp.headers)
++        except requests.HTTPError as exc:
++            cls._handle_fail(link, exc, url)
++        except SSLError as exc:
++            reason = "There was a problem confirming the ssl certificate: "
++            reason += str(exc)
++            cls._handle_fail(link, reason, url, meth=logger.info)
++        except requests.ConnectionError as exc:
++            cls._handle_fail(link, "connection error: %s" % exc, url)
++        except requests.Timeout:
++            cls._handle_fail(link, "timed out", url)
++        else:
++            return inst
++
++    @staticmethod
++    def _handle_fail(link, reason, url, meth=None):
++        if meth is None:
++            meth = logger.debug
++
++        meth("Could not fetch URL %s: %s - skipping", link, reason)
++
++    @staticmethod
++    def _get_content_type(url, session):
++        """Get the Content-Type of the given url, using a HEAD request"""
++        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(url)
++        if scheme not in {'http', 'https'}:
++            # FIXME: some warning or something?
++            # assertion error?
++            return ''
++
++        resp = session.head(url, allow_redirects=True)
++        resp.raise_for_status()
++
++        return resp.headers.get("Content-Type", "")
++
++    @cached_property
++    def base_url(self):
++        bases = [
++            x for x in self.parsed.findall(".//base")
++            if x.get("href") is not None
++        ]
++        if bases and bases[0].get("href"):
++            return bases[0].get("href")
++        else:
++            return self.url
++
++    @property
++    def links(self):
++        """Yields all links in the page"""
++        for anchor in self.parsed.findall(".//a"):
++            if anchor.get("href"):
++                href = anchor.get("href")
++                url = self.clean_link(
++                    urllib_parse.urljoin(self.base_url, href)
++                )
++                pyrequire = anchor.get('data-requires-python')
++                pyrequire = unescape(pyrequire) if pyrequire else None
++                yield Link(url, self, requires_python=pyrequire)
++
++    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\|-]', re.I)
++
++    def clean_link(self, url):
++        """Makes sure a link is fully encoded.  That is, if a ' ' shows up in
++        the link, it will be rewritten to %20 (while not over-quoting
++        % or other characters)."""
++        return self._clean_re.sub(
++            lambda match: '%%%2x' % ord(match.group(0)), url)
++
++
++class Link(object):
++
++    def __init__(self, url, comes_from=None, requires_python=None):
++        """
++        Object representing a parsed link from https://pypi.org/simple/*
++
++        url:
++            url of the resource pointed to (href of the link)
++        comes_from:
++            instance of HTMLPage where the link was found, or string.
++        requires_python:
++            String containing the `Requires-Python` metadata field, specified
++            in PEP 345. This may be specified by a data-requires-python
++            attribute in the HTML link tag, as described in PEP 503.
++        """
++
++        # url can be a UNC windows share
++        if url.startswith('\\\\'):
++            url = path_to_url(url)
++
++        self.url = url
++        self.comes_from = comes_from
++        self.requires_python = requires_python if requires_python else None
++
++    def __str__(self):
++        if self.requires_python:
++            rp = ' (requires-python:%s)' % self.requires_python
++        else:
++            rp = ''
++        if self.comes_from:
++            return '%s (from %s)%s' % (self.url, self.comes_from, rp)
++        else:
++            return str(self.url)
++
++    def __repr__(self):
++        return '<Link %s>' % self
++
++    def __eq__(self, other):
++        if not isinstance(other, Link):
++            return NotImplemented
++        return self.url == other.url
++
++    def __ne__(self, other):
++        if not isinstance(other, Link):
++            return NotImplemented
++        return self.url != other.url
++
++    def __lt__(self, other):
++        if not isinstance(other, Link):
++            return NotImplemented
++        return self.url < other.url
++
++    def __le__(self, other):
++        if not isinstance(other, Link):
++            return NotImplemented
++        return self.url <= other.url
++
++    def __gt__(self, other):
++        if not isinstance(other, Link):
++            return NotImplemented
++        return self.url > other.url
++
++    def __ge__(self, other):
++        if not isinstance(other, Link):
++            return NotImplemented
++        return self.url >= other.url
++
++    def __hash__(self):
++        return hash(self.url)
++
++    @property
++    def filename(self):
++        _, netloc, path, _, _ = urllib_parse.urlsplit(self.url)
++        name = posixpath.basename(path.rstrip('/')) or netloc
++        name = urllib_parse.unquote(name)
++        assert name, ('URL %r produced no filename' % self.url)
++        return name
++
++    @property
++    def scheme(self):
++        return urllib_parse.urlsplit(self.url)[0]
++
++    @property
++    def netloc(self):
++        return urllib_parse.urlsplit(self.url)[1]
++
++    @property
++    def path(self):
++        return urllib_parse.unquote(urllib_parse.urlsplit(self.url)[2])
++
++    def splitext(self):
++        return splitext(posixpath.basename(self.path.rstrip('/')))
++
++    @property
++    def ext(self):
++        return self.splitext()[1]
++
++    @property
++    def url_without_fragment(self):
++        scheme, netloc, path, query, fragment = urllib_parse.urlsplit(self.url)
++        return urllib_parse.urlunsplit((scheme, netloc, path, query, None))
++
++    _egg_fragment_re = re.compile(r'[#&]egg=([^&]*)')
++
++    @property
++    def egg_fragment(self):
++        match = self._egg_fragment_re.search(self.url)
++        if not match:
++            return None
++        return match.group(1)
++
++    _subdirectory_fragment_re = re.compile(r'[#&]subdirectory=([^&]*)')
++
++    @property
++    def subdirectory_fragment(self):
++        match = self._subdirectory_fragment_re.search(self.url)
++        if not match:
++            return None
++        return match.group(1)
++
++    _hash_re = re.compile(
++        r'(sha1|sha224|sha384|sha256|sha512|md5)=([a-f0-9]+)'
++    )
++
++    @property
++    def hash(self):
++        match = self._hash_re.search(self.url)
++        if match:
++            return match.group(2)
++        return None
++
++    @property
++    def hash_name(self):
++        match = self._hash_re.search(self.url)
++        if match:
++            return match.group(1)
++        return None
++
++    @property
++    def show_url(self):
++        return posixpath.basename(self.url.split('#', 1)[0].split('?', 1)[0])
++
++    @property
++    def is_wheel(self):
++        return self.ext == wheel_ext
++
++    @property
++    def is_artifact(self):
++        """
++        Determines if this points to an actual artifact (e.g. a tarball) or if
++        it points to an "abstract" thing like a path or a VCS location.
++        """
++        from pip._internal.vcs import vcs
++
++        if self.scheme in vcs.all_schemes:
++            return False
++
++        return True
++
++
++FormatControl = namedtuple('FormatControl', 'no_binary only_binary')
++"""This object has two fields, no_binary and only_binary.
++
++If a field is falsy, it isn't set. If it is {':all:'}, it should match all
++packages except those listed in the other field. Only one field can be set
++to {':all:'} at a time. The rest of the time exact package name matches
++are listed, with any given package only showing up in one field at a time.
++"""
++
++
++def fmt_ctl_handle_mutual_exclude(value, target, other):
++    new = value.split(',')
++    while ':all:' in new:
++        other.clear()
++        target.clear()
++        target.add(':all:')
++        del new[:new.index(':all:') + 1]
++        if ':none:' not in new:
++            # Without a none, we want to discard everything as :all: covers it
++            return
++    for name in new:
++        if name == ':none:':
++            target.clear()
++            continue
++        name = canonicalize_name(name)
++        other.discard(name)
++        target.add(name)
++
++
++def fmt_ctl_formats(fmt_ctl, canonical_name):
++    result = {"binary", "source"}
++    if canonical_name in fmt_ctl.only_binary:
++        result.discard('source')
++    elif canonical_name in fmt_ctl.no_binary:
++        result.discard('binary')
++    elif ':all:' in fmt_ctl.only_binary:
++        result.discard('source')
++    elif ':all:' in fmt_ctl.no_binary:
++        result.discard('binary')
++    return frozenset(result)
++
++
++def fmt_ctl_no_binary(fmt_ctl):
++    fmt_ctl_handle_mutual_exclude(
++        ':all:', fmt_ctl.no_binary, fmt_ctl.only_binary,
++    )
++
++
++Search = namedtuple('Search', 'supplied canonical formats')
++"""Capture key aspects of a search.
++
++:attribute supplied: The user supplied package.
++:attribute canonical: The canonical package name.
++:attribute formats: The formats allowed for this package. Should be a set
++    with 'binary' or 'source' or both in it.
++"""
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/wheel.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/wheel.py	(date 1573549700045)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/wheel.py	(date 1573549700045)
+@@ -0,0 +1,817 @@
++"""
++Support for installing and building the "wheel" binary package format.
++"""
++from __future__ import absolute_import
++
++import collections
++import compileall
++import copy
++import csv
++import hashlib
++import logging
++import os.path
++import re
++import shutil
++import stat
++import sys
++import warnings
++from base64 import urlsafe_b64encode
++from email.parser import Parser
++
++from pip._vendor import pkg_resources
++from pip._vendor.distlib.scripts import ScriptMaker
++from pip._vendor.packaging.utils import canonicalize_name
++from pip._vendor.six import StringIO
++
++from pip._internal import pep425tags
++from pip._internal.build_env import BuildEnvironment
++from pip._internal.download import path_to_url, unpack_url
++from pip._internal.exceptions import (
++    InstallationError, InvalidWheelFilename, UnsupportedWheel,
++)
++from pip._internal.locations import (
++    PIP_DELETE_MARKER_FILENAME, distutils_scheme,
++)
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import (
++    call_subprocess, captured_stdout, ensure_dir, read_chunks,
++)
++from pip._internal.utils.setuptools_build import SETUPTOOLS_SHIM
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++from pip._internal.utils.ui import open_spinner
++
++if MYPY_CHECK_RUNNING:
++    from typing import Dict, List, Optional
++
++wheel_ext = '.whl'
++
++VERSION_COMPATIBLE = (1, 0)
++
++
++logger = logging.getLogger(__name__)
++
++
++def rehash(path, algo='sha256', blocksize=1 << 20):
++    """Return (hash, length) for path using hashlib.new(algo)"""
++    h = hashlib.new(algo)
++    length = 0
++    with open(path, 'rb') as f:
++        for block in read_chunks(f, size=blocksize):
++            length += len(block)
++            h.update(block)
++    digest = 'sha256=' + urlsafe_b64encode(
++        h.digest()
++    ).decode('latin1').rstrip('=')
++    return (digest, length)
++
++
++def open_for_csv(name, mode):
++    if sys.version_info[0] < 3:
++        nl = {}
++        bin = 'b'
++    else:
++        nl = {'newline': ''}
++        bin = ''
++    return open(name, mode + bin, **nl)
++
++
++def fix_script(path):
++    """Replace #!python with #!/path/to/python
++    Return True if file was changed."""
++    # XXX RECORD hashes will need to be updated
++    if os.path.isfile(path):
++        with open(path, 'rb') as script:
++            firstline = script.readline()
++            if not firstline.startswith(b'#!python'):
++                return False
++            exename = sys.executable.encode(sys.getfilesystemencoding())
++            firstline = b'#!' + exename + os.linesep.encode("ascii")
++            rest = script.read()
++        with open(path, 'wb') as script:
++            script.write(firstline)
++            script.write(rest)
++        return True
++
++
++dist_info_re = re.compile(r"""^(?P<namever>(?P<name>.+?)(-(?P<ver>.+?))?)
++                                \.dist-info$""", re.VERBOSE)
++
++
++def root_is_purelib(name, wheeldir):
++    """
++    Return True if the extracted wheel in wheeldir should go into purelib.
++    """
++    name_folded = name.replace("-", "_")
++    for item in os.listdir(wheeldir):
++        match = dist_info_re.match(item)
++        if match and match.group('name') == name_folded:
++            with open(os.path.join(wheeldir, item, 'WHEEL')) as wheel:
++                for line in wheel:
++                    line = line.lower().rstrip()
++                    if line == "root-is-purelib: true":
++                        return True
++    return False
++
++
++def get_entrypoints(filename):
++    if not os.path.exists(filename):
++        return {}, {}
++
++    # This is done because you can pass a string to entry_points wrappers which
++    # means that they may or may not be valid INI files. The attempt here is to
++    # strip leading and trailing whitespace in order to make them valid INI
++    # files.
++    with open(filename) as fp:
++        data = StringIO()
++        for line in fp:
++            data.write(line.strip())
++            data.write("\n")
++        data.seek(0)
++
++    # get the entry points and then the script names
++    entry_points = pkg_resources.EntryPoint.parse_map(data)
++    console = entry_points.get('console_scripts', {})
++    gui = entry_points.get('gui_scripts', {})
++
++    def _split_ep(s):
++        """get the string representation of EntryPoint, remove space and split
++        on '='"""
++        return str(s).replace(" ", "").split("=")
++
++    # convert the EntryPoint objects into strings with module:function
++    console = dict(_split_ep(v) for v in console.values())
++    gui = dict(_split_ep(v) for v in gui.values())
++    return console, gui
++
++
++def message_about_scripts_not_on_PATH(scripts):
++    # type: (List[str]) -> Optional[str]
++    """Determine if any scripts are not on PATH and format a warning.
++
++    Returns a warning message if one or more scripts are not on PATH,
++    otherwise None.
++    """
++    if not scripts:
++        return None
++
++    # Group scripts by the path they were installed in
++    grouped_by_dir = collections.defaultdict(set)  # type: Dict[str, set]
++    for destfile in scripts:
++        parent_dir = os.path.dirname(destfile)
++        script_name = os.path.basename(destfile)
++        grouped_by_dir[parent_dir].add(script_name)
++
++    # We don't want to warn for directories that are on PATH.
++    not_warn_dirs = [
++        os.path.normcase(i) for i in os.environ["PATH"].split(os.pathsep)
++    ]
++    # If an executable sits with sys.executable, we don't warn for it.
++    #     This covers the case of venv invocations without activating the venv.
++    not_warn_dirs.append(os.path.normcase(os.path.dirname(sys.executable)))
++    warn_for = {
++        parent_dir: scripts for parent_dir, scripts in grouped_by_dir.items()
++        if os.path.normcase(parent_dir) not in not_warn_dirs
++    }
++    if not warn_for:
++        return None
++
++    # Format a message
++    msg_lines = []
++    for parent_dir, scripts in warn_for.items():
++        scripts = sorted(scripts)
++        if len(scripts) == 1:
++            start_text = "script {} is".format(scripts[0])
++        else:
++            start_text = "scripts {} are".format(
++                ", ".join(scripts[:-1]) + " and " + scripts[-1]
++            )
++
++        msg_lines.append(
++            "The {} installed in '{}' which is not on PATH."
++            .format(start_text, parent_dir)
++        )
++
++    last_line_fmt = (
++        "Consider adding {} to PATH or, if you prefer "
++        "to suppress this warning, use --no-warn-script-location."
++    )
++    if len(msg_lines) == 1:
++        msg_lines.append(last_line_fmt.format("this directory"))
++    else:
++        msg_lines.append(last_line_fmt.format("these directories"))
++
++    # Returns the formatted multiline message
++    return "\n".join(msg_lines)
++
++
++def move_wheel_files(name, req, wheeldir, user=False, home=None, root=None,
++                     pycompile=True, scheme=None, isolated=False, prefix=None,
++                     warn_script_location=True):
++    """Install a wheel"""
++
++    if not scheme:
++        scheme = distutils_scheme(
++            name, user=user, home=home, root=root, isolated=isolated,
++            prefix=prefix,
++        )
++
++    if root_is_purelib(name, wheeldir):
++        lib_dir = scheme['purelib']
++    else:
++        lib_dir = scheme['platlib']
++
++    info_dir = []
++    data_dirs = []
++    source = wheeldir.rstrip(os.path.sep) + os.path.sep
++
++    # Record details of the files moved
++    #   installed = files copied from the wheel to the destination
++    #   changed = files changed while installing (scripts #! line typically)
++    #   generated = files newly generated during the install (script wrappers)
++    installed = {}
++    changed = set()
++    generated = []
++
++    # Compile all of the pyc files that we're going to be installing
++    if pycompile:
++        with captured_stdout() as stdout:
++            with warnings.catch_warnings():
++                warnings.filterwarnings('ignore')
++                compileall.compile_dir(source, force=True, quiet=True)
++        logger.debug(stdout.getvalue())
++
++    def normpath(src, p):
++        return os.path.relpath(src, p).replace(os.path.sep, '/')
++
++    def record_installed(srcfile, destfile, modified=False):
++        """Map archive RECORD paths to installation RECORD paths."""
++        oldpath = normpath(srcfile, wheeldir)
++        newpath = normpath(destfile, lib_dir)
++        installed[oldpath] = newpath
++        if modified:
++            changed.add(destfile)
++
++    def clobber(source, dest, is_base, fixer=None, filter=None):
++        ensure_dir(dest)  # common for the 'include' path
++
++        for dir, subdirs, files in os.walk(source):
++            basedir = dir[len(source):].lstrip(os.path.sep)
++            destdir = os.path.join(dest, basedir)
++            if is_base and basedir.split(os.path.sep, 1)[0].endswith('.data'):
++                continue
++            for s in subdirs:
++                destsubdir = os.path.join(dest, basedir, s)
++                if is_base and basedir == '' and destsubdir.endswith('.data'):
++                    data_dirs.append(s)
++                    continue
++                elif (is_base and
++                        s.endswith('.dist-info') and
++                        canonicalize_name(s).startswith(
++                            canonicalize_name(req.name))):
++                    assert not info_dir, ('Multiple .dist-info directories: ' +
++                                          destsubdir + ', ' +
++                                          ', '.join(info_dir))
++                    info_dir.append(destsubdir)
++            for f in files:
++                # Skip unwanted files
++                if filter and filter(f):
++                    continue
++                srcfile = os.path.join(dir, f)
++                destfile = os.path.join(dest, basedir, f)
++                # directory creation is lazy and after the file filtering above
++                # to ensure we don't install empty dirs; empty dirs can't be
++                # uninstalled.
++                ensure_dir(destdir)
++
++                # We use copyfile (not move, copy, or copy2) to be extra sure
++                # that we are not moving directories over (copyfile fails for
++                # directories) as well as to ensure that we are not copying
++                # over any metadata because we want more control over what
++                # metadata we actually copy over.
++                shutil.copyfile(srcfile, destfile)
++
++                # Copy over the metadata for the file, currently this only
++                # includes the atime and mtime.
++                st = os.stat(srcfile)
++                if hasattr(os, "utime"):
++                    os.utime(destfile, (st.st_atime, st.st_mtime))
++
++                # If our file is executable, then make our destination file
++                # executable.
++                if os.access(srcfile, os.X_OK):
++                    st = os.stat(srcfile)
++                    permissions = (
++                        st.st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH
++                    )
++                    os.chmod(destfile, permissions)
++
++                changed = False
++                if fixer:
++                    changed = fixer(destfile)
++                record_installed(srcfile, destfile, changed)
++
++    clobber(source, lib_dir, True)
++
++    assert info_dir, "%s .dist-info directory not found" % req
++
++    # Get the defined entry points
++    ep_file = os.path.join(info_dir[0], 'entry_points.txt')
++    console, gui = get_entrypoints(ep_file)
++
++    def is_entrypoint_wrapper(name):
++        # EP, EP.exe and EP-script.py are scripts generated for
++        # entry point EP by setuptools
++        if name.lower().endswith('.exe'):
++            matchname = name[:-4]
++        elif name.lower().endswith('-script.py'):
++            matchname = name[:-10]
++        elif name.lower().endswith(".pya"):
++            matchname = name[:-4]
++        else:
++            matchname = name
++        # Ignore setuptools-generated scripts
++        return (matchname in console or matchname in gui)
++
++    for datadir in data_dirs:
++        fixer = None
++        filter = None
++        for subdir in os.listdir(os.path.join(wheeldir, datadir)):
++            fixer = None
++            if subdir == 'scripts':
++                fixer = fix_script
++                filter = is_entrypoint_wrapper
++            source = os.path.join(wheeldir, datadir, subdir)
++            dest = scheme[subdir]
++            clobber(source, dest, False, fixer=fixer, filter=filter)
++
++    maker = ScriptMaker(None, scheme['scripts'])
++
++    # Ensure old scripts are overwritten.
++    # See https://github.com/pypa/pip/issues/1800
++    maker.clobber = True
++
++    # Ensure we don't generate any variants for scripts because this is almost
++    # never what somebody wants.
++    # See https://bitbucket.org/pypa/distlib/issue/35/
++    maker.variants = {''}
++
++    # This is required because otherwise distlib creates scripts that are not
++    # executable.
++    # See https://bitbucket.org/pypa/distlib/issue/32/
++    maker.set_mode = True
++
++    # Simplify the script and fix the fact that the default script swallows
++    # every single stack trace.
++    # See https://bitbucket.org/pypa/distlib/issue/34/
++    # See https://bitbucket.org/pypa/distlib/issue/33/
++    def _get_script_text(entry):
++        if entry.suffix is None:
++            raise InstallationError(
++                "Invalid script entry point: %s for req: %s - A callable "
++                "suffix is required. Cf https://packaging.python.org/en/"
++                "latest/distributing.html#console-scripts for more "
++                "information." % (entry, req)
++            )
++        return maker.script_template % {
++            "module": entry.prefix,
++            "import_name": entry.suffix.split(".")[0],
++            "func": entry.suffix,
++        }
++
++    maker._get_script_text = _get_script_text
++    maker.script_template = r"""# -*- coding: utf-8 -*-
++import re
++import sys
++
++from %(module)s import %(import_name)s
++
++if __name__ == '__main__':
++    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
++    sys.exit(%(func)s())
++"""
++
++    # Special case pip and setuptools to generate versioned wrappers
++    #
++    # The issue is that some projects (specifically, pip and setuptools) use
++    # code in setup.py to create "versioned" entry points - pip2.7 on Python
++    # 2.7, pip3.3 on Python 3.3, etc. But these entry points are baked into
++    # the wheel metadata at build time, and so if the wheel is installed with
++    # a *different* version of Python the entry points will be wrong. The
++    # correct fix for this is to enhance the metadata to be able to describe
++    # such versioned entry points, but that won't happen till Metadata 2.0 is
++    # available.
++    # In the meantime, projects using versioned entry points will either have
++    # incorrect versioned entry points, or they will not be able to distribute
++    # "universal" wheels (i.e., they will need a wheel per Python version).
++    #
++    # Because setuptools and pip are bundled with _ensurepip and virtualenv,
++    # we need to use universal wheels. So, as a stopgap until Metadata 2.0, we
++    # override the versioned entry points in the wheel and generate the
++    # correct ones. This code is purely a short-term measure until Metadata 2.0
++    # is available.
++    #
++    # To add the level of hack in this section of code, in order to support
++    # ensurepip this code will look for an ``ENSUREPIP_OPTIONS`` environment
++    # variable which will control which version scripts get installed.
++    #
++    # ENSUREPIP_OPTIONS=altinstall
++    #   - Only pipX.Y and easy_install-X.Y will be generated and installed
++    # ENSUREPIP_OPTIONS=install
++    #   - pipX.Y, pipX, easy_install-X.Y will be generated and installed. Note
++    #     that this option is technically if ENSUREPIP_OPTIONS is set and is
++    #     not altinstall
++    # DEFAULT
++    #   - The default behavior is to install pip, pipX, pipX.Y, easy_install
++    #     and easy_install-X.Y.
++    pip_script = console.pop('pip', None)
++    if pip_script:
++        if "ENSUREPIP_OPTIONS" not in os.environ:
++            spec = 'pip = ' + pip_script
++            generated.extend(maker.make(spec))
++
++        if os.environ.get("ENSUREPIP_OPTIONS", "") != "altinstall":
++            spec = 'pip%s = %s' % (sys.version[:1], pip_script)
++            generated.extend(maker.make(spec))
++
++        spec = 'pip%s = %s' % (sys.version[:3], pip_script)
++        generated.extend(maker.make(spec))
++        # Delete any other versioned pip entry points
++        pip_ep = [k for k in console if re.match(r'pip(\d(\.\d)?)?$', k)]
++        for k in pip_ep:
++            del console[k]
++    easy_install_script = console.pop('easy_install', None)
++    if easy_install_script:
++        if "ENSUREPIP_OPTIONS" not in os.environ:
++            spec = 'easy_install = ' + easy_install_script
++            generated.extend(maker.make(spec))
++
++        spec = 'easy_install-%s = %s' % (sys.version[:3], easy_install_script)
++        generated.extend(maker.make(spec))
++        # Delete any other versioned easy_install entry points
++        easy_install_ep = [
++            k for k in console if re.match(r'easy_install(-\d\.\d)?$', k)
++        ]
++        for k in easy_install_ep:
++            del console[k]
++
++    # Generate the console and GUI entry points specified in the wheel
++    if len(console) > 0:
++        generated_console_scripts = maker.make_multiple(
++            ['%s = %s' % kv for kv in console.items()]
++        )
++        generated.extend(generated_console_scripts)
++
++        if warn_script_location:
++            msg = message_about_scripts_not_on_PATH(generated_console_scripts)
++            if msg is not None:
++                logger.warn(msg)
++
++    if len(gui) > 0:
++        generated.extend(
++            maker.make_multiple(
++                ['%s = %s' % kv for kv in gui.items()],
++                {'gui': True}
++            )
++        )
++
++    # Record pip as the installer
++    installer = os.path.join(info_dir[0], 'INSTALLER')
++    temp_installer = os.path.join(info_dir[0], 'INSTALLER.pip')
++    with open(temp_installer, 'wb') as installer_file:
++        installer_file.write(b'pip\n')
++    shutil.move(temp_installer, installer)
++    generated.append(installer)
++
++    # Record details of all files installed
++    record = os.path.join(info_dir[0], 'RECORD')
++    temp_record = os.path.join(info_dir[0], 'RECORD.pip')
++    with open_for_csv(record, 'r') as record_in:
++        with open_for_csv(temp_record, 'w+') as record_out:
++            reader = csv.reader(record_in)
++            writer = csv.writer(record_out)
++            for row in reader:
++                row[0] = installed.pop(row[0], row[0])
++                if row[0] in changed:
++                    row[1], row[2] = rehash(row[0])
++                writer.writerow(row)
++            for f in generated:
++                h, l = rehash(f)
++                writer.writerow((normpath(f, lib_dir), h, l))
++            for f in installed:
++                writer.writerow((installed[f], '', ''))
++    shutil.move(temp_record, record)
++
++
++def wheel_version(source_dir):
++    """
++    Return the Wheel-Version of an extracted wheel, if possible.
++
++    Otherwise, return False if we couldn't parse / extract it.
++    """
++    try:
++        dist = [d for d in pkg_resources.find_on_path(None, source_dir)][0]
++
++        wheel_data = dist.get_metadata('WHEEL')
++        wheel_data = Parser().parsestr(wheel_data)
++
++        version = wheel_data['Wheel-Version'].strip()
++        version = tuple(map(int, version.split('.')))
++        return version
++    except:
++        return False
++
++
++def check_compatibility(version, name):
++    """
++    Raises errors or warns if called with an incompatible Wheel-Version.
++
++    Pip should refuse to install a Wheel-Version that's a major series
++    ahead of what it's compatible with (e.g 2.0 > 1.1); and warn when
++    installing a version only minor version ahead (e.g 1.2 > 1.1).
++
++    version: a 2-tuple representing a Wheel-Version (Major, Minor)
++    name: name of wheel or package to raise exception about
++
++    :raises UnsupportedWheel: when an incompatible Wheel-Version is given
++    """
++    if not version:
++        raise UnsupportedWheel(
++            "%s is in an unsupported or invalid wheel" % name
++        )
++    if version[0] > VERSION_COMPATIBLE[0]:
++        raise UnsupportedWheel(
++            "%s's Wheel-Version (%s) is not compatible with this version "
++            "of pip" % (name, '.'.join(map(str, version)))
++        )
++    elif version > VERSION_COMPATIBLE:
++        logger.warning(
++            'Installing from a newer Wheel-Version (%s)',
++            '.'.join(map(str, version)),
++        )
++
++
++class Wheel(object):
++    """A wheel file"""
++
++    # TODO: maybe move the install code into this class
++
++    wheel_file_re = re.compile(
++        r"""^(?P<namever>(?P<name>.+?)-(?P<ver>.*?))
++        ((-(?P<build>\d[^-]*?))?-(?P<pyver>.+?)-(?P<abi>.+?)-(?P<plat>.+?)
++        \.whl|\.dist-info)$""",
++        re.VERBOSE
++    )
++
++    def __init__(self, filename):
++        """
++        :raises InvalidWheelFilename: when the filename is invalid for a wheel
++        """
++        wheel_info = self.wheel_file_re.match(filename)
++        if not wheel_info:
++            raise InvalidWheelFilename(
++                "%s is not a valid wheel filename." % filename
++            )
++        self.filename = filename
++        self.name = wheel_info.group('name').replace('_', '-')
++        # we'll assume "_" means "-" due to wheel naming scheme
++        # (https://github.com/pypa/pip/issues/1150)
++        self.version = wheel_info.group('ver').replace('_', '-')
++        self.build_tag = wheel_info.group('build')
++        self.pyversions = wheel_info.group('pyver').split('.')
++        self.abis = wheel_info.group('abi').split('.')
++        self.plats = wheel_info.group('plat').split('.')
++
++        # All the tag combinations from this file
++        self.file_tags = {
++            (x, y, z) for x in self.pyversions
++            for y in self.abis for z in self.plats
++        }
++
++    def support_index_min(self, tags=None):
++        """
++        Return the lowest index that one of the wheel's file_tag combinations
++        achieves in the supported_tags list e.g. if there are 8 supported tags,
++        and one of the file tags is first in the list, then return 0.  Returns
++        None is the wheel is not supported.
++        """
++        if tags is None:  # for mock
++            tags = pep425tags.get_supported()
++        indexes = [tags.index(c) for c in self.file_tags if c in tags]
++        return min(indexes) if indexes else None
++
++    def supported(self, tags=None):
++        """Is this wheel supported on this system?"""
++        if tags is None:  # for mock
++            tags = pep425tags.get_supported()
++        return bool(set(tags).intersection(self.file_tags))
++
++
++class WheelBuilder(object):
++    """Build wheels from a RequirementSet."""
++
++    def __init__(self, finder, preparer, wheel_cache,
++                 build_options=None, global_options=None, no_clean=False):
++        self.finder = finder
++        self.preparer = preparer
++        self.wheel_cache = wheel_cache
++
++        self._wheel_dir = preparer.wheel_download_dir
++
++        self.build_options = build_options or []
++        self.global_options = global_options or []
++        self.no_clean = no_clean
++
++    def _build_one(self, req, output_dir, python_tag=None):
++        """Build one wheel.
++
++        :return: The filename of the built wheel, or None if the build failed.
++        """
++        # Install build deps into temporary directory (PEP 518)
++        with req.build_env:
++            return self._build_one_inside_env(req, output_dir,
++                                              python_tag=python_tag)
++
++    def _build_one_inside_env(self, req, output_dir, python_tag=None):
++        with TempDirectory(kind="wheel") as temp_dir:
++            if self.__build_one(req, temp_dir.path, python_tag=python_tag):
++                try:
++                    wheel_name = os.listdir(temp_dir.path)[0]
++                    wheel_path = os.path.join(output_dir, wheel_name)
++                    shutil.move(
++                        os.path.join(temp_dir.path, wheel_name), wheel_path
++                    )
++                    logger.info('Stored in directory: %s', output_dir)
++                    return wheel_path
++                except:
++                    pass
++            # Ignore return, we can't do anything else useful.
++            self._clean_one(req)
++            return None
++
++    def _base_setup_args(self, req):
++        # NOTE: Eventually, we'd want to also -S to the flags here, when we're
++        # isolating. Currently, it breaks Python in virtualenvs, because it
++        # relies on site.py to find parts of the standard library outside the
++        # virtualenv.
++        return [
++            sys.executable, '-u', '-c',
++            SETUPTOOLS_SHIM % req.setup_py
++        ] + list(self.global_options)
++
++    def __build_one(self, req, tempd, python_tag=None):
++        base_args = self._base_setup_args(req)
++
++        spin_message = 'Running setup.py bdist_wheel for %s' % (req.name,)
++        with open_spinner(spin_message) as spinner:
++            logger.debug('Destination directory: %s', tempd)
++            wheel_args = base_args + ['bdist_wheel', '-d', tempd] \
++                + self.build_options
++
++            if python_tag is not None:
++                wheel_args += ["--python-tag", python_tag]
++
++            try:
++                call_subprocess(wheel_args, cwd=req.setup_py_dir,
++                                show_stdout=False, spinner=spinner)
++                return True
++            except:
++                spinner.finish("error")
++                logger.error('Failed building wheel for %s', req.name)
++                return False
++
++    def _clean_one(self, req):
++        base_args = self._base_setup_args(req)
++
++        logger.info('Running setup.py clean for %s', req.name)
++        clean_args = base_args + ['clean', '--all']
++        try:
++            call_subprocess(clean_args, cwd=req.source_dir, show_stdout=False)
++            return True
++        except:
++            logger.error('Failed cleaning build dir for %s', req.name)
++            return False
++
++    def build(self, requirements, session, autobuilding=False):
++        """Build wheels.
++
++        :param unpack: If True, replace the sdist we built from with the
++            newly built wheel, in preparation for installation.
++        :return: True if all the wheels built correctly.
++        """
++        from pip._internal import index
++
++        building_is_possible = self._wheel_dir or (
++            autobuilding and self.wheel_cache.cache_dir
++        )
++        assert building_is_possible
++
++        buildset = []
++        for req in requirements:
++            if req.constraint:
++                continue
++            if req.is_wheel:
++                if not autobuilding:
++                    logger.info(
++                        'Skipping %s, due to already being wheel.', req.name,
++                    )
++            elif autobuilding and req.editable:
++                pass
++            elif autobuilding and not req.source_dir:
++                pass
++            elif autobuilding and req.link and not req.link.is_artifact:
++                # VCS checkout. Build wheel just for this run.
++                buildset.append((req, True))
++            else:
++                ephem_cache = False
++                if autobuilding:
++                    link = req.link
++                    base, ext = link.splitext()
++                    if index.egg_info_matches(base, None, link) is None:
++                        # E.g. local directory. Build wheel just for this run.
++                        ephem_cache = True
++                    if "binary" not in index.fmt_ctl_formats(
++                            self.finder.format_control,
++                            canonicalize_name(req.name)):
++                        logger.info(
++                            "Skipping bdist_wheel for %s, due to binaries "
++                            "being disabled for it.", req.name,
++                        )
++                        continue
++                buildset.append((req, ephem_cache))
++
++        if not buildset:
++            return True
++
++        # Build the wheels.
++        logger.info(
++            'Building wheels for collected packages: %s',
++            ', '.join([req.name for (req, _) in buildset]),
++        )
++        _cache = self.wheel_cache  # shorter name
++        with indent_log():
++            build_success, build_failure = [], []
++            for req, ephem in buildset:
++                python_tag = None
++                if autobuilding:
++                    python_tag = pep425tags.implementation_tag
++                    if ephem:
++                        output_dir = _cache.get_ephem_path_for_link(req.link)
++                    else:
++                        output_dir = _cache.get_path_for_link(req.link)
++                    try:
++                        ensure_dir(output_dir)
++                    except OSError as e:
++                        logger.warning("Building wheel for %s failed: %s",
++                                       req.name, e)
++                        build_failure.append(req)
++                        continue
++                else:
++                    output_dir = self._wheel_dir
++                wheel_file = self._build_one(
++                    req, output_dir,
++                    python_tag=python_tag,
++                )
++                if wheel_file:
++                    build_success.append(req)
++                    if autobuilding:
++                        # XXX: This is mildly duplicative with prepare_files,
++                        # but not close enough to pull out to a single common
++                        # method.
++                        # The code below assumes temporary source dirs -
++                        # prevent it doing bad things.
++                        if req.source_dir and not os.path.exists(os.path.join(
++                                req.source_dir, PIP_DELETE_MARKER_FILENAME)):
++                            raise AssertionError(
++                                "bad source dir - missing marker")
++                        # Delete the source we built the wheel from
++                        req.remove_temporary_source()
++                        # set the build directory again - name is known from
++                        # the work prepare_files did.
++                        req.source_dir = req.build_location(
++                            self.preparer.build_dir
++                        )
++                        # Update the link for this.
++                        req.link = index.Link(path_to_url(wheel_file))
++                        assert req.link.is_wheel
++                        # extract the wheel into the dir
++                        unpack_url(
++                            req.link, req.source_dir, None, False,
++                            session=session,
++                        )
++                else:
++                    build_failure.append(req)
++
++        # notify success/failure
++        if build_success:
++            logger.info(
++                'Successfully built %s',
++                ' '.join([req.name for req in build_success]),
++            )
++        if build_failure:
++            logger.info(
++                'Failed to build %s',
++                ' '.join([req.name for req in build_failure]),
++            )
++        # Return True if all builds were successful
++        return len(build_failure) == 0
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/build_env.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/build_env.py	(date 1573549699934)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/build_env.py	(date 1573549699934)
+@@ -0,0 +1,92 @@
++"""Build Environment used for isolation during sdist building
++"""
++
++import os
++from distutils.sysconfig import get_python_lib
++from sysconfig import get_paths
++
++from pip._internal.utils.temp_dir import TempDirectory
++
++
++class BuildEnvironment(object):
++    """Creates and manages an isolated environment to install build deps
++    """
++
++    def __init__(self, no_clean):
++        self._temp_dir = TempDirectory(kind="build-env")
++        self._no_clean = no_clean
++
++    @property
++    def path(self):
++        return self._temp_dir.path
++
++    def __enter__(self):
++        self._temp_dir.create()
++
++        self.save_path = os.environ.get('PATH', None)
++        self.save_pythonpath = os.environ.get('PYTHONPATH', None)
++        self.save_nousersite = os.environ.get('PYTHONNOUSERSITE', None)
++
++        install_scheme = 'nt' if (os.name == 'nt') else 'posix_prefix'
++        install_dirs = get_paths(install_scheme, vars={
++            'base': self.path,
++            'platbase': self.path,
++        })
++
++        scripts = install_dirs['scripts']
++        if self.save_path:
++            os.environ['PATH'] = scripts + os.pathsep + self.save_path
++        else:
++            os.environ['PATH'] = scripts + os.pathsep + os.defpath
++
++        # Note: prefer distutils' sysconfig to get the
++        # library paths so PyPy is correctly supported.
++        purelib = get_python_lib(plat_specific=0, prefix=self.path)
++        platlib = get_python_lib(plat_specific=1, prefix=self.path)
++        if purelib == platlib:
++            lib_dirs = purelib
++        else:
++            lib_dirs = purelib + os.pathsep + platlib
++        if self.save_pythonpath:
++            os.environ['PYTHONPATH'] = lib_dirs + os.pathsep + \
++                self.save_pythonpath
++        else:
++            os.environ['PYTHONPATH'] = lib_dirs
++
++        os.environ['PYTHONNOUSERSITE'] = '1'
++
++        return self.path
++
++    def __exit__(self, exc_type, exc_val, exc_tb):
++        if not self._no_clean:
++            self._temp_dir.cleanup()
++
++        def restore_var(varname, old_value):
++            if old_value is None:
++                os.environ.pop(varname, None)
++            else:
++                os.environ[varname] = old_value
++
++        restore_var('PATH', self.save_path)
++        restore_var('PYTHONPATH', self.save_pythonpath)
++        restore_var('PYTHONNOUSERSITE', self.save_nousersite)
++
++    def cleanup(self):
++        self._temp_dir.cleanup()
++
++
++class NoOpBuildEnvironment(BuildEnvironment):
++    """A no-op drop-in replacement for BuildEnvironment
++    """
++
++    def __init__(self, no_clean):
++        pass
++
++    def __enter__(self):
++        pass
++
++    def __exit__(self, exc_type, exc_val, exc_tb):
++        pass
++
++    def cleanup(self):
++        pass
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/pep425tags.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/pep425tags.py	(date 1573549700007)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/pep425tags.py	(date 1573549700007)
+@@ -0,0 +1,317 @@
++"""Generate and work with PEP 425 Compatibility Tags."""
++from __future__ import absolute_import
++
++import distutils.util
++import logging
++import platform
++import re
++import sys
++import sysconfig
++import warnings
++from collections import OrderedDict
++
++import pip._internal.utils.glibc
++
++logger = logging.getLogger(__name__)
++
++_osx_arch_pat = re.compile(r'(.+)_(\d+)_(\d+)_(.+)')
++
++
++def get_config_var(var):
++    try:
++        return sysconfig.get_config_var(var)
++    except IOError as e:  # Issue #1074
++        warnings.warn("{}".format(e), RuntimeWarning)
++        return None
++
++
++def get_abbr_impl():
++    """Return abbreviated implementation name."""
++    if hasattr(sys, 'pypy_version_info'):
++        pyimpl = 'pp'
++    elif sys.platform.startswith('java'):
++        pyimpl = 'jy'
++    elif sys.platform == 'cli':
++        pyimpl = 'ip'
++    else:
++        pyimpl = 'cp'
++    return pyimpl
++
++
++def get_impl_ver():
++    """Return implementation version."""
++    impl_ver = get_config_var("py_version_nodot")
++    if not impl_ver or get_abbr_impl() == 'pp':
++        impl_ver = ''.join(map(str, get_impl_version_info()))
++    return impl_ver
++
++
++def get_impl_version_info():
++    """Return sys.version_info-like tuple for use in decrementing the minor
++    version."""
++    if get_abbr_impl() == 'pp':
++        # as per https://github.com/pypa/pip/issues/2882
++        return (sys.version_info[0], sys.pypy_version_info.major,
++                sys.pypy_version_info.minor)
++    else:
++        return sys.version_info[0], sys.version_info[1]
++
++
++def get_impl_tag():
++    """
++    Returns the Tag for this specific implementation.
++    """
++    return "{}{}".format(get_abbr_impl(), get_impl_ver())
++
++
++def get_flag(var, fallback, expected=True, warn=True):
++    """Use a fallback method for determining SOABI flags if the needed config
++    var is unset or unavailable."""
++    val = get_config_var(var)
++    if val is None:
++        if warn:
++            logger.debug("Config variable '%s' is unset, Python ABI tag may "
++                         "be incorrect", var)
++        return fallback()
++    return val == expected
++
++
++def get_abi_tag():
++    """Return the ABI tag based on SOABI (if available) or emulate SOABI
++    (CPython 2, PyPy)."""
++    soabi = get_config_var('SOABI')
++    impl = get_abbr_impl()
++    if not soabi and impl in {'cp', 'pp'} and hasattr(sys, 'maxunicode'):
++        d = ''
++        m = ''
++        u = ''
++        if get_flag('Py_DEBUG',
++                    lambda: hasattr(sys, 'gettotalrefcount'),
++                    warn=(impl == 'cp')):
++            d = 'd'
++        if get_flag('WITH_PYMALLOC',
++                    lambda: impl == 'cp',
++                    warn=(impl == 'cp')):
++            m = 'm'
++        if get_flag('Py_UNICODE_SIZE',
++                    lambda: sys.maxunicode == 0x10ffff,
++                    expected=4,
++                    warn=(impl == 'cp' and
++                          sys.version_info < (3, 3))) \
++                and sys.version_info < (3, 3):
++            u = 'u'
++        abi = '%s%s%s%s%s' % (impl, get_impl_ver(), d, m, u)
++    elif soabi and soabi.startswith('cpython-'):
++        abi = 'cp' + soabi.split('-')[1]
++    elif soabi:
++        abi = soabi.replace('.', '_').replace('-', '_')
++    else:
++        abi = None
++    return abi
++
++
++def _is_running_32bit():
++    return sys.maxsize == 2147483647
++
++
++def get_platform():
++    """Return our platform name 'win32', 'linux_x86_64'"""
++    if sys.platform == 'darwin':
++        # distutils.util.get_platform() returns the release based on the value
++        # of MACOSX_DEPLOYMENT_TARGET on which Python was built, which may
++        # be significantly older than the user's current machine.
++        release, _, machine = platform.mac_ver()
++        split_ver = release.split('.')
++
++        if machine == "x86_64" and _is_running_32bit():
++            machine = "i386"
++        elif machine == "ppc64" and _is_running_32bit():
++            machine = "ppc"
++
++        return 'macosx_{}_{}_{}'.format(split_ver[0], split_ver[1], machine)
++
++    # XXX remove distutils dependency
++    result = distutils.util.get_platform().replace('.', '_').replace('-', '_')
++    if result == "linux_x86_64" and _is_running_32bit():
++        # 32 bit Python program (running on a 64 bit Linux): pip should only
++        # install and run 32 bit compiled extensions in that case.
++        result = "linux_i686"
++
++    return result
++
++
++def is_manylinux1_compatible():
++    # Only Linux, and only x86-64 / i686
++    if get_platform() not in {"linux_x86_64", "linux_i686"}:
++        return False
++
++    # Check for presence of _manylinux module
++    try:
++        import _manylinux
++        return bool(_manylinux.manylinux1_compatible)
++    except (ImportError, AttributeError):
++        # Fall through to heuristic check below
++        pass
++
++    # Check glibc version. CentOS 5 uses glibc 2.5.
++    return pip._internal.utils.glibc.have_compatible_glibc(2, 5)
++
++
++def get_darwin_arches(major, minor, machine):
++    """Return a list of supported arches (including group arches) for
++    the given major, minor and machine architecture of an macOS machine.
++    """
++    arches = []
++
++    def _supports_arch(major, minor, arch):
++        # Looking at the application support for macOS versions in the chart
++        # provided by https://en.wikipedia.org/wiki/OS_X#Versions it appears
++        # our timeline looks roughly like:
++        #
++        # 10.0 - Introduces ppc support.
++        # 10.4 - Introduces ppc64, i386, and x86_64 support, however the ppc64
++        #        and x86_64 support is CLI only, and cannot be used for GUI
++        #        applications.
++        # 10.5 - Extends ppc64 and x86_64 support to cover GUI applications.
++        # 10.6 - Drops support for ppc64
++        # 10.7 - Drops support for ppc
++        #
++        # Given that we do not know if we're installing a CLI or a GUI
++        # application, we must be conservative and assume it might be a GUI
++        # application and behave as if ppc64 and x86_64 support did not occur
++        # until 10.5.
++        #
++        # Note: The above information is taken from the "Application support"
++        #       column in the chart not the "Processor support" since I believe
++        #       that we care about what instruction sets an application can use
++        #       not which processors the OS supports.
++        if arch == 'ppc':
++            return (major, minor) <= (10, 5)
++        if arch == 'ppc64':
++            return (major, minor) == (10, 5)
++        if arch == 'i386':
++            return (major, minor) >= (10, 4)
++        if arch == 'x86_64':
++            return (major, minor) >= (10, 5)
++        if arch in groups:
++            for garch in groups[arch]:
++                if _supports_arch(major, minor, garch):
++                    return True
++        return False
++
++    groups = OrderedDict([
++        ("fat", ("i386", "ppc")),
++        ("intel", ("x86_64", "i386")),
++        ("fat64", ("x86_64", "ppc64")),
++        ("fat32", ("x86_64", "i386", "ppc")),
++    ])
++
++    if _supports_arch(major, minor, machine):
++        arches.append(machine)
++
++    for garch in groups:
++        if machine in groups[garch] and _supports_arch(major, minor, garch):
++            arches.append(garch)
++
++    arches.append('universal')
++
++    return arches
++
++
++def get_supported(versions=None, noarch=False, platform=None,
++                  impl=None, abi=None):
++    """Return a list of supported tags for each version specified in
++    `versions`.
++
++    :param versions: a list of string versions, of the form ["33", "32"],
++        or None. The first version will be assumed to support our ABI.
++    :param platform: specify the exact platform you want valid
++        tags for, or None. If None, use the local system platform.
++    :param impl: specify the exact implementation you want valid
++        tags for, or None. If None, use the local interpreter impl.
++    :param abi: specify the exact abi you want valid
++        tags for, or None. If None, use the local interpreter abi.
++    """
++    supported = []
++
++    # Versions must be given with respect to the preference
++    if versions is None:
++        versions = []
++        version_info = get_impl_version_info()
++        major = version_info[:-1]
++        # Support all previous minor Python versions.
++        for minor in range(version_info[-1], -1, -1):
++            versions.append(''.join(map(str, major + (minor,))))
++
++    impl = impl or get_abbr_impl()
++
++    abis = []
++
++    abi = abi or get_abi_tag()
++    if abi:
++        abis[0:0] = [abi]
++
++    abi3s = set()
++    import imp
++    for suffix in imp.get_suffixes():
++        if suffix[0].startswith('.abi'):
++            abi3s.add(suffix[0].split('.', 2)[1])
++
++    abis.extend(sorted(list(abi3s)))
++
++    abis.append('none')
++
++    if not noarch:
++        arch = platform or get_platform()
++        if arch.startswith('macosx'):
++            # support macosx-10.6-intel on macosx-10.9-x86_64
++            match = _osx_arch_pat.match(arch)
++            if match:
++                name, major, minor, actual_arch = match.groups()
++                tpl = '{}_{}_%i_%s'.format(name, major)
++                arches = []
++                for m in reversed(range(int(minor) + 1)):
++                    for a in get_darwin_arches(int(major), m, actual_arch):
++                        arches.append(tpl % (m, a))
++            else:
++                # arch pattern didn't match (?!)
++                arches = [arch]
++        elif platform is None and is_manylinux1_compatible():
++            arches = [arch.replace('linux', 'manylinux1'), arch]
++        else:
++            arches = [arch]
++
++        # Current version, current API (built specifically for our Python):
++        for abi in abis:
++            for arch in arches:
++                supported.append(('%s%s' % (impl, versions[0]), abi, arch))
++
++        # abi3 modules compatible with older version of Python
++        for version in versions[1:]:
++            # abi3 was introduced in Python 3.2
++            if version in {'31', '30'}:
++                break
++            for abi in abi3s:   # empty set if not Python 3
++                for arch in arches:
++                    supported.append(("%s%s" % (impl, version), abi, arch))
++
++        # Has binaries, does not use the Python API:
++        for arch in arches:
++            supported.append(('py%s' % (versions[0][0]), 'none', arch))
++
++    # No abi / arch, but requires our implementation:
++    supported.append(('%s%s' % (impl, versions[0]), 'none', 'any'))
++    # Tagged specifically as being cross-version compatible
++    # (with just the major version specified)
++    supported.append(('%s%s' % (impl, versions[0][0]), 'none', 'any'))
++
++    # No abi / arch, generic Python
++    for i, version in enumerate(versions):
++        supported.append(('py%s' % (version,), 'none', 'any'))
++        if i == 0:
++            supported.append(('py%s' % (version[0]), 'none', 'any'))
++
++    return supported
++
++
++implementation_tag = get_impl_tag()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/configuration.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/configuration.py	(date 1573549699964)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/configuration.py	(date 1573549699964)
+@@ -0,0 +1,378 @@
++"""Configuration management setup
++
++Some terminology:
++- name
++  As written in config files.
++- value
++  Value associated with a name
++- key
++  Name combined with it's section (section.name)
++- variant
++  A single word describing where the configuration key-value pair came from
++"""
++
++import locale
++import logging
++import os
++
++from pip._vendor import six
++from pip._vendor.six.moves import configparser
++
++from pip._internal.exceptions import ConfigurationError
++from pip._internal.locations import (
++    legacy_config_file, new_config_file, running_under_virtualenv,
++    site_config_files, venv_config_file,
++)
++from pip._internal.utils.misc import ensure_dir, enum
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++
++if MYPY_CHECK_RUNNING:
++    from typing import Any, Dict, Iterable, List, NewType, Optional, Tuple
++
++    RawConfigParser = configparser.RawConfigParser  # Shorthand
++    Kind = NewType("Kind", str)
++
++logger = logging.getLogger(__name__)
++
++
++# NOTE: Maybe use the optionx attribute to normalize keynames.
++def _normalize_name(name):
++    # type: (str) -> str
++    """Make a name consistent regardless of source (environment or file)
++    """
++    name = name.lower().replace('_', '-')
++    if name.startswith('--'):
++        name = name[2:]  # only prefer long opts
++    return name
++
++
++def _disassemble_key(name):
++    # type: (str) -> List[str]
++    return name.split(".", 1)
++
++
++# The kinds of configurations there are.
++kinds = enum(
++    USER="user",        # User Specific
++    GLOBAL="global",    # System Wide
++    VENV="venv",        # Virtual Environment Specific
++    ENV="env",          # from PIP_CONFIG_FILE
++    ENV_VAR="env-var",  # from Environment Variables
++)
++
++
++class Configuration(object):
++    """Handles management of configuration.
++
++    Provides an interface to accessing and managing configuration files.
++
++    This class converts provides an API that takes "section.key-name" style
++    keys and stores the value associated with it as "key-name" under the
++    section "section".
++
++    This allows for a clean interface wherein the both the section and the
++    key-name are preserved in an easy to manage form in the configuration files
++    and the data stored is also nice.
++    """
++
++    def __init__(self, isolated, load_only=None):
++        # type: (bool, Kind) -> None
++        super(Configuration, self).__init__()
++
++        _valid_load_only = [kinds.USER, kinds.GLOBAL, kinds.VENV, None]
++        if load_only not in _valid_load_only:
++            raise ConfigurationError(
++                "Got invalid value for load_only - should be one of {}".format(
++                    ", ".join(map(repr, _valid_load_only[:-1]))
++                )
++            )
++        self.isolated = isolated  # type: bool
++        self.load_only = load_only  # type: Optional[Kind]
++
++        # The order here determines the override order.
++        self._override_order = [
++            kinds.GLOBAL, kinds.USER, kinds.VENV, kinds.ENV, kinds.ENV_VAR
++        ]
++
++        self._ignore_env_names = ["version", "help"]
++
++        # Because we keep track of where we got the data from
++        self._parsers = {
++            variant: [] for variant in self._override_order
++        }  # type: Dict[Kind, List[Tuple[str, RawConfigParser]]]
++        self._config = {
++            variant: {} for variant in self._override_order
++        }  # type: Dict[Kind, Dict[str, Any]]
++        self._modified_parsers = []  # type: List[Tuple[str, RawConfigParser]]
++
++    def load(self):
++        # type: () -> None
++        """Loads configuration from configuration files and environment
++        """
++        self._load_config_files()
++        if not self.isolated:
++            self._load_environment_vars()
++
++    def get_file_to_edit(self):
++        # type: () -> Optional[str]
++        """Returns the file with highest priority in configuration
++        """
++        assert self.load_only is not None, \
++            "Need to be specified a file to be editing"
++
++        try:
++            return self._get_parser_to_modify()[0]
++        except IndexError:
++            return None
++
++    def items(self):
++        # type: () -> Iterable[Tuple[str, Any]]
++        """Returns key-value pairs like dict.items() representing the loaded
++        configuration
++        """
++        return self._dictionary.items()
++
++    def get_value(self, key):
++        # type: (str) -> Any
++        """Get a value from the configuration.
++        """
++        try:
++            return self._dictionary[key]
++        except KeyError:
++            raise ConfigurationError("No such key - {}".format(key))
++
++    def set_value(self, key, value):
++        # type: (str, Any) -> None
++        """Modify a value in the configuration.
++        """
++        self._ensure_have_load_only()
++
++        fname, parser = self._get_parser_to_modify()
++
++        if parser is not None:
++            section, name = _disassemble_key(key)
++
++            # Modify the parser and the configuration
++            if not parser.has_section(section):
++                parser.add_section(section)
++            parser.set(section, name, value)
++
++        self._config[self.load_only][key] = value
++        self._mark_as_modified(fname, parser)
++
++    def unset_value(self, key):
++        # type: (str) -> None
++        """Unset a value in the configuration.
++        """
++        self._ensure_have_load_only()
++
++        if key not in self._config[self.load_only]:
++            raise ConfigurationError("No such key - {}".format(key))
++
++        fname, parser = self._get_parser_to_modify()
++
++        if parser is not None:
++            section, name = _disassemble_key(key)
++
++            # Remove the key in the parser
++            modified_something = False
++            if parser.has_section(section):
++                # Returns whether the option was removed or not
++                modified_something = parser.remove_option(section, name)
++
++            if modified_something:
++                # name removed from parser, section may now be empty
++                section_iter = iter(parser.items(section))
++                try:
++                    val = six.next(section_iter)
++                except StopIteration:
++                    val = None
++
++                if val is None:
++                    parser.remove_section(section)
++
++                self._mark_as_modified(fname, parser)
++            else:
++                raise ConfigurationError(
++                    "Fatal Internal error [id=1]. Please report as a bug."
++                )
++
++        del self._config[self.load_only][key]
++
++    def save(self):
++        # type: () -> None
++        """Save the currentin-memory state.
++        """
++        self._ensure_have_load_only()
++
++        for fname, parser in self._modified_parsers:
++            logger.info("Writing to %s", fname)
++
++            # Ensure directory exists.
++            ensure_dir(os.path.dirname(fname))
++
++            with open(fname, "w") as f:
++                parser.write(f)  # type: ignore
++
++    #
++    # Private routines
++    #
++
++    def _ensure_have_load_only(self):
++        # type: () -> None
++        if self.load_only is None:
++            raise ConfigurationError("Needed a specific file to be modifying.")
++        logger.debug("Will be working with %s variant only", self.load_only)
++
++    @property
++    def _dictionary(self):
++        # type: () -> Dict[str, Any]
++        """A dictionary representing the loaded configuration.
++        """
++        # NOTE: Dictionaries are not populated if not loaded. So, conditionals
++        #       are not needed here.
++        retval = {}
++
++        for variant in self._override_order:
++            retval.update(self._config[variant])
++
++        return retval
++
++    def _load_config_files(self):
++        # type: () -> None
++        """Loads configuration from configuration files
++        """
++        config_files = dict(self._iter_config_files())
++        if config_files[kinds.ENV][0:1] == [os.devnull]:
++            logger.debug(
++                "Skipping loading configuration files due to "
++                "environment's PIP_CONFIG_FILE being os.devnull"
++            )
++            return
++
++        for variant, files in config_files.items():
++            for fname in files:
++                # If there's specific variant set in `load_only`, load only
++                # that variant, not the others.
++                if self.load_only is not None and variant != self.load_only:
++                    logger.debug(
++                        "Skipping file '%s' (variant: %s)", fname, variant
++                    )
++                    continue
++
++                parser = self._load_file(variant, fname)
++
++                # Keeping track of the parsers used
++                self._parsers[variant].append((fname, parser))
++
++    def _load_file(self, variant, fname):
++        # type: (Kind, str) -> RawConfigParser
++        logger.debug("For variant '%s', will try loading '%s'", variant, fname)
++        parser = self._construct_parser(fname)
++
++        for section in parser.sections():
++            items = parser.items(section)
++            self._config[variant].update(self._normalized_keys(section, items))
++
++        return parser
++
++    def _construct_parser(self, fname):
++        # type: (str) -> RawConfigParser
++        parser = configparser.RawConfigParser()
++        # If there is no such file, don't bother reading it but create the
++        # parser anyway, to hold the data.
++        # Doing this is useful when modifying and saving files, where we don't
++        # need to construct a parser.
++        if os.path.exists(fname):
++            try:
++                parser.read(fname)
++            except UnicodeDecodeError:
++                raise ConfigurationError((
++                    "ERROR: "
++                    "Configuration file contains invalid %s characters.\n"
++                    "Please fix your configuration, located at %s\n"
++                ) % (locale.getpreferredencoding(False), fname))
++        return parser
++
++    def _load_environment_vars(self):
++        # type: () -> None
++        """Loads configuration from environment variables
++        """
++        self._config[kinds.ENV_VAR].update(
++            self._normalized_keys(":env:", self._get_environ_vars())
++        )
++
++    def _normalized_keys(self, section, items):
++        # type: (str, Iterable[Tuple[str, Any]]) -> Dict[str, Any]
++        """Normalizes items to construct a dictionary with normalized keys.
++
++        This routine is where the names become keys and are made the same
++        regardless of source - configuration files or environment.
++        """
++        normalized = {}
++        for name, val in items:
++            key = section + "." + _normalize_name(name)
++            normalized[key] = val
++        return normalized
++
++    def _get_environ_vars(self):
++        # type: () -> Iterable[Tuple[str, str]]
++        """Returns a generator with all environmental vars with prefix PIP_"""
++        for key, val in os.environ.items():
++            should_be_yielded = (
++                key.startswith("PIP_") and
++                key[4:].lower() not in self._ignore_env_names
++            )
++            if should_be_yielded:
++                yield key[4:].lower(), val
++
++    # XXX: This is patched in the tests.
++    def _iter_config_files(self):
++        # type: () -> Iterable[Tuple[Kind, List[str]]]
++        """Yields variant and configuration files associated with it.
++
++        This should be treated like items of a dictionary.
++        """
++        # SMELL: Move the conditions out of this function
++
++        # environment variables have the lowest priority
++        config_file = os.environ.get('PIP_CONFIG_FILE', None)
++        if config_file is not None:
++            yield kinds.ENV, [config_file]
++        else:
++            yield kinds.ENV, []
++
++        # at the base we have any global configuration
++        yield kinds.GLOBAL, list(site_config_files)
++
++        # per-user configuration next
++        should_load_user_config = not self.isolated and not (
++            config_file and os.path.exists(config_file)
++        )
++        if should_load_user_config:
++            # The legacy config file is overridden by the new config file
++            yield kinds.USER, [legacy_config_file, new_config_file]
++
++        # finally virtualenv configuration first trumping others
++        if running_under_virtualenv():
++            yield kinds.VENV, [venv_config_file]
++
++    def _get_parser_to_modify(self):
++        # type: () -> Tuple[str, RawConfigParser]
++        # Determine which parser to modify
++        parsers = self._parsers[self.load_only]
++        if not parsers:
++            # This should not happen if everything works correctly.
++            raise ConfigurationError(
++                "Fatal Internal error [id=2]. Please report as a bug."
++            )
++
++        # Use the highest priority parser.
++        return parsers[-1]
++
++    # XXX: This is patched in the tests.
++    def _mark_as_modified(self, fname, parser):
++        # type: (str, RawConfigParser) -> None
++        file_parser_tuple = (fname, parser)
++        if file_parser_tuple not in self._modified_parsers:
++            self._modified_parsers.append(file_parser_tuple)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/cache.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/cache.py	(date 1573549699944)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/cache.py	(date 1573549699944)
+@@ -0,0 +1,202 @@
++"""Cache Management
++"""
++
++import errno
++import hashlib
++import logging
++import os
++
++from pip._vendor.packaging.utils import canonicalize_name
++
++from pip._internal import index
++from pip._internal.compat import expanduser
++from pip._internal.download import path_to_url
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.wheel import InvalidWheelFilename, Wheel
++
++logger = logging.getLogger(__name__)
++
++
++class Cache(object):
++    """An abstract class - provides cache directories for data from links
++
++
++        :param cache_dir: The root of the cache.
++        :param format_control: A pip.index.FormatControl object to limit
++            binaries being read from the cache.
++        :param allowed_formats: which formats of files the cache should store.
++            ('binary' and 'source' are the only allowed values)
++    """
++
++    def __init__(self, cache_dir, format_control, allowed_formats):
++        super(Cache, self).__init__()
++        self.cache_dir = expanduser(cache_dir) if cache_dir else None
++        self.format_control = format_control
++        self.allowed_formats = allowed_formats
++
++        _valid_formats = {"source", "binary"}
++        assert self.allowed_formats.union(_valid_formats) == _valid_formats
++
++    def _get_cache_path_parts(self, link):
++        """Get parts of part that must be os.path.joined with cache_dir
++        """
++
++        # We want to generate an url to use as our cache key, we don't want to
++        # just re-use the URL because it might have other items in the fragment
++        # and we don't care about those.
++        key_parts = [link.url_without_fragment]
++        if link.hash_name is not None and link.hash is not None:
++            key_parts.append("=".join([link.hash_name, link.hash]))
++        key_url = "#".join(key_parts)
++
++        # Encode our key url with sha224, we'll use this because it has similar
++        # security properties to sha256, but with a shorter total output (and
++        # thus less secure). However the differences don't make a lot of
++        # difference for our use case here.
++        hashed = hashlib.sha224(key_url.encode()).hexdigest()
++
++        # We want to nest the directories some to prevent having a ton of top
++        # level directories where we might run out of sub directories on some
++        # FS.
++        parts = [hashed[:2], hashed[2:4], hashed[4:6], hashed[6:]]
++
++        return parts
++
++    def _get_candidates(self, link, package_name):
++        can_not_cache = (
++            not self.cache_dir or
++            not package_name or
++            not link
++        )
++        if can_not_cache:
++            return []
++
++        canonical_name = canonicalize_name(package_name)
++        formats = index.fmt_ctl_formats(
++            self.format_control, canonical_name
++        )
++        if not self.allowed_formats.intersection(formats):
++            return []
++
++        root = self.get_path_for_link(link)
++        try:
++            return os.listdir(root)
++        except OSError as err:
++            if err.errno in {errno.ENOENT, errno.ENOTDIR}:
++                return []
++            raise
++
++    def get_path_for_link(self, link):
++        """Return a directory to store cached items in for link.
++        """
++        raise NotImplementedError()
++
++    def get(self, link, package_name):
++        """Returns a link to a cached item if it exists, otherwise returns the
++        passed link.
++        """
++        raise NotImplementedError()
++
++    def _link_for_candidate(self, link, candidate):
++        root = self.get_path_for_link(link)
++        path = os.path.join(root, candidate)
++
++        return index.Link(path_to_url(path))
++
++    def cleanup(self):
++        pass
++
++
++class SimpleWheelCache(Cache):
++    """A cache of wheels for future installs.
++    """
++
++    def __init__(self, cache_dir, format_control):
++        super(SimpleWheelCache, self).__init__(
++            cache_dir, format_control, {"binary"}
++        )
++
++    def get_path_for_link(self, link):
++        """Return a directory to store cached wheels for link
++
++        Because there are M wheels for any one sdist, we provide a directory
++        to cache them in, and then consult that directory when looking up
++        cache hits.
++
++        We only insert things into the cache if they have plausible version
++        numbers, so that we don't contaminate the cache with things that were
++        not unique. E.g. ./package might have dozens of installs done for it
++        and build a version of 0.0...and if we built and cached a wheel, we'd
++        end up using the same wheel even if the source has been edited.
++
++        :param link: The link of the sdist for which this will cache wheels.
++        """
++        parts = self._get_cache_path_parts(link)
++
++        # Store wheels within the root cache_dir
++        return os.path.join(self.cache_dir, "wheels", *parts)
++
++    def get(self, link, package_name):
++        candidates = []
++
++        for wheel_name in self._get_candidates(link, package_name):
++            try:
++                wheel = Wheel(wheel_name)
++            except InvalidWheelFilename:
++                continue
++            if not wheel.supported():
++                # Built for a different python/arch/etc
++                continue
++            candidates.append((wheel.support_index_min(), wheel_name))
++
++        if not candidates:
++            return link
++
++        return self._link_for_candidate(link, min(candidates)[1])
++
++
++class EphemWheelCache(SimpleWheelCache):
++    """A SimpleWheelCache that creates it's own temporary cache directory
++    """
++
++    def __init__(self, format_control):
++        self._temp_dir = TempDirectory(kind="ephem-wheel-cache")
++        self._temp_dir.create()
++
++        super(EphemWheelCache, self).__init__(
++            self._temp_dir.path, format_control
++        )
++
++    def cleanup(self):
++        self._temp_dir.cleanup()
++
++
++class WheelCache(Cache):
++    """Wraps EphemWheelCache and SimpleWheelCache into a single Cache
++
++    This Cache allows for gracefully degradation, using the ephem wheel cache
++    when a certain link is not found in the simple wheel cache first.
++    """
++
++    def __init__(self, cache_dir, format_control):
++        super(WheelCache, self).__init__(
++            cache_dir, format_control, {'binary'}
++        )
++        self._wheel_cache = SimpleWheelCache(cache_dir, format_control)
++        self._ephem_cache = EphemWheelCache(format_control)
++
++    def get_path_for_link(self, link):
++        return self._wheel_cache.get_path_for_link(link)
++
++    def get_ephem_path_for_link(self, link):
++        return self._ephem_cache.get_path_for_link(link)
++
++    def get(self, link, package_name):
++        retval = self._wheel_cache.get(link, package_name)
++        if retval is link:
++            retval = self._ephem_cache.get(link, package_name)
++        return retval
++
++    def cleanup(self):
++        self._wheel_cache.cleanup()
++        self._ephem_cache.cleanup()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/compat.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/compat.py	(date 1573549699956)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/compat.py	(date 1573549699956)
+@@ -0,0 +1,235 @@
++"""Stuff that differs in different Python versions and platform
++distributions."""
++from __future__ import absolute_import, division
++
++import codecs
++import locale
++import logging
++import os
++import shutil
++import sys
++
++from pip._vendor.six import text_type
++
++try:
++    import ipaddress
++except ImportError:
++    try:
++        from pip._vendor import ipaddress  # type: ignore
++    except ImportError:
++        import ipaddr as ipaddress  # type: ignore
++        ipaddress.ip_address = ipaddress.IPAddress
++        ipaddress.ip_network = ipaddress.IPNetwork
++
++
++__all__ = [
++    "ipaddress", "uses_pycache", "console_to_str", "native_str",
++    "get_path_uid", "stdlib_pkgs", "WINDOWS", "samefile", "get_terminal_size",
++]
++
++
++logger = logging.getLogger(__name__)
++
++if sys.version_info >= (3, 4):
++    uses_pycache = True
++    from importlib.util import cache_from_source
++else:
++    import imp
++
++    try:
++        cache_from_source = imp.cache_from_source  # type: ignore
++    except AttributeError:
++        # does not use __pycache__
++        cache_from_source = None
++
++    uses_pycache = cache_from_source is not None
++
++
++if sys.version_info >= (3, 5):
++    backslashreplace_decode = "backslashreplace"
++else:
++    # In version 3.4 and older, backslashreplace exists
++    # but does not support use for decoding.
++    # We implement our own replace handler for this
++    # situation, so that we can consistently use
++    # backslash replacement for all versions.
++    def backslashreplace_decode_fn(err):
++        raw_bytes = (err.object[i] for i in range(err.start, err.end))
++        if sys.version_info[0] == 2:
++            # Python 2 gave us characters - convert to numeric bytes
++            raw_bytes = (ord(b) for b in raw_bytes)
++        return u"".join(u"\\x%x" % c for c in raw_bytes), err.end
++    codecs.register_error(
++        "backslashreplace_decode",
++        backslashreplace_decode_fn,
++    )
++    backslashreplace_decode = "backslashreplace_decode"
++
++
++def console_to_str(data):
++    """Return a string, safe for output, of subprocess output.
++
++    We assume the data is in the locale preferred encoding.
++    If it won't decode properly, we warn the user but decode as
++    best we can.
++
++    We also ensure that the output can be safely written to
++    standard output without encoding errors.
++    """
++
++    # First, get the encoding we assume. This is the preferred
++    # encoding for the locale, unless that is not found, or
++    # it is ASCII, in which case assume UTF-8
++    encoding = locale.getpreferredencoding()
++    if (not encoding) or codecs.lookup(encoding).name == "ascii":
++        encoding = "utf-8"
++
++    # Now try to decode the data - if we fail, warn the user and
++    # decode with replacement.
++    try:
++        s = data.decode(encoding)
++    except UnicodeDecodeError:
++        logger.warning(
++            "Subprocess output does not appear to be encoded as %s",
++            encoding,
++        )
++        s = data.decode(encoding, errors=backslashreplace_decode)
++
++    # Make sure we can print the output, by encoding it to the output
++    # encoding with replacement of unencodable characters, and then
++    # decoding again.
++    # We use stderr's encoding because it's less likely to be
++    # redirected and if we don't find an encoding we skip this
++    # step (on the assumption that output is wrapped by something
++    # that won't fail).
++    # The double getattr is to deal with the possibility that we're
++    # being called in a situation where sys.__stderr__ doesn't exist,
++    # or doesn't have an encoding attribute. Neither of these cases
++    # should occur in normal pip use, but there's no harm in checking
++    # in case people use pip in (unsupported) unusual situations.
++    output_encoding = getattr(getattr(sys, "__stderr__", None),
++                              "encoding", None)
++
++    if output_encoding:
++        s = s.encode(output_encoding, errors="backslashreplace")
++        s = s.decode(output_encoding)
++
++    return s
++
++
++if sys.version_info >= (3,):
++    def native_str(s, replace=False):
++        if isinstance(s, bytes):
++            return s.decode('utf-8', 'replace' if replace else 'strict')
++        return s
++
++else:
++    def native_str(s, replace=False):
++        # Replace is ignored -- unicode to UTF-8 can't fail
++        if isinstance(s, text_type):
++            return s.encode('utf-8')
++        return s
++
++
++def get_path_uid(path):
++    """
++    Return path's uid.
++
++    Does not follow symlinks:
++        https://github.com/pypa/pip/pull/935#discussion_r5307003
++
++    Placed this function in compat due to differences on AIX and
++    Jython, that should eventually go away.
++
++    :raises OSError: When path is a symlink or can't be read.
++    """
++    if hasattr(os, 'O_NOFOLLOW'):
++        fd = os.open(path, os.O_RDONLY | os.O_NOFOLLOW)
++        file_uid = os.fstat(fd).st_uid
++        os.close(fd)
++    else:  # AIX and Jython
++        # WARNING: time of check vulnerability, but best we can do w/o NOFOLLOW
++        if not os.path.islink(path):
++            # older versions of Jython don't have `os.fstat`
++            file_uid = os.stat(path).st_uid
++        else:
++            # raise OSError for parity with os.O_NOFOLLOW above
++            raise OSError(
++                "%s is a symlink; Will not return uid for symlinks" % path
++            )
++    return file_uid
++
++
++def expanduser(path):
++    """
++    Expand ~ and ~user constructions.
++
++    Includes a workaround for http://bugs.python.org/issue14768
++    """
++    expanded = os.path.expanduser(path)
++    if path.startswith('~/') and expanded.startswith('//'):
++        expanded = expanded[1:]
++    return expanded
++
++
++# packages in the stdlib that may have installation metadata, but should not be
++# considered 'installed'.  this theoretically could be determined based on
++# dist.location (py27:`sysconfig.get_paths()['stdlib']`,
++# py26:sysconfig.get_config_vars('LIBDEST')), but fear platform variation may
++# make this ineffective, so hard-coding
++stdlib_pkgs = {"python", "wsgiref", "argparse"}
++
++
++# windows detection, covers cpython and ironpython
++WINDOWS = (sys.platform.startswith("win") or
++           (sys.platform == 'cli' and os.name == 'nt'))
++
++
++def samefile(file1, file2):
++    """Provide an alternative for os.path.samefile on Windows/Python2"""
++    if hasattr(os.path, 'samefile'):
++        return os.path.samefile(file1, file2)
++    else:
++        path1 = os.path.normcase(os.path.abspath(file1))
++        path2 = os.path.normcase(os.path.abspath(file2))
++        return path1 == path2
++
++
++if hasattr(shutil, 'get_terminal_size'):
++    def get_terminal_size():
++        """
++        Returns a tuple (x, y) representing the width(x) and the height(y)
++        in characters of the terminal window.
++        """
++        return tuple(shutil.get_terminal_size())
++else:
++    def get_terminal_size():
++        """
++        Returns a tuple (x, y) representing the width(x) and the height(y)
++        in characters of the terminal window.
++        """
++        def ioctl_GWINSZ(fd):
++            try:
++                import fcntl
++                import termios
++                import struct
++                cr = struct.unpack_from(
++                    'hh',
++                    fcntl.ioctl(fd, termios.TIOCGWINSZ, '12345678')
++                )
++            except:
++                return None
++            if cr == (0, 0):
++                return None
++            return cr
++        cr = ioctl_GWINSZ(0) or ioctl_GWINSZ(1) or ioctl_GWINSZ(2)
++        if not cr:
++            try:
++                fd = os.open(os.ctermid(), os.O_RDONLY)
++                cr = ioctl_GWINSZ(fd)
++                os.close(fd)
++            except:
++                pass
++        if not cr:
++            cr = (os.environ.get('LINES', 25), os.environ.get('COLUMNS', 80))
++        return int(cr[1]), int(cr[0])
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/__init__.py	(date 1573549699912)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/__init__.py	(date 1573549699912)
+@@ -0,0 +1,246 @@
++#!/usr/bin/env python
++from __future__ import absolute_import
++
++import locale
++import logging
++import os
++import optparse
++import warnings
++
++import sys
++
++# 2016-06-17 barry@debian.org: urllib3 1.14 added optional support for socks,
++# but if invoked (i.e. imported), it will issue a warning to stderr if socks
++# isn't available.  requests unconditionally imports urllib3's socks contrib
++# module, triggering this warning.  The warning breaks DEP-8 tests (because of
++# the stderr output) and is just plain annoying in normal usage.  I don't want
++# to add socks as yet another dependency for pip, nor do I want to allow-stder
++# in the DEP-8 tests, so just suppress the warning.  pdb tells me this has to
++# be done before the import of pip.vcs.
++from pip._vendor.urllib3.exceptions import DependencyWarning
++warnings.filterwarnings("ignore", category=DependencyWarning)  # noqa
++
++# We want to inject the use of SecureTransport as early as possible so that any
++# references or sessions or what have you are ensured to have it, however we
++# only want to do this in the case that we're running on macOS and the linked
++# OpenSSL is too old to handle TLSv1.2
++try:
++    import ssl
++except ImportError:
++    pass
++else:
++    # Checks for OpenSSL 1.0.1 on MacOS
++    if sys.platform == "darwin" and ssl.OPENSSL_VERSION_NUMBER < 0x1000100f:
++        try:
++            from pip._vendor.urllib3.contrib import securetransport
++        except (ImportError, OSError):
++            pass
++        else:
++            securetransport.inject_into_urllib3()
++
++from pip import __version__
++from pip._internal import cmdoptions
++from pip._internal.exceptions import CommandError, PipError
++from pip._internal.utils.misc import get_installed_distributions, get_prog
++from pip._internal.utils import deprecation
++from pip._internal.vcs import git, mercurial, subversion, bazaar  # noqa
++from pip._internal.baseparser import (
++    ConfigOptionParser, UpdatingDefaultsHelpFormatter,
++)
++from pip._internal.commands import get_summaries, get_similar_commands
++from pip._internal.commands import commands_dict
++from pip._vendor.urllib3.exceptions import InsecureRequestWarning
++
++logger = logging.getLogger(__name__)
++
++# Hide the InsecureRequestWarning from urllib3
++warnings.filterwarnings("ignore", category=InsecureRequestWarning)
++
++
++def autocomplete():
++    """Command and option completion for the main option parser (and options)
++    and its subcommands (and options).
++
++    Enable by sourcing one of the completion shell scripts (bash, zsh or fish).
++    """
++    # Don't complete if user hasn't sourced bash_completion file.
++    if 'PIP_AUTO_COMPLETE' not in os.environ:
++        return
++    cwords = os.environ['COMP_WORDS'].split()[1:]
++    cword = int(os.environ['COMP_CWORD'])
++    try:
++        current = cwords[cword - 1]
++    except IndexError:
++        current = ''
++
++    subcommands = [cmd for cmd, summary in get_summaries()]
++    options = []
++    # subcommand
++    try:
++        subcommand_name = [w for w in cwords if w in subcommands][0]
++    except IndexError:
++        subcommand_name = None
++
++    parser = create_main_parser()
++    # subcommand options
++    if subcommand_name:
++        # special case: 'help' subcommand has no options
++        if subcommand_name == 'help':
++            sys.exit(1)
++        # special case: list locally installed dists for show and uninstall
++        should_list_installed = (
++            subcommand_name in ['show', 'uninstall'] and
++            not current.startswith('-')
++        )
++        if should_list_installed:
++            installed = []
++            lc = current.lower()
++            for dist in get_installed_distributions(local_only=True):
++                if dist.key.startswith(lc) and dist.key not in cwords[1:]:
++                    installed.append(dist.key)
++            # if there are no dists installed, fall back to option completion
++            if installed:
++                for dist in installed:
++                    print(dist)
++                sys.exit(1)
++
++        subcommand = commands_dict[subcommand_name]()
++
++        for opt in subcommand.parser.option_list_all:
++            if opt.help != optparse.SUPPRESS_HELP:
++                for opt_str in opt._long_opts + opt._short_opts:
++                    options.append((opt_str, opt.nargs))
++
++        # filter out previously specified options from available options
++        prev_opts = [x.split('=')[0] for x in cwords[1:cword - 1]]
++        options = [(x, v) for (x, v) in options if x not in prev_opts]
++        # filter options by current input
++        options = [(k, v) for k, v in options if k.startswith(current)]
++        for option in options:
++            opt_label = option[0]
++            # append '=' to options which require args
++            if option[1] and option[0][:2] == "--":
++                opt_label += '='
++            print(opt_label)
++    else:
++        # show main parser options only when necessary
++        if current.startswith('-') or current.startswith('--'):
++            opts = [i.option_list for i in parser.option_groups]
++            opts.append(parser.option_list)
++            opts = (o for it in opts for o in it)
++
++            for opt in opts:
++                if opt.help != optparse.SUPPRESS_HELP:
++                    subcommands += opt._long_opts + opt._short_opts
++
++        print(' '.join([x for x in subcommands if x.startswith(current)]))
++    sys.exit(1)
++
++
++def create_main_parser():
++    parser_kw = {
++        'usage': '\n%prog <command> [options]',
++        'add_help_option': False,
++        'formatter': UpdatingDefaultsHelpFormatter(),
++        'name': 'global',
++        'prog': get_prog(),
++    }
++
++    parser = ConfigOptionParser(**parser_kw)
++    parser.disable_interspersed_args()
++
++    pip_pkg_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
++    parser.version = 'pip %s from %s (python %s)' % (
++        __version__, pip_pkg_dir, sys.version[:3],
++    )
++
++    # add the general options
++    gen_opts = cmdoptions.make_option_group(cmdoptions.general_group, parser)
++    parser.add_option_group(gen_opts)
++
++    parser.main = True  # so the help formatter knows
++
++    # create command listing for description
++    command_summaries = get_summaries()
++    description = [''] + ['%-27s %s' % (i, j) for i, j in command_summaries]
++    parser.description = '\n'.join(description)
++
++    return parser
++
++
++def parseopts(args):
++    parser = create_main_parser()
++
++    # Note: parser calls disable_interspersed_args(), so the result of this
++    # call is to split the initial args into the general options before the
++    # subcommand and everything else.
++    # For example:
++    #  args: ['--timeout=5', 'install', '--user', 'INITools']
++    #  general_options: ['--timeout==5']
++    #  args_else: ['install', '--user', 'INITools']
++    general_options, args_else = parser.parse_args(args)
++
++    # --version
++    if general_options.version:
++        sys.stdout.write(parser.version)
++        sys.stdout.write(os.linesep)
++        sys.exit()
++
++    # pip || pip help -> print_help()
++    if not args_else or (args_else[0] == 'help' and len(args_else) == 1):
++        parser.print_help()
++        sys.exit()
++
++    # the subcommand name
++    cmd_name = args_else[0]
++
++    if cmd_name not in commands_dict:
++        guess = get_similar_commands(cmd_name)
++
++        msg = ['unknown command "%s"' % cmd_name]
++        if guess:
++            msg.append('maybe you meant "%s"' % guess)
++
++        raise CommandError(' - '.join(msg))
++
++    # all the args without the subcommand
++    cmd_args = args[:]
++    cmd_args.remove(cmd_name)
++
++    return cmd_name, cmd_args
++
++
++def check_isolated(args):
++    isolated = False
++
++    if "--isolated" in args:
++        isolated = True
++
++    return isolated
++
++
++def main(args=None):
++    if args is None:
++        args = sys.argv[1:]
++
++    # Configure our deprecation warnings to be sent through loggers
++    deprecation.install_warning_logger()
++
++    autocomplete()
++
++    try:
++        cmd_name, cmd_args = parseopts(args)
++    except PipError as exc:
++        sys.stderr.write("ERROR: %s" % exc)
++        sys.stderr.write(os.linesep)
++        sys.exit(1)
++
++    # Needed for locale.getpreferredencoding(False) to work
++    # in pip._internal.utils.encoding.auto_decode
++    try:
++        locale.setlocale(locale.LC_ALL, '')
++    except locale.Error as e:
++        # setlocale can apparently crash if locale are uninitialized
++        logger.debug("Ignoring error %s when setting locale", e)
++    command = commands_dict[cmd_name](isolated=check_isolated(cmd_args))
++    return command.main(cmd_args)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/baseparser.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/baseparser.py	(date 1573549699927)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/baseparser.py	(date 1573549699927)
+@@ -0,0 +1,240 @@
++"""Base option parser setup"""
++from __future__ import absolute_import
++
++import logging
++import optparse
++import sys
++import textwrap
++from distutils.util import strtobool
++
++from pip._vendor.six import string_types
++
++from pip._internal.compat import get_terminal_size
++from pip._internal.configuration import Configuration, ConfigurationError
++
++logger = logging.getLogger(__name__)
++
++
++class PrettyHelpFormatter(optparse.IndentedHelpFormatter):
++    """A prettier/less verbose help formatter for optparse."""
++
++    def __init__(self, *args, **kwargs):
++        # help position must be aligned with __init__.parseopts.description
++        kwargs['max_help_position'] = 30
++        kwargs['indent_increment'] = 1
++        kwargs['width'] = get_terminal_size()[0] - 2
++        optparse.IndentedHelpFormatter.__init__(self, *args, **kwargs)
++
++    def format_option_strings(self, option):
++        return self._format_option_strings(option, ' <%s>', ', ')
++
++    def _format_option_strings(self, option, mvarfmt=' <%s>', optsep=', '):
++        """
++        Return a comma-separated list of option strings and metavars.
++
++        :param option:  tuple of (short opt, long opt), e.g: ('-f', '--format')
++        :param mvarfmt: metavar format string - evaluated as mvarfmt % metavar
++        :param optsep:  separator
++        """
++        opts = []
++
++        if option._short_opts:
++            opts.append(option._short_opts[0])
++        if option._long_opts:
++            opts.append(option._long_opts[0])
++        if len(opts) > 1:
++            opts.insert(1, optsep)
++
++        if option.takes_value():
++            metavar = option.metavar or option.dest.lower()
++            opts.append(mvarfmt % metavar.lower())
++
++        return ''.join(opts)
++
++    def format_heading(self, heading):
++        if heading == 'Options':
++            return ''
++        return heading + ':\n'
++
++    def format_usage(self, usage):
++        """
++        Ensure there is only one newline between usage and the first heading
++        if there is no description.
++        """
++        msg = '\nUsage: %s\n' % self.indent_lines(textwrap.dedent(usage), "  ")
++        return msg
++
++    def format_description(self, description):
++        # leave full control over description to us
++        if description:
++            if hasattr(self.parser, 'main'):
++                label = 'Commands'
++            else:
++                label = 'Description'
++            # some doc strings have initial newlines, some don't
++            description = description.lstrip('\n')
++            # some doc strings have final newlines and spaces, some don't
++            description = description.rstrip()
++            # dedent, then reindent
++            description = self.indent_lines(textwrap.dedent(description), "  ")
++            description = '%s:\n%s\n' % (label, description)
++            return description
++        else:
++            return ''
++
++    def format_epilog(self, epilog):
++        # leave full control over epilog to us
++        if epilog:
++            return epilog
++        else:
++            return ''
++
++    def indent_lines(self, text, indent):
++        new_lines = [indent + line for line in text.split('\n')]
++        return "\n".join(new_lines)
++
++
++class UpdatingDefaultsHelpFormatter(PrettyHelpFormatter):
++    """Custom help formatter for use in ConfigOptionParser.
++
++    This is updates the defaults before expanding them, allowing
++    them to show up correctly in the help listing.
++    """
++
++    def expand_default(self, option):
++        if self.parser is not None:
++            self.parser._update_defaults(self.parser.defaults)
++        return optparse.IndentedHelpFormatter.expand_default(self, option)
++
++
++class CustomOptionParser(optparse.OptionParser):
++
++    def insert_option_group(self, idx, *args, **kwargs):
++        """Insert an OptionGroup at a given position."""
++        group = self.add_option_group(*args, **kwargs)
++
++        self.option_groups.pop()
++        self.option_groups.insert(idx, group)
++
++        return group
++
++    @property
++    def option_list_all(self):
++        """Get a list of all options, including those in option groups."""
++        res = self.option_list[:]
++        for i in self.option_groups:
++            res.extend(i.option_list)
++
++        return res
++
++
++class ConfigOptionParser(CustomOptionParser):
++    """Custom option parser which updates its defaults by checking the
++    configuration files and environmental variables"""
++
++    def __init__(self, *args, **kwargs):
++        self.name = kwargs.pop('name')
++
++        isolated = kwargs.pop("isolated", False)
++        self.config = Configuration(isolated)
++
++        assert self.name
++        optparse.OptionParser.__init__(self, *args, **kwargs)
++
++    def check_default(self, option, key, val):
++        try:
++            return option.check_value(key, val)
++        except optparse.OptionValueError as exc:
++            print("An error occurred during configuration: %s" % exc)
++            sys.exit(3)
++
++    def _get_ordered_configuration_items(self):
++        # Configuration gives keys in an unordered manner. Order them.
++        override_order = ["global", self.name, ":env:"]
++
++        # Pool the options into different groups
++        section_items = {name: [] for name in override_order}
++        for section_key, val in self.config.items():
++            # ignore empty values
++            if not val:
++                logger.debug(
++                    "Ignoring configuration key '%s' as it's value is empty.",
++                    section_key
++                )
++                continue
++
++            section, key = section_key.split(".", 1)
++            if section in override_order:
++                section_items[section].append((key, val))
++
++        # Yield each group in their override order
++        for section in override_order:
++            for key, val in section_items[section]:
++                yield key, val
++
++    def _update_defaults(self, defaults):
++        """Updates the given defaults with values from the config files and
++        the environ. Does a little special handling for certain types of
++        options (lists)."""
++
++        # Accumulate complex default state.
++        self.values = optparse.Values(self.defaults)
++        late_eval = set()
++        # Then set the options with those values
++        for key, val in self._get_ordered_configuration_items():
++            # '--' because configuration supports only long names
++            option = self.get_option('--' + key)
++
++            # Ignore options not present in this parser. E.g. non-globals put
++            # in [global] by users that want them to apply to all applicable
++            # commands.
++            if option is None:
++                continue
++
++            if option.action in ('store_true', 'store_false', 'count'):
++                val = strtobool(val)
++            elif option.action == 'append':
++                val = val.split()
++                val = [self.check_default(option, key, v) for v in val]
++            elif option.action == 'callback':
++                late_eval.add(option.dest)
++                opt_str = option.get_opt_string()
++                val = option.convert_value(opt_str, val)
++                # From take_action
++                args = option.callback_args or ()
++                kwargs = option.callback_kwargs or {}
++                option.callback(option, opt_str, val, self, *args, **kwargs)
++            else:
++                val = self.check_default(option, key, val)
++
++            defaults[option.dest] = val
++
++        for key in late_eval:
++            defaults[key] = getattr(self.values, key)
++        self.values = None
++        return defaults
++
++    def get_default_values(self):
++        """Overriding to make updating the defaults after instantiation of
++        the option parser possible, _update_defaults() does the dirty work."""
++        if not self.process_default_values:
++            # Old, pre-Optik 1.5 behaviour.
++            return optparse.Values(self.defaults)
++
++        # Load the configuration, or error out in case of an error
++        try:
++            self.config.load()
++        except ConfigurationError as err:
++            self.exit(2, err.args[0])
++
++        defaults = self._update_defaults(self.defaults.copy())  # ours
++        for option in self._get_all_options():
++            default = defaults.get(option.dest)
++            if isinstance(default, string_types):
++                opt_str = option.get_opt_string()
++                defaults[option.dest] = option.check_value(opt_str, default)
++        return optparse.Values(defaults)
++
++    def error(self, msg):
++        self.print_usage(sys.stderr)
++        self.exit(2, "%s\n" % msg)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_file.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_file.py	(date 1573549700261)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_file.py	(date 1573549700261)
+@@ -0,0 +1,338 @@
++"""
++Requirements file parsing
++"""
++
++from __future__ import absolute_import
++
++import optparse
++import os
++import re
++import shlex
++import sys
++
++from pip._vendor.six.moves import filterfalse
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++
++from pip._internal import cmdoptions
++from pip._internal.download import get_file_content
++from pip._internal.exceptions import RequirementsFileParseError
++from pip._internal.req.req_install import InstallRequirement
++
++__all__ = ['parse_requirements']
++
++SCHEME_RE = re.compile(r'^(http|https|file):', re.I)
++COMMENT_RE = re.compile(r'(^|\s)+#.*$')
++
++# Matches environment variable-style values in '${MY_VARIABLE_1}' with the
++# variable name consisting of only uppercase letters, digits or the '_'
++# (underscore). This follows the POSIX standard defined in IEEE Std 1003.1,
++# 2013 Edition.
++ENV_VAR_RE = re.compile(r'(?P<var>\$\{(?P<name>[A-Z0-9_]+)\})')
++
++SUPPORTED_OPTIONS = [
++    cmdoptions.constraints,
++    cmdoptions.editable,
++    cmdoptions.requirements,
++    cmdoptions.no_index,
++    cmdoptions.index_url,
++    cmdoptions.find_links,
++    cmdoptions.extra_index_url,
++    cmdoptions.always_unzip,
++    cmdoptions.no_binary,
++    cmdoptions.only_binary,
++    cmdoptions.pre,
++    cmdoptions.process_dependency_links,
++    cmdoptions.trusted_host,
++    cmdoptions.require_hashes,
++]
++
++# options to be passed to requirements
++SUPPORTED_OPTIONS_REQ = [
++    cmdoptions.install_options,
++    cmdoptions.global_options,
++    cmdoptions.hash,
++]
++
++# the 'dest' string values
++SUPPORTED_OPTIONS_REQ_DEST = [o().dest for o in SUPPORTED_OPTIONS_REQ]
++
++
++def parse_requirements(filename, finder=None, comes_from=None, options=None,
++                       session=None, constraint=False, wheel_cache=None):
++    """Parse a requirements file and yield InstallRequirement instances.
++
++    :param filename:    Path or url of requirements file.
++    :param finder:      Instance of pip.index.PackageFinder.
++    :param comes_from:  Origin description of requirements.
++    :param options:     cli options.
++    :param session:     Instance of pip.download.PipSession.
++    :param constraint:  If true, parsing a constraint file rather than
++        requirements file.
++    :param wheel_cache: Instance of pip.wheel.WheelCache
++    """
++    if session is None:
++        raise TypeError(
++            "parse_requirements() missing 1 required keyword argument: "
++            "'session'"
++        )
++
++    _, content = get_file_content(
++        filename, comes_from=comes_from, session=session
++    )
++
++    lines_enum = preprocess(content, options)
++
++    for line_number, line in lines_enum:
++        req_iter = process_line(line, filename, line_number, finder,
++                                comes_from, options, session, wheel_cache,
++                                constraint=constraint)
++        for req in req_iter:
++            yield req
++
++
++def preprocess(content, options):
++    """Split, filter, and join lines, and return a line iterator
++
++    :param content: the content of the requirements file
++    :param options: cli options
++    """
++    lines_enum = enumerate(content.splitlines(), start=1)
++    lines_enum = join_lines(lines_enum)
++    lines_enum = ignore_comments(lines_enum)
++    lines_enum = skip_regex(lines_enum, options)
++    lines_enum = expand_env_variables(lines_enum)
++    return lines_enum
++
++
++def process_line(line, filename, line_number, finder=None, comes_from=None,
++                 options=None, session=None, wheel_cache=None,
++                 constraint=False):
++    """Process a single requirements line; This can result in creating/yielding
++    requirements, or updating the finder.
++
++    For lines that contain requirements, the only options that have an effect
++    are from SUPPORTED_OPTIONS_REQ, and they are scoped to the
++    requirement. Other options from SUPPORTED_OPTIONS may be present, but are
++    ignored.
++
++    For lines that do not contain requirements, the only options that have an
++    effect are from SUPPORTED_OPTIONS. Options from SUPPORTED_OPTIONS_REQ may
++    be present, but are ignored. These lines may contain multiple options
++    (although our docs imply only one is supported), and all our parsed and
++    affect the finder.
++
++    :param constraint: If True, parsing a constraints file.
++    :param options: OptionParser options that we may update
++    """
++    parser = build_parser(line)
++    defaults = parser.get_default_values()
++    defaults.index_url = None
++    if finder:
++        # `finder.format_control` will be updated during parsing
++        defaults.format_control = finder.format_control
++    args_str, options_str = break_args_options(line)
++    if sys.version_info < (2, 7, 3):
++        # Prior to 2.7.3, shlex cannot deal with unicode entries
++        options_str = options_str.encode('utf8')
++    opts, _ = parser.parse_args(shlex.split(options_str), defaults)
++
++    # preserve for the nested code path
++    line_comes_from = '%s %s (line %s)' % (
++        '-c' if constraint else '-r', filename, line_number,
++    )
++
++    # yield a line requirement
++    if args_str:
++        isolated = options.isolated_mode if options else False
++        if options:
++            cmdoptions.check_install_build_global(options, opts)
++        # get the options that apply to requirements
++        req_options = {}
++        for dest in SUPPORTED_OPTIONS_REQ_DEST:
++            if dest in opts.__dict__ and opts.__dict__[dest]:
++                req_options[dest] = opts.__dict__[dest]
++        yield InstallRequirement.from_line(
++            args_str, line_comes_from, constraint=constraint,
++            isolated=isolated, options=req_options, wheel_cache=wheel_cache
++        )
++
++    # yield an editable requirement
++    elif opts.editables:
++        isolated = options.isolated_mode if options else False
++        yield InstallRequirement.from_editable(
++            opts.editables[0], comes_from=line_comes_from,
++            constraint=constraint, isolated=isolated, wheel_cache=wheel_cache
++        )
++
++    # parse a nested requirements file
++    elif opts.requirements or opts.constraints:
++        if opts.requirements:
++            req_path = opts.requirements[0]
++            nested_constraint = False
++        else:
++            req_path = opts.constraints[0]
++            nested_constraint = True
++        # original file is over http
++        if SCHEME_RE.search(filename):
++            # do a url join so relative paths work
++            req_path = urllib_parse.urljoin(filename, req_path)
++        # original file and nested file are paths
++        elif not SCHEME_RE.search(req_path):
++            # do a join so relative paths work
++            req_path = os.path.join(os.path.dirname(filename), req_path)
++        # TODO: Why not use `comes_from='-r {} (line {})'` here as well?
++        parser = parse_requirements(
++            req_path, finder, comes_from, options, session,
++            constraint=nested_constraint, wheel_cache=wheel_cache
++        )
++        for req in parser:
++            yield req
++
++    # percolate hash-checking option upward
++    elif opts.require_hashes:
++        options.require_hashes = opts.require_hashes
++
++    # set finder options
++    elif finder:
++        if opts.index_url:
++            finder.index_urls = [opts.index_url]
++        if opts.no_index is True:
++            finder.index_urls = []
++        if opts.extra_index_urls:
++            finder.index_urls.extend(opts.extra_index_urls)
++        if opts.find_links:
++            # FIXME: it would be nice to keep track of the source
++            # of the find_links: support a find-links local path
++            # relative to a requirements file.
++            value = opts.find_links[0]
++            req_dir = os.path.dirname(os.path.abspath(filename))
++            relative_to_reqs_file = os.path.join(req_dir, value)
++            if os.path.exists(relative_to_reqs_file):
++                value = relative_to_reqs_file
++            finder.find_links.append(value)
++        if opts.pre:
++            finder.allow_all_prereleases = True
++        if opts.process_dependency_links:
++            finder.process_dependency_links = True
++        if opts.trusted_hosts:
++            finder.secure_origins.extend(
++                ("*", host, "*") for host in opts.trusted_hosts)
++
++
++def break_args_options(line):
++    """Break up the line into an args and options string.  We only want to shlex
++    (and then optparse) the options, not the args.  args can contain markers
++    which are corrupted by shlex.
++    """
++    tokens = line.split(' ')
++    args = []
++    options = tokens[:]
++    for token in tokens:
++        if token.startswith('-') or token.startswith('--'):
++            break
++        else:
++            args.append(token)
++            options.pop(0)
++    return ' '.join(args), ' '.join(options)
++
++
++def build_parser(line):
++    """
++    Return a parser for parsing requirement lines
++    """
++    parser = optparse.OptionParser(add_help_option=False)
++
++    option_factories = SUPPORTED_OPTIONS + SUPPORTED_OPTIONS_REQ
++    for option_factory in option_factories:
++        option = option_factory()
++        parser.add_option(option)
++
++    # By default optparse sys.exits on parsing errors. We want to wrap
++    # that in our own exception.
++    def parser_exit(self, msg):
++        # add offending line
++        msg = 'Invalid requirement: %s\n%s' % (line, msg)
++        raise RequirementsFileParseError(msg)
++    parser.exit = parser_exit
++
++    return parser
++
++
++def join_lines(lines_enum):
++    """Joins a line ending in '\' with the previous line (except when following
++    comments).  The joined line takes on the index of the first line.
++    """
++    primary_line_number = None
++    new_line = []
++    for line_number, line in lines_enum:
++        if not line.endswith('\\') or COMMENT_RE.match(line):
++            if COMMENT_RE.match(line):
++                # this ensures comments are always matched later
++                line = ' ' + line
++            if new_line:
++                new_line.append(line)
++                yield primary_line_number, ''.join(new_line)
++                new_line = []
++            else:
++                yield line_number, line
++        else:
++            if not new_line:
++                primary_line_number = line_number
++            new_line.append(line.strip('\\'))
++
++    # last line contains \
++    if new_line:
++        yield primary_line_number, ''.join(new_line)
++
++    # TODO: handle space after '\'.
++
++
++def ignore_comments(lines_enum):
++    """
++    Strips comments and filter empty lines.
++    """
++    for line_number, line in lines_enum:
++        line = COMMENT_RE.sub('', line)
++        line = line.strip()
++        if line:
++            yield line_number, line
++
++
++def skip_regex(lines_enum, options):
++    """
++    Skip lines that match '--skip-requirements-regex' pattern
++
++    Note: the regex pattern is only built once
++    """
++    skip_regex = options.skip_requirements_regex if options else None
++    if skip_regex:
++        pattern = re.compile(skip_regex)
++        lines_enum = filterfalse(lambda e: pattern.search(e[1]), lines_enum)
++    return lines_enum
++
++
++def expand_env_variables(lines_enum):
++    """Replace all environment variables that can be retrieved via `os.getenv`.
++
++    The only allowed format for environment variables defined in the
++    requirement file is `${MY_VARIABLE_1}` to ensure two things:
++
++    1. Strings that contain a `$` aren't accidentally (partially) expanded.
++    2. Ensure consistency across platforms for requirement files.
++
++    These points are the result of a discusssion on the `github pull
++    request #3514 <https://github.com/pypa/pip/pull/3514>`_.
++
++    Valid characters in variable names follow the `POSIX standard
++    <http://pubs.opengroup.org/onlinepubs/9699919799/>`_ and are limited
++    to uppercase letter, digits and the `_` (underscore).
++    """
++    for line_number, line in lines_enum:
++        for env_var, var_name in ENV_VAR_RE.findall(line):
++            value = os.getenv(var_name)
++            if not value:
++                continue
++
++            line = line.replace(env_var, value)
++
++        yield line_number, line
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_install.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_install.py	(date 1573549700269)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_install.py	(date 1573549700269)
+@@ -0,0 +1,1115 @@
++from __future__ import absolute_import
++
++import logging
++import os
++import re
++import shutil
++import sys
++import sysconfig
++import traceback
++import warnings
++import zipfile
++from distutils.util import change_root
++from email.parser import FeedParser  # type: ignore
++
++from pip._vendor import pkg_resources, pytoml, six
++from pip._vendor.packaging import specifiers
++from pip._vendor.packaging.markers import Marker
++from pip._vendor.packaging.requirements import InvalidRequirement, Requirement
++from pip._vendor.packaging.utils import canonicalize_name
++from pip._vendor.packaging.version import parse as parse_version
++from pip._vendor.packaging.version import Version
++from pip._vendor.pkg_resources import RequirementParseError, parse_requirements
++
++from pip._internal import wheel
++from pip._internal.build_env import BuildEnvironment
++from pip._internal.compat import native_str
++from pip._internal.download import (
++    is_archive_file, is_url, path_to_url, url_to_path,
++)
++from pip._internal.exceptions import InstallationError, UninstallationError
++from pip._internal.locations import (
++    PIP_DELETE_MARKER_FILENAME, running_under_virtualenv,
++)
++from pip._internal.req.req_uninstall import UninstallPathSet
++from pip._internal.utils.deprecation import RemovedInPip11Warning
++from pip._internal.utils.hashes import Hashes
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import (
++    _make_build_dir, ask_path_exists, backup_dir, call_subprocess,
++    display_path, dist_in_site_packages, dist_in_usersite, ensure_dir,
++    get_installed_version, is_installable_dir, read_text_file, rmtree,
++)
++from pip._internal.utils.setuptools_build import SETUPTOOLS_SHIM
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.utils.ui import open_spinner
++from pip._internal.vcs import vcs
++from pip._internal.wheel import Wheel, move_wheel_files
++
++logger = logging.getLogger(__name__)
++
++operators = specifiers.Specifier._operators.keys()
++
++
++def _strip_extras(path):
++    m = re.match(r'^(.+)(\[[^\]]+\])$', path)
++    extras = None
++    if m:
++        path_no_extras = m.group(1)
++        extras = m.group(2)
++    else:
++        path_no_extras = path
++
++    return path_no_extras, extras
++
++
++class InstallRequirement(object):
++    """
++    Represents something that may be installed later on, may have information
++    about where to fetch the relavant requirement and also contains logic for
++    installing the said requirement.
++    """
++
++    def __init__(self, req, comes_from, source_dir=None, editable=False,
++                 link=None, update=True, markers=None,
++                 isolated=False, options=None, wheel_cache=None,
++                 constraint=False, extras=()):
++        assert req is None or isinstance(req, Requirement), req
++        self.req = req
++        self.comes_from = comes_from
++        self.constraint = constraint
++        if source_dir is not None:
++            self.source_dir = os.path.normpath(os.path.abspath(source_dir))
++        else:
++            self.source_dir = None
++        self.editable = editable
++
++        self._wheel_cache = wheel_cache
++        if link is not None:
++            self.link = self.original_link = link
++        else:
++            from pip._internal.index import Link
++            self.link = self.original_link = req and req.url and Link(req.url)
++
++        if extras:
++            self.extras = extras
++        elif req:
++            self.extras = {
++                pkg_resources.safe_extra(extra) for extra in req.extras
++            }
++        else:
++            self.extras = set()
++        if markers is not None:
++            self.markers = markers
++        else:
++            self.markers = req and req.marker
++        self._egg_info_path = None
++        # This holds the pkg_resources.Distribution object if this requirement
++        # is already available:
++        self.satisfied_by = None
++        # This hold the pkg_resources.Distribution object if this requirement
++        # conflicts with another installed distribution:
++        self.conflicts_with = None
++        # Temporary build location
++        self._temp_build_dir = TempDirectory(kind="req-build")
++        # Used to store the global directory where the _temp_build_dir should
++        # have been created. Cf _correct_build_location method.
++        self._ideal_build_dir = None
++        # True if the editable should be updated:
++        self.update = update
++        # Set to True after successful installation
++        self.install_succeeded = None
++        # UninstallPathSet of uninstalled distribution (for possible rollback)
++        self.uninstalled_pathset = None
++        self.options = options if options else {}
++        # Set to True after successful preparation of this requirement
++        self.prepared = False
++        self.is_direct = False
++
++        self.isolated = isolated
++        self.build_env = BuildEnvironment(no_clean=True)
++
++    @classmethod
++    def from_editable(cls, editable_req, comes_from=None, isolated=False,
++                      options=None, wheel_cache=None, constraint=False):
++        from pip._internal.index import Link
++
++        name, url, extras_override = parse_editable(editable_req)
++        if url.startswith('file:'):
++            source_dir = url_to_path(url)
++        else:
++            source_dir = None
++
++        if name is not None:
++            try:
++                req = Requirement(name)
++            except InvalidRequirement:
++                raise InstallationError("Invalid requirement: '%s'" % name)
++        else:
++            req = None
++        return cls(
++            req, comes_from, source_dir=source_dir,
++            editable=True,
++            link=Link(url),
++            constraint=constraint,
++            isolated=isolated,
++            options=options if options else {},
++            wheel_cache=wheel_cache,
++            extras=extras_override or (),
++        )
++
++    @classmethod
++    def from_req(cls, req, comes_from=None, isolated=False, wheel_cache=None):
++        try:
++            req = Requirement(req)
++        except InvalidRequirement:
++            raise InstallationError("Invalid requirement: '%s'" % req)
++        if req.url:
++            raise InstallationError(
++                "Direct url requirement (like %s) are not allowed for "
++                "dependencies" % req
++            )
++        return cls(req, comes_from, isolated=isolated, wheel_cache=wheel_cache)
++
++    @classmethod
++    def from_line(
++            cls, name, comes_from=None, isolated=False, options=None,
++            wheel_cache=None, constraint=False):
++        """Creates an InstallRequirement from a name, which might be a
++        requirement, directory containing 'setup.py', filename, or URL.
++        """
++        from pip._internal.index import Link
++
++        if is_url(name):
++            marker_sep = '; '
++        else:
++            marker_sep = ';'
++        if marker_sep in name:
++            name, markers = name.split(marker_sep, 1)
++            markers = markers.strip()
++            if not markers:
++                markers = None
++            else:
++                markers = Marker(markers)
++        else:
++            markers = None
++        name = name.strip()
++        req = None
++        path = os.path.normpath(os.path.abspath(name))
++        link = None
++        extras = None
++
++        if is_url(name):
++            link = Link(name)
++        else:
++            p, extras = _strip_extras(path)
++            looks_like_dir = os.path.isdir(p) and (
++                os.path.sep in name or
++                (os.path.altsep is not None and os.path.altsep in name) or
++                name.startswith('.')
++            )
++            if looks_like_dir:
++                if not is_installable_dir(p):
++                    raise InstallationError(
++                        "Directory %r is not installable. File 'setup.py' "
++                        "not found." % name
++                    )
++                link = Link(path_to_url(p))
++            elif is_archive_file(p):
++                if not os.path.isfile(p):
++                    logger.warning(
++                        'Requirement %r looks like a filename, but the '
++                        'file does not exist',
++                        name
++                    )
++                link = Link(path_to_url(p))
++
++        # it's a local file, dir, or url
++        if link:
++            # Handle relative file URLs
++            if link.scheme == 'file' and re.search(r'\.\./', link.url):
++                link = Link(
++                    path_to_url(os.path.normpath(os.path.abspath(link.path))))
++            # wheel file
++            if link.is_wheel:
++                wheel = Wheel(link.filename)  # can raise InvalidWheelFilename
++                req = "%s==%s" % (wheel.name, wheel.version)
++            else:
++                # set the req to the egg fragment.  when it's not there, this
++                # will become an 'unnamed' requirement
++                req = link.egg_fragment
++
++        # a requirement specifier
++        else:
++            req = name
++
++        if extras:
++            extras = Requirement("placeholder" + extras.lower()).extras
++        else:
++            extras = ()
++        if req is not None:
++            try:
++                req = Requirement(req)
++            except InvalidRequirement:
++                if os.path.sep in req:
++                    add_msg = "It looks like a path."
++                    add_msg += deduce_helpful_msg(req)
++                elif '=' in req and not any(op in req for op in operators):
++                    add_msg = "= is not a valid operator. Did you mean == ?"
++                else:
++                    add_msg = traceback.format_exc()
++                raise InstallationError(
++                    "Invalid requirement: '%s'\n%s" % (req, add_msg))
++        return cls(
++            req, comes_from, link=link, markers=markers,
++            isolated=isolated,
++            options=options if options else {},
++            wheel_cache=wheel_cache,
++            constraint=constraint,
++            extras=extras,
++        )
++
++    def __str__(self):
++        if self.req:
++            s = str(self.req)
++            if self.link:
++                s += ' from %s' % self.link.url
++        else:
++            s = self.link.url if self.link else None
++        if self.satisfied_by is not None:
++            s += ' in %s' % display_path(self.satisfied_by.location)
++        if self.comes_from:
++            if isinstance(self.comes_from, six.string_types):
++                comes_from = self.comes_from
++            else:
++                comes_from = self.comes_from.from_path()
++            if comes_from:
++                s += ' (from %s)' % comes_from
++        return s
++
++    def __repr__(self):
++        return '<%s object: %s editable=%r>' % (
++            self.__class__.__name__, str(self), self.editable)
++
++    def populate_link(self, finder, upgrade, require_hashes):
++        """Ensure that if a link can be found for this, that it is found.
++
++        Note that self.link may still be None - if Upgrade is False and the
++        requirement is already installed.
++
++        If require_hashes is True, don't use the wheel cache, because cached
++        wheels, always built locally, have different hashes than the files
++        downloaded from the index server and thus throw false hash mismatches.
++        Furthermore, cached wheels at present have undeterministic contents due
++        to file modification times.
++        """
++        if self.link is None:
++            self.link = finder.find_requirement(self, upgrade)
++        if self._wheel_cache is not None and not require_hashes:
++            old_link = self.link
++            self.link = self._wheel_cache.get(self.link, self.name)
++            if old_link != self.link:
++                logger.debug('Using cached wheel link: %s', self.link)
++
++    @property
++    def specifier(self):
++        return self.req.specifier
++
++    @property
++    def is_pinned(self):
++        """Return whether I am pinned to an exact version.
++
++        For example, some-package==1.2 is pinned; some-package>1.2 is not.
++        """
++        specifiers = self.specifier
++        return (len(specifiers) == 1 and
++                next(iter(specifiers)).operator in {'==', '==='})
++
++    def from_path(self):
++        if self.req is None:
++            return None
++        s = str(self.req)
++        if self.comes_from:
++            if isinstance(self.comes_from, six.string_types):
++                comes_from = self.comes_from
++            else:
++                comes_from = self.comes_from.from_path()
++            if comes_from:
++                s += '->' + comes_from
++        return s
++
++    def build_location(self, build_dir):
++        assert build_dir is not None
++        if self._temp_build_dir.path is not None:
++            return self._temp_build_dir.path
++        if self.req is None:
++            # for requirement via a path to a directory: the name of the
++            # package is not available yet so we create a temp directory
++            # Once run_egg_info will have run, we'll be able
++            # to fix it via _correct_build_location
++            # Some systems have /tmp as a symlink which confuses custom
++            # builds (such as numpy). Thus, we ensure that the real path
++            # is returned.
++            self._temp_build_dir.create()
++            self._ideal_build_dir = build_dir
++
++            return self._temp_build_dir.path
++        if self.editable:
++            name = self.name.lower()
++        else:
++            name = self.name
++        # FIXME: Is there a better place to create the build_dir? (hg and bzr
++        # need this)
++        if not os.path.exists(build_dir):
++            logger.debug('Creating directory %s', build_dir)
++            _make_build_dir(build_dir)
++        return os.path.join(build_dir, name)
++
++    def _correct_build_location(self):
++        """Move self._temp_build_dir to self._ideal_build_dir/self.req.name
++
++        For some requirements (e.g. a path to a directory), the name of the
++        package is not available until we run egg_info, so the build_location
++        will return a temporary directory and store the _ideal_build_dir.
++
++        This is only called by self.egg_info_path to fix the temporary build
++        directory.
++        """
++        if self.source_dir is not None:
++            return
++        assert self.req is not None
++        assert self._temp_build_dir.path
++        assert self._ideal_build_dir.path
++        old_location = self._temp_build_dir.path
++        self._temp_build_dir.path = None
++
++        new_location = self.build_location(self._ideal_build_dir)
++        if os.path.exists(new_location):
++            raise InstallationError(
++                'A package already exists in %s; please remove it to continue'
++                % display_path(new_location))
++        logger.debug(
++            'Moving package %s from %s to new location %s',
++            self, display_path(old_location), display_path(new_location),
++        )
++        shutil.move(old_location, new_location)
++        self._temp_build_dir.path = new_location
++        self._ideal_build_dir = None
++        self.source_dir = os.path.normpath(os.path.abspath(new_location))
++        self._egg_info_path = None
++
++    @property
++    def name(self):
++        if self.req is None:
++            return None
++        return native_str(pkg_resources.safe_name(self.req.name))
++
++    @property
++    def setup_py_dir(self):
++        return os.path.join(
++            self.source_dir,
++            self.link and self.link.subdirectory_fragment or '')
++
++    @property
++    def setup_py(self):
++        assert self.source_dir, "No source dir for %s" % self
++
++        setup_py = os.path.join(self.setup_py_dir, 'setup.py')
++
++        # Python2 __file__ should not be unicode
++        if six.PY2 and isinstance(setup_py, six.text_type):
++            setup_py = setup_py.encode(sys.getfilesystemencoding())
++
++        return setup_py
++
++    @property
++    def pyproject_toml(self):
++        assert self.source_dir, "No source dir for %s" % self
++
++        pp_toml = os.path.join(self.setup_py_dir, 'pyproject.toml')
++
++        # Python2 __file__ should not be unicode
++        if six.PY2 and isinstance(pp_toml, six.text_type):
++            pp_toml = pp_toml.encode(sys.getfilesystemencoding())
++
++        return pp_toml
++
++    def get_pep_518_info(self):
++        """Get a list of the packages required to build the project, if any,
++        and a flag indicating whether pyproject.toml is present, indicating
++        that the build should be isolated.
++
++        Build requirements can be specified in a pyproject.toml, as described
++        in PEP 518. If this file exists but doesn't specify build
++        requirements, pip will default to installing setuptools and wheel.
++        """
++        if os.path.isfile(self.pyproject_toml):
++            with open(self.pyproject_toml) as f:
++                pp_toml = pytoml.load(f)
++            build_sys = pp_toml.get('build-system', {})
++            return (build_sys.get('requires', ['setuptools', 'wheel']), True)
++        return (['setuptools', 'wheel'], False)
++
++    def run_egg_info(self):
++        assert self.source_dir
++        if self.name:
++            logger.debug(
++                'Running setup.py (path:%s) egg_info for package %s',
++                self.setup_py, self.name,
++            )
++        else:
++            logger.debug(
++                'Running setup.py (path:%s) egg_info for package from %s',
++                self.setup_py, self.link,
++            )
++
++        with indent_log():
++            script = SETUPTOOLS_SHIM % self.setup_py
++            base_cmd = [sys.executable, '-c', script]
++            if self.isolated:
++                base_cmd += ["--no-user-cfg"]
++            egg_info_cmd = base_cmd + ['egg_info']
++            # We can't put the .egg-info files at the root, because then the
++            # source code will be mistaken for an installed egg, causing
++            # problems
++            if self.editable:
++                egg_base_option = []
++            else:
++                egg_info_dir = os.path.join(self.setup_py_dir, 'pip-egg-info')
++                ensure_dir(egg_info_dir)
++                egg_base_option = ['--egg-base', 'pip-egg-info']
++            with self.build_env:
++                call_subprocess(
++                    egg_info_cmd + egg_base_option,
++                    cwd=self.setup_py_dir,
++                    show_stdout=False,
++                    command_desc='python setup.py egg_info')
++
++        if not self.req:
++            if isinstance(parse_version(self.pkg_info()["Version"]), Version):
++                op = "=="
++            else:
++                op = "==="
++            self.req = Requirement(
++                "".join([
++                    self.pkg_info()["Name"],
++                    op,
++                    self.pkg_info()["Version"],
++                ])
++            )
++            self._correct_build_location()
++        else:
++            metadata_name = canonicalize_name(self.pkg_info()["Name"])
++            if canonicalize_name(self.req.name) != metadata_name:
++                logger.warning(
++                    'Running setup.py (path:%s) egg_info for package %s '
++                    'produced metadata for project name %s. Fix your '
++                    '#egg=%s fragments.',
++                    self.setup_py, self.name, metadata_name, self.name
++                )
++                self.req = Requirement(metadata_name)
++
++    def egg_info_data(self, filename):
++        if self.satisfied_by is not None:
++            if not self.satisfied_by.has_metadata(filename):
++                return None
++            return self.satisfied_by.get_metadata(filename)
++        assert self.source_dir
++        filename = self.egg_info_path(filename)
++        if not os.path.exists(filename):
++            return None
++        data = read_text_file(filename)
++        return data
++
++    def egg_info_path(self, filename):
++        if self._egg_info_path is None:
++            if self.editable:
++                base = self.source_dir
++            else:
++                base = os.path.join(self.setup_py_dir, 'pip-egg-info')
++            filenames = os.listdir(base)
++            if self.editable:
++                filenames = []
++                for root, dirs, files in os.walk(base):
++                    for dir in vcs.dirnames:
++                        if dir in dirs:
++                            dirs.remove(dir)
++                    # Iterate over a copy of ``dirs``, since mutating
++                    # a list while iterating over it can cause trouble.
++                    # (See https://github.com/pypa/pip/pull/462.)
++                    for dir in list(dirs):
++                        # Don't search in anything that looks like a virtualenv
++                        # environment
++                        if (
++                                os.path.lexists(
++                                    os.path.join(root, dir, 'bin', 'python')
++                                ) or
++                                os.path.exists(
++                                    os.path.join(
++                                        root, dir, 'Scripts', 'Python.exe'
++                                    )
++                                )):
++                            dirs.remove(dir)
++                        # Also don't search through tests
++                        elif dir == 'test' or dir == 'tests':
++                            dirs.remove(dir)
++                    filenames.extend([os.path.join(root, dir)
++                                      for dir in dirs])
++                filenames = [f for f in filenames if f.endswith('.egg-info')]
++
++            if not filenames:
++                raise InstallationError(
++                    'No files/directories in %s (from %s)' % (base, filename)
++                )
++            assert filenames, \
++                "No files/directories in %s (from %s)" % (base, filename)
++
++            # if we have more than one match, we pick the toplevel one.  This
++            # can easily be the case if there is a dist folder which contains
++            # an extracted tarball for testing purposes.
++            if len(filenames) > 1:
++                filenames.sort(
++                    key=lambda x: x.count(os.path.sep) +
++                    (os.path.altsep and x.count(os.path.altsep) or 0)
++                )
++            self._egg_info_path = os.path.join(base, filenames[0])
++        return os.path.join(self._egg_info_path, filename)
++
++    def pkg_info(self):
++        p = FeedParser()
++        data = self.egg_info_data('PKG-INFO')
++        if not data:
++            logger.warning(
++                'No PKG-INFO file found in %s',
++                display_path(self.egg_info_path('PKG-INFO')),
++            )
++        p.feed(data or '')
++        return p.close()
++
++    _requirements_section_re = re.compile(r'\[(.*?)\]')
++
++    @property
++    def installed_version(self):
++        return get_installed_version(self.name)
++
++    def assert_source_matches_version(self):
++        assert self.source_dir
++        version = self.pkg_info()['version']
++        if self.req.specifier and version not in self.req.specifier:
++            logger.warning(
++                'Requested %s, but installing version %s',
++                self,
++                version,
++            )
++        else:
++            logger.debug(
++                'Source in %s has version %s, which satisfies requirement %s',
++                display_path(self.source_dir),
++                version,
++                self,
++            )
++
++    def update_editable(self, obtain=True):
++        if not self.link:
++            logger.debug(
++                "Cannot update repository at %s; repository location is "
++                "unknown",
++                self.source_dir,
++            )
++            return
++        assert self.editable
++        assert self.source_dir
++        if self.link.scheme == 'file':
++            # Static paths don't get updated
++            return
++        assert '+' in self.link.url, "bad url: %r" % self.link.url
++        if not self.update:
++            return
++        vc_type, url = self.link.url.split('+', 1)
++        backend = vcs.get_backend(vc_type)
++        if backend:
++            vcs_backend = backend(self.link.url)
++            if obtain:
++                vcs_backend.obtain(self.source_dir)
++            else:
++                vcs_backend.export(self.source_dir)
++        else:
++            assert 0, (
++                'Unexpected version control type (in %s): %s'
++                % (self.link, vc_type))
++
++    def uninstall(self, auto_confirm=False, verbose=False,
++                  use_user_site=False):
++        """
++        Uninstall the distribution currently satisfying this requirement.
++
++        Prompts before removing or modifying files unless
++        ``auto_confirm`` is True.
++
++        Refuses to delete or modify files outside of ``sys.prefix`` -
++        thus uninstallation within a virtual environment can only
++        modify that virtual environment, even if the virtualenv is
++        linked to global site-packages.
++
++        """
++        if not self.check_if_exists(use_user_site):
++            logger.warning("Skipping %s as it is not installed.", self.name)
++            return
++        dist = self.satisfied_by or self.conflicts_with
++
++        uninstalled_pathset = UninstallPathSet.from_dist(dist)
++        uninstalled_pathset.remove(auto_confirm, verbose)
++        return uninstalled_pathset
++
++    def archive(self, build_dir):
++        assert self.source_dir
++        create_archive = True
++        archive_name = '%s-%s.zip' % (self.name, self.pkg_info()["version"])
++        archive_path = os.path.join(build_dir, archive_name)
++        if os.path.exists(archive_path):
++            response = ask_path_exists(
++                'The file %s exists. (i)gnore, (w)ipe, (b)ackup, (a)bort ' %
++                display_path(archive_path), ('i', 'w', 'b', 'a'))
++            if response == 'i':
++                create_archive = False
++            elif response == 'w':
++                logger.warning('Deleting %s', display_path(archive_path))
++                os.remove(archive_path)
++            elif response == 'b':
++                dest_file = backup_dir(archive_path)
++                logger.warning(
++                    'Backing up %s to %s',
++                    display_path(archive_path),
++                    display_path(dest_file),
++                )
++                shutil.move(archive_path, dest_file)
++            elif response == 'a':
++                sys.exit(-1)
++        if create_archive:
++            zip = zipfile.ZipFile(
++                archive_path, 'w', zipfile.ZIP_DEFLATED,
++                allowZip64=True
++            )
++            dir = os.path.normcase(os.path.abspath(self.setup_py_dir))
++            for dirpath, dirnames, filenames in os.walk(dir):
++                if 'pip-egg-info' in dirnames:
++                    dirnames.remove('pip-egg-info')
++                for dirname in dirnames:
++                    dirname = os.path.join(dirpath, dirname)
++                    name = self._clean_zip_name(dirname, dir)
++                    zipdir = zipfile.ZipInfo(self.name + '/' + name + '/')
++                    zipdir.external_attr = 0x1ED << 16  # 0o755
++                    zip.writestr(zipdir, '')
++                for filename in filenames:
++                    if filename == PIP_DELETE_MARKER_FILENAME:
++                        continue
++                    filename = os.path.join(dirpath, filename)
++                    name = self._clean_zip_name(filename, dir)
++                    zip.write(filename, self.name + '/' + name)
++            zip.close()
++            logger.info('Saved %s', display_path(archive_path))
++
++    def _clean_zip_name(self, name, prefix):
++        assert name.startswith(prefix + os.path.sep), (
++            "name %r doesn't start with prefix %r" % (name, prefix)
++        )
++        name = name[len(prefix) + 1:]
++        name = name.replace(os.path.sep, '/')
++        return name
++
++    def match_markers(self, extras_requested=None):
++        if not extras_requested:
++            # Provide an extra to safely evaluate the markers
++            # without matching any extra
++            extras_requested = ('',)
++        if self.markers is not None:
++            return any(
++                self.markers.evaluate({'extra': extra})
++                for extra in extras_requested)
++        else:
++            return True
++
++    def install(self, install_options, global_options=None, root=None,
++                home=None, prefix=None, warn_script_location=True,
++                use_user_site=False, pycompile=True):
++        global_options = global_options if global_options is not None else []
++        if self.editable:
++            self.install_editable(
++                install_options, global_options, prefix=prefix,
++            )
++            return
++        if self.is_wheel:
++            version = wheel.wheel_version(self.source_dir)
++            wheel.check_compatibility(version, self.name)
++
++            self.move_wheel_files(
++                self.source_dir, root=root, prefix=prefix, home=home,
++                warn_script_location=warn_script_location,
++                use_user_site=use_user_site, pycompile=pycompile,
++            )
++            self.install_succeeded = True
++            return
++
++        # Extend the list of global and install options passed on to
++        # the setup.py call with the ones from the requirements file.
++        # Options specified in requirements file override those
++        # specified on the command line, since the last option given
++        # to setup.py is the one that is used.
++        global_options = list(global_options) + \
++            self.options.get('global_options', [])
++        install_options = list(install_options) + \
++            self.options.get('install_options', [])
++
++        if self.isolated:
++            global_options = global_options + ["--no-user-cfg"]
++
++        with TempDirectory(kind="record") as temp_dir:
++            record_filename = os.path.join(temp_dir.path, 'install-record.txt')
++            install_args = self.get_install_args(
++                global_options, record_filename, root, prefix, pycompile,
++            )
++            msg = 'Running setup.py install for %s' % (self.name,)
++            with open_spinner(msg) as spinner:
++                with indent_log():
++                    with self.build_env:
++                        call_subprocess(
++                            install_args + install_options,
++                            cwd=self.setup_py_dir,
++                            show_stdout=False,
++                            spinner=spinner,
++                        )
++
++            if not os.path.exists(record_filename):
++                logger.debug('Record file %s not found', record_filename)
++                return
++            self.install_succeeded = True
++
++            def prepend_root(path):
++                if root is None or not os.path.isabs(path):
++                    return path
++                else:
++                    return change_root(root, path)
++
++            with open(record_filename) as f:
++                for line in f:
++                    directory = os.path.dirname(line)
++                    if directory.endswith('.egg-info'):
++                        egg_info_dir = prepend_root(directory)
++                        break
++                else:
++                    logger.warning(
++                        'Could not find .egg-info directory in install record'
++                        ' for %s',
++                        self,
++                    )
++                    # FIXME: put the record somewhere
++                    # FIXME: should this be an error?
++                    return
++            new_lines = []
++            with open(record_filename) as f:
++                for line in f:
++                    filename = line.strip()
++                    if os.path.isdir(filename):
++                        filename += os.path.sep
++                    new_lines.append(
++                        os.path.relpath(prepend_root(filename), egg_info_dir)
++                    )
++            new_lines.sort()
++            ensure_dir(egg_info_dir)
++            inst_files_path = os.path.join(egg_info_dir, 'installed-files.txt')
++            with open(inst_files_path, 'w') as f:
++                f.write('\n'.join(new_lines) + '\n')
++
++    def ensure_has_source_dir(self, parent_dir):
++        """Ensure that a source_dir is set.
++
++        This will create a temporary build dir if the name of the requirement
++        isn't known yet.
++
++        :param parent_dir: The ideal pip parent_dir for the source_dir.
++            Generally src_dir for editables and build_dir for sdists.
++        :return: self.source_dir
++        """
++        if self.source_dir is None:
++            self.source_dir = self.build_location(parent_dir)
++        return self.source_dir
++
++    def get_install_args(self, global_options, record_filename, root, prefix,
++                         pycompile):
++        install_args = [sys.executable, "-u"]
++        install_args.append('-c')
++        install_args.append(SETUPTOOLS_SHIM % self.setup_py)
++        install_args += list(global_options) + \
++            ['install', '--record', record_filename]
++        install_args += ['--single-version-externally-managed']
++
++        if root is not None:
++            install_args += ['--root', root]
++        if prefix is not None:
++            install_args += ['--prefix', prefix]
++
++        if pycompile:
++            install_args += ["--compile"]
++        else:
++            install_args += ["--no-compile"]
++
++        if running_under_virtualenv():
++            py_ver_str = 'python' + sysconfig.get_python_version()
++            install_args += ['--install-headers',
++                             os.path.join(sys.prefix, 'include', 'site',
++                                          py_ver_str, self.name)]
++
++        return install_args
++
++    def remove_temporary_source(self):
++        """Remove the source files from this requirement, if they are marked
++        for deletion"""
++        if self.source_dir and os.path.exists(
++                os.path.join(self.source_dir, PIP_DELETE_MARKER_FILENAME)):
++            logger.debug('Removing source in %s', self.source_dir)
++            rmtree(self.source_dir)
++        self.source_dir = None
++        self._temp_build_dir.cleanup()
++        self.build_env.cleanup()
++
++    def install_editable(self, install_options,
++                         global_options=(), prefix=None):
++        logger.info('Running setup.py develop for %s', self.name)
++
++        if self.isolated:
++            global_options = list(global_options) + ["--no-user-cfg"]
++
++        if prefix:
++            prefix_param = ['--prefix={}'.format(prefix)]
++            install_options = list(install_options) + prefix_param
++
++        with indent_log():
++            # FIXME: should we do --install-headers here too?
++            with self.build_env:
++                call_subprocess(
++                    [
++                        sys.executable,
++                        '-c',
++                        SETUPTOOLS_SHIM % self.setup_py
++                    ] +
++                    list(global_options) +
++                    ['develop', '--no-deps'] +
++                    list(install_options),
++
++                    cwd=self.setup_py_dir,
++                    show_stdout=False,
++                )
++
++        self.install_succeeded = True
++
++    def check_if_exists(self, use_user_site):
++        """Find an installed distribution that satisfies or conflicts
++        with this requirement, and set self.satisfied_by or
++        self.conflicts_with appropriately.
++        """
++        if self.req is None:
++            return False
++        try:
++            # get_distribution() will resolve the entire list of requirements
++            # anyway, and we've already determined that we need the requirement
++            # in question, so strip the marker so that we don't try to
++            # evaluate it.
++            no_marker = Requirement(str(self.req))
++            no_marker.marker = None
++            self.satisfied_by = pkg_resources.get_distribution(str(no_marker))
++            if self.editable and self.satisfied_by:
++                self.conflicts_with = self.satisfied_by
++                # when installing editables, nothing pre-existing should ever
++                # satisfy
++                self.satisfied_by = None
++                return True
++        except pkg_resources.DistributionNotFound:
++            return False
++        except pkg_resources.VersionConflict:
++            existing_dist = pkg_resources.get_distribution(
++                self.req.name
++            )
++            if use_user_site:
++                if dist_in_usersite(existing_dist):
++                    self.conflicts_with = existing_dist
++                elif (running_under_virtualenv() and
++                        dist_in_site_packages(existing_dist)):
++                    raise InstallationError(
++                        "Will not install to the user site because it will "
++                        "lack sys.path precedence to %s in %s" %
++                        (existing_dist.project_name, existing_dist.location)
++                    )
++            else:
++                self.conflicts_with = existing_dist
++        return True
++
++    @property
++    def is_wheel(self):
++        return self.link and self.link.is_wheel
++
++    def move_wheel_files(self, wheeldir, root=None, home=None, prefix=None,
++                         warn_script_location=True, use_user_site=False,
++                         pycompile=True):
++        move_wheel_files(
++            self.name, self.req, wheeldir,
++            user=use_user_site,
++            home=home,
++            root=root,
++            prefix=prefix,
++            pycompile=pycompile,
++            isolated=self.isolated,
++            warn_script_location=warn_script_location,
++        )
++
++    def get_dist(self):
++        """Return a pkg_resources.Distribution built from self.egg_info_path"""
++        egg_info = self.egg_info_path('').rstrip(os.path.sep)
++        base_dir = os.path.dirname(egg_info)
++        metadata = pkg_resources.PathMetadata(base_dir, egg_info)
++        dist_name = os.path.splitext(os.path.basename(egg_info))[0]
++        return pkg_resources.Distribution(
++            os.path.dirname(egg_info),
++            project_name=dist_name,
++            metadata=metadata,
++        )
++
++    @property
++    def has_hash_options(self):
++        """Return whether any known-good hashes are specified as options.
++
++        These activate --require-hashes mode; hashes specified as part of a
++        URL do not.
++
++        """
++        return bool(self.options.get('hashes', {}))
++
++    def hashes(self, trust_internet=True):
++        """Return a hash-comparer that considers my option- and URL-based
++        hashes to be known-good.
++
++        Hashes in URLs--ones embedded in the requirements file, not ones
++        downloaded from an index server--are almost peers with ones from
++        flags. They satisfy --require-hashes (whether it was implicitly or
++        explicitly activated) but do not activate it. md5 and sha224 are not
++        allowed in flags, which should nudge people toward good algos. We
++        always OR all hashes together, even ones from URLs.
++
++        :param trust_internet: Whether to trust URL-based (#md5=...) hashes
++            downloaded from the internet, as by populate_link()
++
++        """
++        good_hashes = self.options.get('hashes', {}).copy()
++        link = self.link if trust_internet else self.original_link
++        if link and link.hash:
++            good_hashes.setdefault(link.hash_name, []).append(link.hash)
++        return Hashes(good_hashes)
++
++
++def _strip_postfix(req):
++    """
++        Strip req postfix ( -dev, 0.2, etc )
++    """
++    # FIXME: use package_to_requirement?
++    match = re.search(r'^(.*?)(?:-dev|-\d.*)$', req)
++    if match:
++        # Strip off -dev, -0.2, etc.
++        warnings.warn(
++            "#egg cleanup for editable urls will be dropped in the future",
++            RemovedInPip11Warning,
++        )
++        req = match.group(1)
++    return req
++
++
++def parse_editable(editable_req):
++    """Parses an editable requirement into:
++        - a requirement name
++        - an URL
++        - extras
++        - editable options
++    Accepted requirements:
++        svn+http://blahblah@rev#egg=Foobar[baz]&subdirectory=version_subdir
++        .[some_extra]
++    """
++
++    from pip._internal.index import Link
++
++    url = editable_req
++
++    # If a file path is specified with extras, strip off the extras.
++    url_no_extras, extras = _strip_extras(url)
++
++    if os.path.isdir(url_no_extras):
++        if not os.path.exists(os.path.join(url_no_extras, 'setup.py')):
++            raise InstallationError(
++                "Directory %r is not installable. File 'setup.py' not found." %
++                url_no_extras
++            )
++        # Treating it as code that has already been checked out
++        url_no_extras = path_to_url(url_no_extras)
++
++    if url_no_extras.lower().startswith('file:'):
++        package_name = Link(url_no_extras).egg_fragment
++        if extras:
++            return (
++                package_name,
++                url_no_extras,
++                Requirement("placeholder" + extras.lower()).extras,
++            )
++        else:
++            return package_name, url_no_extras, None
++
++    for version_control in vcs:
++        if url.lower().startswith('%s:' % version_control):
++            url = '%s+%s' % (version_control, url)
++            break
++
++    if '+' not in url:
++        raise InstallationError(
++            '%s should either be a path to a local project or a VCS url '
++            'beginning with svn+, git+, hg+, or bzr+' %
++            editable_req
++        )
++
++    vc_type = url.split('+', 1)[0].lower()
++
++    if not vcs.get_backend(vc_type):
++        error_message = 'For --editable=%s only ' % editable_req + \
++            ', '.join([backend.name + '+URL' for backend in vcs.backends]) + \
++            ' is currently supported'
++        raise InstallationError(error_message)
++
++    package_name = Link(url).egg_fragment
++    if not package_name:
++        raise InstallationError(
++            "Could not detect requirement name for '%s', please specify one "
++            "with #egg=your_package_name" % editable_req
++        )
++    return _strip_postfix(package_name), url, None
++
++
++def deduce_helpful_msg(req):
++    """Returns helpful msg in case requirements file does not exist,
++    or cannot be parsed.
++
++    :params req: Requirements file path
++    """
++    msg = ""
++    if os.path.exists(req):
++        msg = " It does exist."
++        # Try to parse and check if it is a requirements file.
++        try:
++            with open(req, 'r') as fp:
++                # parse first line only
++                next(parse_requirements(fp.read()))
++                msg += " The argument you provided " + \
++                    "(%s) appears to be a" % (req) + \
++                    " requirements file. If that is the" + \
++                    " case, use the '-r' flag to install" + \
++                    " the packages specified within it."
++        except RequirementParseError:
++            logger.debug("Cannot parse '%s' as requirements \
++            file" % (req), exc_info=1)
++    else:
++        msg += " File '%s' does not exist." % (req)
++    return msg
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_set.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_set.py	(date 1573549700275)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_set.py	(date 1573549700275)
+@@ -0,0 +1,164 @@
++from __future__ import absolute_import
++
++import logging
++from collections import OrderedDict
++
++from pip._internal.exceptions import InstallationError
++from pip._internal.utils.logging import indent_log
++from pip._internal.wheel import Wheel
++
++logger = logging.getLogger(__name__)
++
++
++class RequirementSet(object):
++
++    def __init__(self, require_hashes=False):
++        """Create a RequirementSet.
++
++        :param wheel_cache: The pip wheel cache, for passing to
++            InstallRequirement.
++        """
++
++        self.requirements = OrderedDict()
++        self.require_hashes = require_hashes
++
++        # Mapping of alias: real_name
++        self.requirement_aliases = {}
++        self.unnamed_requirements = []
++        self.successfully_downloaded = []
++        self.reqs_to_cleanup = []
++
++    def __str__(self):
++        reqs = [req for req in self.requirements.values()
++                if not req.comes_from]
++        reqs.sort(key=lambda req: req.name.lower())
++        return ' '.join([str(req.req) for req in reqs])
++
++    def __repr__(self):
++        reqs = [req for req in self.requirements.values()]
++        reqs.sort(key=lambda req: req.name.lower())
++        reqs_str = ', '.join([str(req.req) for req in reqs])
++        return ('<%s object; %d requirement(s): %s>'
++                % (self.__class__.__name__, len(reqs), reqs_str))
++
++    def add_requirement(self, install_req, parent_req_name=None,
++                        extras_requested=None):
++        """Add install_req as a requirement to install.
++
++        :param parent_req_name: The name of the requirement that needed this
++            added. The name is used because when multiple unnamed requirements
++            resolve to the same name, we could otherwise end up with dependency
++            links that point outside the Requirements set. parent_req must
++            already be added. Note that None implies that this is a user
++            supplied requirement, vs an inferred one.
++        :param extras_requested: an iterable of extras used to evaluate the
++            environment markers.
++        :return: Additional requirements to scan. That is either [] if
++            the requirement is not applicable, or [install_req] if the
++            requirement is applicable and has just been added.
++        """
++        name = install_req.name
++        if not install_req.match_markers(extras_requested):
++            logger.info("Ignoring %s: markers '%s' don't match your "
++                        "environment", install_req.name,
++                        install_req.markers)
++            return [], None
++
++        # This check has to come after we filter requirements with the
++        # environment markers.
++        if install_req.link and install_req.link.is_wheel:
++            wheel = Wheel(install_req.link.filename)
++            if not wheel.supported():
++                raise InstallationError(
++                    "%s is not a supported wheel on this platform." %
++                    wheel.filename
++                )
++
++        # This next bit is really a sanity check.
++        assert install_req.is_direct == (parent_req_name is None), (
++            "a direct req shouldn't have a parent and also, "
++            "a non direct req should have a parent"
++        )
++
++        if not name:
++            # url or path requirement w/o an egg fragment
++            self.unnamed_requirements.append(install_req)
++            return [install_req], None
++        else:
++            try:
++                existing_req = self.get_requirement(name)
++            except KeyError:
++                existing_req = None
++            if (parent_req_name is None and existing_req and not
++                    existing_req.constraint and
++                    existing_req.extras == install_req.extras and not
++                    existing_req.req.specifier == install_req.req.specifier):
++                raise InstallationError(
++                    'Double requirement given: %s (already in %s, name=%r)'
++                    % (install_req, existing_req, name))
++            if not existing_req:
++                # Add requirement
++                self.requirements[name] = install_req
++                # FIXME: what about other normalizations?  E.g., _ vs. -?
++                if name.lower() != name:
++                    self.requirement_aliases[name.lower()] = name
++                result = [install_req]
++            else:
++                # Assume there's no need to scan, and that we've already
++                # encountered this for scanning.
++                result = []
++                if not install_req.constraint and existing_req.constraint:
++                    if (install_req.link and not (existing_req.link and
++                       install_req.link.path == existing_req.link.path)):
++                        self.reqs_to_cleanup.append(install_req)
++                        raise InstallationError(
++                            "Could not satisfy constraints for '%s': "
++                            "installation from path or url cannot be "
++                            "constrained to a version" % name,
++                        )
++                    # If we're now installing a constraint, mark the existing
++                    # object for real installation.
++                    existing_req.constraint = False
++                    existing_req.extras = tuple(
++                        sorted(set(existing_req.extras).union(
++                               set(install_req.extras))))
++                    logger.debug("Setting %s extras to: %s",
++                                 existing_req, existing_req.extras)
++                    # And now we need to scan this.
++                    result = [existing_req]
++                # Canonicalise to the already-added object for the backref
++                # check below.
++                install_req = existing_req
++
++            # We return install_req here to allow for the caller to add it to
++            # the dependency information for the parent package.
++            return result, install_req
++
++    def has_requirement(self, project_name):
++        name = project_name.lower()
++        if (name in self.requirements and
++           not self.requirements[name].constraint or
++           name in self.requirement_aliases and
++           not self.requirements[self.requirement_aliases[name]].constraint):
++            return True
++        return False
++
++    @property
++    def has_requirements(self):
++        return list(req for req in self.requirements.values() if not
++                    req.constraint) or self.unnamed_requirements
++
++    def get_requirement(self, project_name):
++        for name in project_name, project_name.lower():
++            if name in self.requirements:
++                return self.requirements[name]
++            if name in self.requirement_aliases:
++                return self.requirements[self.requirement_aliases[name]]
++        raise KeyError("No project with the name %r" % project_name)
++
++    def cleanup_files(self):
++        """Clean up files, remove builds."""
++        logger.debug('Cleaning up...')
++        with indent_log():
++            for req in self.reqs_to_cleanup:
++                req.remove_temporary_source()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_uninstall.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_uninstall.py	(date 1573549700283)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/req_uninstall.py	(date 1573549700283)
+@@ -0,0 +1,455 @@
++from __future__ import absolute_import
++
++import csv
++import functools
++import logging
++import os
++import sys
++import sysconfig
++
++from pip._vendor import pkg_resources
++
++from pip._internal.compat import WINDOWS, cache_from_source, uses_pycache
++from pip._internal.exceptions import UninstallationError
++from pip._internal.locations import bin_py, bin_user
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import (
++    FakeFile, ask, dist_in_usersite, dist_is_local, egg_link_path, is_local,
++    normalize_path, renames,
++)
++from pip._internal.utils.temp_dir import TempDirectory
++
++logger = logging.getLogger(__name__)
++
++
++def _script_names(dist, script_name, is_gui):
++    """Create the fully qualified name of the files created by
++    {console,gui}_scripts for the given ``dist``.
++    Returns the list of file names
++    """
++    if dist_in_usersite(dist):
++        bin_dir = bin_user
++    else:
++        bin_dir = bin_py
++    exe_name = os.path.join(bin_dir, script_name)
++    paths_to_remove = [exe_name]
++    if WINDOWS:
++        paths_to_remove.append(exe_name + '.exe')
++        paths_to_remove.append(exe_name + '.exe.manifest')
++        if is_gui:
++            paths_to_remove.append(exe_name + '-script.pyw')
++        else:
++            paths_to_remove.append(exe_name + '-script.py')
++    return paths_to_remove
++
++
++def _unique(fn):
++    @functools.wraps(fn)
++    def unique(*args, **kw):
++        seen = set()
++        for item in fn(*args, **kw):
++            if item not in seen:
++                seen.add(item)
++                yield item
++    return unique
++
++
++@_unique
++def uninstallation_paths(dist):
++    """
++    Yield all the uninstallation paths for dist based on RECORD-without-.pyc
++
++    Yield paths to all the files in RECORD. For each .py file in RECORD, add
++    the .pyc in the same directory.
++
++    UninstallPathSet.add() takes care of the __pycache__ .pyc.
++    """
++    r = csv.reader(FakeFile(dist.get_metadata_lines('RECORD')))
++    for row in r:
++        path = os.path.join(dist.location, row[0])
++        yield path
++        if path.endswith('.py'):
++            dn, fn = os.path.split(path)
++            base = fn[:-3]
++            path = os.path.join(dn, base + '.pyc')
++            yield path
++
++
++def compact(paths):
++    """Compact a path set to contain the minimal number of paths
++    necessary to contain all paths in the set. If /a/path/ and
++    /a/path/to/a/file.txt are both in the set, leave only the
++    shorter path."""
++
++    sep = os.path.sep
++    short_paths = set()
++    for path in sorted(paths, key=len):
++        should_add = any(
++            path.startswith(shortpath.rstrip("*")) and
++            path[len(shortpath.rstrip("*").rstrip(sep))] == sep
++            for shortpath in short_paths
++        )
++        if not should_add:
++            short_paths.add(path)
++    return short_paths
++
++
++def compress_for_output_listing(paths):
++    """Returns a tuple of 2 sets of which paths to display to user
++
++    The first set contains paths that would be deleted. Files of a package
++    are not added and the top-level directory of the package has a '*' added
++    at the end - to signify that all it's contents are removed.
++
++    The second set contains files that would have been skipped in the above
++    folders.
++    """
++
++    will_remove = list(paths)
++    will_skip = set()
++
++    # Determine folders and files
++    folders = set()
++    files = set()
++    for path in will_remove:
++        if path.endswith(".pyc"):
++            continue
++        if path.endswith("__init__.py") or ".dist-info" in path:
++            folders.add(os.path.dirname(path))
++        files.add(path)
++
++    folders = compact(folders)
++
++    # This walks the tree using os.walk to not miss extra folders
++    # that might get added.
++    for folder in folders:
++        for dirpath, _, dirfiles in os.walk(folder):
++            for fname in dirfiles:
++                if fname.endswith(".pyc"):
++                    continue
++
++                file_ = os.path.normcase(os.path.join(dirpath, fname))
++                if os.path.isfile(file_) and file_ not in files:
++                    # We are skipping this file. Add it to the set.
++                    will_skip.add(file_)
++
++    will_remove = files | {
++        os.path.join(folder, "*") for folder in folders
++    }
++
++    return will_remove, will_skip
++
++
++class UninstallPathSet(object):
++    """A set of file paths to be removed in the uninstallation of a
++    requirement."""
++    def __init__(self, dist):
++        self.paths = set()
++        self._refuse = set()
++        self.pth = {}
++        self.dist = dist
++        self.save_dir = TempDirectory(kind="uninstall")
++        self._moved_paths = []
++
++    def _permitted(self, path):
++        """
++        Return True if the given path is one we are permitted to
++        remove/modify, False otherwise.
++
++        """
++        return is_local(path)
++
++    def add(self, path):
++        head, tail = os.path.split(path)
++
++        # we normalize the head to resolve parent directory symlinks, but not
++        # the tail, since we only want to uninstall symlinks, not their targets
++        path = os.path.join(normalize_path(head), os.path.normcase(tail))
++
++        if not os.path.exists(path):
++            return
++        if self._permitted(path):
++            self.paths.add(path)
++        else:
++            self._refuse.add(path)
++
++        # __pycache__ files can show up after 'installed-files.txt' is created,
++        # due to imports
++        if os.path.splitext(path)[1] == '.py' and uses_pycache:
++            self.add(cache_from_source(path))
++
++    def add_pth(self, pth_file, entry):
++        pth_file = normalize_path(pth_file)
++        if self._permitted(pth_file):
++            if pth_file not in self.pth:
++                self.pth[pth_file] = UninstallPthEntries(pth_file)
++            self.pth[pth_file].add(entry)
++        else:
++            self._refuse.add(pth_file)
++
++    def _stash(self, path):
++        return os.path.join(
++            self.save_dir.path, os.path.splitdrive(path)[1].lstrip(os.path.sep)
++        )
++
++    def remove(self, auto_confirm=False, verbose=False):
++        """Remove paths in ``self.paths`` with confirmation (unless
++        ``auto_confirm`` is True)."""
++
++        if not self.paths:
++            logger.info(
++                "Can't uninstall '%s'. No files were found to uninstall.",
++                self.dist.project_name,
++            )
++            return
++
++        dist_name_version = (
++            self.dist.project_name + "-" + self.dist.version
++        )
++        logger.info('Uninstalling %s:', dist_name_version)
++
++        with indent_log():
++            if auto_confirm or self._allowed_to_proceed(verbose):
++                self.save_dir.create()
++
++                for path in sorted(compact(self.paths)):
++                    new_path = self._stash(path)
++                    logger.debug('Removing file or directory %s', path)
++                    self._moved_paths.append(path)
++                    renames(path, new_path)
++                for pth in self.pth.values():
++                    pth.remove()
++
++                logger.info('Successfully uninstalled %s', dist_name_version)
++
++    def _allowed_to_proceed(self, verbose):
++        """Display which files would be deleted and prompt for confirmation
++        """
++
++        def _display(msg, paths):
++            if not paths:
++                return
++
++            logger.info(msg)
++            with indent_log():
++                for path in sorted(compact(paths)):
++                    logger.info(path)
++
++        if not verbose:
++            will_remove, will_skip = compress_for_output_listing(self.paths)
++        else:
++            # In verbose mode, display all the files that are going to be
++            # deleted.
++            will_remove = list(self.paths)
++            will_skip = set()
++
++        _display('Would remove:', will_remove)
++        _display('Would not remove (might be manually added):', will_skip)
++        _display('Would not remove (outside of prefix):', self._refuse)
++
++        return ask('Proceed (y/n)? ', ('y', 'n')) == 'y'
++
++    def rollback(self):
++        """Rollback the changes previously made by remove()."""
++        if self.save_dir.path is None:
++            logger.error(
++                "Can't roll back %s; was not uninstalled",
++                self.dist.project_name,
++            )
++            return False
++        logger.info('Rolling back uninstall of %s', self.dist.project_name)
++        for path in self._moved_paths:
++            tmp_path = self._stash(path)
++            logger.debug('Replacing %s', path)
++            renames(tmp_path, path)
++        for pth in self.pth.values():
++            pth.rollback()
++
++    def commit(self):
++        """Remove temporary save dir: rollback will no longer be possible."""
++        self.save_dir.cleanup()
++        self._moved_paths = []
++
++    @classmethod
++    def from_dist(cls, dist):
++        dist_path = normalize_path(dist.location)
++        if not dist_is_local(dist):
++            logger.info(
++                "Not uninstalling %s at %s, outside environment %s",
++                dist.key,
++                dist_path,
++                sys.prefix,
++            )
++            return cls(dist)
++
++        if dist_path in {p for p in {sysconfig.get_path("stdlib"),
++                                     sysconfig.get_path("platstdlib")}
++                         if p}:
++            logger.info(
++                "Not uninstalling %s at %s, as it is in the standard library.",
++                dist.key,
++                dist_path,
++            )
++            return cls(dist)
++
++        paths_to_remove = cls(dist)
++        develop_egg_link = egg_link_path(dist)
++        develop_egg_link_egg_info = '{}.egg-info'.format(
++            pkg_resources.to_filename(dist.project_name))
++        egg_info_exists = dist.egg_info and os.path.exists(dist.egg_info)
++        # Special case for distutils installed package
++        distutils_egg_info = getattr(dist._provider, 'path', None)
++
++        # Uninstall cases order do matter as in the case of 2 installs of the
++        # same package, pip needs to uninstall the currently detected version
++        if (egg_info_exists and dist.egg_info.endswith('.egg-info') and
++                not dist.egg_info.endswith(develop_egg_link_egg_info)):
++            # if dist.egg_info.endswith(develop_egg_link_egg_info), we
++            # are in fact in the develop_egg_link case
++            paths_to_remove.add(dist.egg_info)
++            if dist.has_metadata('installed-files.txt'):
++                for installed_file in dist.get_metadata(
++                        'installed-files.txt').splitlines():
++                    path = os.path.normpath(
++                        os.path.join(dist.egg_info, installed_file)
++                    )
++                    paths_to_remove.add(path)
++            # FIXME: need a test for this elif block
++            # occurs with --single-version-externally-managed/--record outside
++            # of pip
++            elif dist.has_metadata('top_level.txt'):
++                if dist.has_metadata('namespace_packages.txt'):
++                    namespaces = dist.get_metadata('namespace_packages.txt')
++                else:
++                    namespaces = []
++                for top_level_pkg in [
++                        p for p
++                        in dist.get_metadata('top_level.txt').splitlines()
++                        if p and p not in namespaces]:
++                    path = os.path.join(dist.location, top_level_pkg)
++                    paths_to_remove.add(path)
++                    paths_to_remove.add(path + '.py')
++                    paths_to_remove.add(path + '.pyc')
++                    paths_to_remove.add(path + '.pyo')
++
++        elif distutils_egg_info:
++            raise UninstallationError(
++                "Cannot uninstall {!r}. It is a distutils installed project "
++                "and thus we cannot accurately determine which files belong "
++                "to it which would lead to only a partial uninstall.".format(
++                    dist.project_name,
++                )
++            )
++
++        elif dist.location.endswith('.egg'):
++            # package installed by easy_install
++            # We cannot match on dist.egg_name because it can slightly vary
++            # i.e. setuptools-0.6c11-py2.6.egg vs setuptools-0.6rc11-py2.6.egg
++            paths_to_remove.add(dist.location)
++            easy_install_egg = os.path.split(dist.location)[1]
++            easy_install_pth = os.path.join(os.path.dirname(dist.location),
++                                            'easy-install.pth')
++            paths_to_remove.add_pth(easy_install_pth, './' + easy_install_egg)
++
++        elif egg_info_exists and dist.egg_info.endswith('.dist-info'):
++            for path in uninstallation_paths(dist):
++                paths_to_remove.add(path)
++
++        elif develop_egg_link:
++            # develop egg
++            with open(develop_egg_link, 'r') as fh:
++                link_pointer = os.path.normcase(fh.readline().strip())
++            assert (link_pointer == dist.location), (
++                'Egg-link %s does not match installed location of %s '
++                '(at %s)' % (link_pointer, dist.project_name, dist.location)
++            )
++            paths_to_remove.add(develop_egg_link)
++            easy_install_pth = os.path.join(os.path.dirname(develop_egg_link),
++                                            'easy-install.pth')
++            paths_to_remove.add_pth(easy_install_pth, dist.location)
++
++        else:
++            logger.debug(
++                'Not sure how to uninstall: %s - Check: %s',
++                dist, dist.location,
++            )
++
++        # find distutils scripts= scripts
++        if dist.has_metadata('scripts') and dist.metadata_isdir('scripts'):
++            for script in dist.metadata_listdir('scripts'):
++                if dist_in_usersite(dist):
++                    bin_dir = bin_user
++                else:
++                    bin_dir = bin_py
++                paths_to_remove.add(os.path.join(bin_dir, script))
++                if WINDOWS:
++                    paths_to_remove.add(os.path.join(bin_dir, script) + '.bat')
++
++        # find console_scripts
++        _scripts_to_remove = []
++        console_scripts = dist.get_entry_map(group='console_scripts')
++        for name in console_scripts.keys():
++            _scripts_to_remove.extend(_script_names(dist, name, False))
++        # find gui_scripts
++        gui_scripts = dist.get_entry_map(group='gui_scripts')
++        for name in gui_scripts.keys():
++            _scripts_to_remove.extend(_script_names(dist, name, True))
++
++        for s in _scripts_to_remove:
++            paths_to_remove.add(s)
++
++        return paths_to_remove
++
++
++class UninstallPthEntries(object):
++    def __init__(self, pth_file):
++        if not os.path.isfile(pth_file):
++            raise UninstallationError(
++                "Cannot remove entries from nonexistent file %s" % pth_file
++            )
++        self.file = pth_file
++        self.entries = set()
++        self._saved_lines = None
++
++    def add(self, entry):
++        entry = os.path.normcase(entry)
++        # On Windows, os.path.normcase converts the entry to use
++        # backslashes.  This is correct for entries that describe absolute
++        # paths outside of site-packages, but all the others use forward
++        # slashes.
++        if WINDOWS and not os.path.splitdrive(entry)[0]:
++            entry = entry.replace('\\', '/')
++        self.entries.add(entry)
++
++    def remove(self):
++        logger.debug('Removing pth entries from %s:', self.file)
++        with open(self.file, 'rb') as fh:
++            # windows uses '\r\n' with py3k, but uses '\n' with py2.x
++            lines = fh.readlines()
++            self._saved_lines = lines
++        if any(b'\r\n' in line for line in lines):
++            endline = '\r\n'
++        else:
++            endline = '\n'
++        # handle missing trailing newline
++        if lines and not lines[-1].endswith(endline.encode("utf-8")):
++            lines[-1] = lines[-1] + endline.encode("utf-8")
++        for entry in self.entries:
++            try:
++                logger.debug('Removing entry: %s', entry)
++                lines.remove((entry + endline).encode("utf-8"))
++            except ValueError:
++                pass
++        with open(self.file, 'wb') as fh:
++            fh.writelines(lines)
++
++    def rollback(self):
++        if self._saved_lines is None:
++            logger.error(
++                'Cannot roll back changes to %s, none were made', self.file
++            )
++            return False
++        logger.debug('Rolling %s back to previous state', self.file)
++        with open(self.file, 'wb') as fh:
++            fh.writelines(self._saved_lines)
++        return True
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/__init__.py	(date 1573549700255)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/req/__init__.py	(date 1573549700255)
+@@ -0,0 +1,69 @@
++from __future__ import absolute_import
++
++import logging
++
++from .req_install import InstallRequirement
++from .req_set import RequirementSet
++from .req_file import parse_requirements
++from pip._internal.utils.logging import indent_log
++
++
++__all__ = [
++    "RequirementSet", "InstallRequirement",
++    "parse_requirements", "install_given_reqs",
++]
++
++logger = logging.getLogger(__name__)
++
++
++def install_given_reqs(to_install, install_options, global_options=(),
++                       *args, **kwargs):
++    """
++    Install everything in the given list.
++
++    (to be called after having downloaded and unpacked the packages)
++    """
++
++    if to_install:
++        logger.info(
++            'Installing collected packages: %s',
++            ', '.join([req.name for req in to_install]),
++        )
++
++    with indent_log():
++        for requirement in to_install:
++            if requirement.conflicts_with:
++                logger.info(
++                    'Found existing installation: %s',
++                    requirement.conflicts_with,
++                )
++                with indent_log():
++                    uninstalled_pathset = requirement.uninstall(
++                        auto_confirm=True
++                    )
++            try:
++                requirement.install(
++                    install_options,
++                    global_options,
++                    *args,
++                    **kwargs
++                )
++            except:
++                should_rollback = (
++                    requirement.conflicts_with and
++                    not requirement.install_succeeded
++                )
++                # if install did not succeed, rollback previous uninstall
++                if should_rollback:
++                    uninstalled_pathset.rollback()
++                raise
++            else:
++                should_commit = (
++                    requirement.conflicts_with and
++                    requirement.install_succeeded
++                )
++                if should_commit:
++                    uninstalled_pathset.commit()
++            requirement.remove_temporary_source()
++
++    return to_install
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/git.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/git.py	(date 1573549700422)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/git.py	(date 1573549700422)
+@@ -0,0 +1,311 @@
++from __future__ import absolute_import
++
++import logging
++import os.path
++import re
++
++from pip._vendor.packaging.version import parse as parse_version
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++from pip._vendor.six.moves.urllib import request as urllib_request
++
++from pip._internal.compat import samefile
++from pip._internal.exceptions import BadCommand
++from pip._internal.utils.misc import display_path
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.vcs import VersionControl, vcs
++
++urlsplit = urllib_parse.urlsplit
++urlunsplit = urllib_parse.urlunsplit
++
++
++logger = logging.getLogger(__name__)
++
++
++HASH_REGEX = re.compile('[a-fA-F0-9]{40}')
++
++
++def looks_like_hash(sha):
++    return bool(HASH_REGEX.match(sha))
++
++
++class Git(VersionControl):
++    name = 'git'
++    dirname = '.git'
++    repo_name = 'clone'
++    schemes = (
++        'git', 'git+http', 'git+https', 'git+ssh', 'git+git', 'git+file',
++    )
++    # Prevent the user's environment variables from interfering with pip:
++    # https://github.com/pypa/pip/issues/1130
++    unset_environ = ('GIT_DIR', 'GIT_WORK_TREE')
++    default_arg_rev = 'HEAD'
++
++    def __init__(self, url=None, *args, **kwargs):
++
++        # Works around an apparent Git bug
++        # (see http://article.gmane.org/gmane.comp.version-control.git/146500)
++        if url:
++            scheme, netloc, path, query, fragment = urlsplit(url)
++            if scheme.endswith('file'):
++                initial_slashes = path[:-len(path.lstrip('/'))]
++                newpath = (
++                    initial_slashes +
++                    urllib_request.url2pathname(path)
++                    .replace('\\', '/').lstrip('/')
++                )
++                url = urlunsplit((scheme, netloc, newpath, query, fragment))
++                after_plus = scheme.find('+') + 1
++                url = scheme[:after_plus] + urlunsplit(
++                    (scheme[after_plus:], netloc, newpath, query, fragment),
++                )
++
++        super(Git, self).__init__(url, *args, **kwargs)
++
++    def get_base_rev_args(self, rev):
++        return [rev]
++
++    def get_git_version(self):
++        VERSION_PFX = 'git version '
++        version = self.run_command(['version'], show_stdout=False)
++        if version.startswith(VERSION_PFX):
++            version = version[len(VERSION_PFX):].split()[0]
++        else:
++            version = ''
++        # get first 3 positions of the git version becasue
++        # on windows it is x.y.z.windows.t, and this parses as
++        # LegacyVersion which always smaller than a Version.
++        version = '.'.join(version.split('.')[:3])
++        return parse_version(version)
++
++    def export(self, location):
++        """Export the Git repository at the url to the destination location"""
++        if not location.endswith('/'):
++            location = location + '/'
++
++        with TempDirectory(kind="export") as temp_dir:
++            self.unpack(temp_dir.path)
++            self.run_command(
++                ['checkout-index', '-a', '-f', '--prefix', location],
++                show_stdout=False, cwd=temp_dir.path
++            )
++
++    def get_revision_sha(self, dest, rev):
++        """
++        Return a commit hash for the given revision if it names a remote
++        branch or tag.  Otherwise, return None.
++
++        Args:
++          dest: the repository directory.
++          rev: the revision name.
++        """
++        # Pass rev to pre-filter the list.
++        output = self.run_command(['show-ref', rev], cwd=dest,
++                                  show_stdout=False, on_returncode='ignore')
++        refs = {}
++        for line in output.strip().splitlines():
++            try:
++                sha, ref = line.split()
++            except ValueError:
++                # Include the offending line to simplify troubleshooting if
++                # this error ever occurs.
++                raise ValueError('unexpected show-ref line: {!r}'.format(line))
++
++            refs[ref] = sha
++
++        branch_ref = 'refs/remotes/origin/{}'.format(rev)
++        tag_ref = 'refs/tags/{}'.format(rev)
++
++        return refs.get(branch_ref) or refs.get(tag_ref)
++
++    def check_rev_options(self, dest, rev_options):
++        """Check the revision options before checkout.
++
++        Returns a new RevOptions object for the SHA1 of the branch or tag
++        if found.
++
++        Args:
++          rev_options: a RevOptions object.
++        """
++        rev = rev_options.arg_rev
++        sha = self.get_revision_sha(dest, rev)
++
++        if sha is not None:
++            return rev_options.make_new(sha)
++
++        # Do not show a warning for the common case of something that has
++        # the form of a Git commit hash.
++        if not looks_like_hash(rev):
++            logger.warning(
++                "Did not find branch or tag '%s', assuming revision or ref.",
++                rev,
++            )
++        return rev_options
++
++    def is_commit_id_equal(self, dest, name):
++        """
++        Return whether the current commit hash equals the given name.
++
++        Args:
++          dest: the repository directory.
++          name: a string name.
++        """
++        if not name:
++            # Then avoid an unnecessary subprocess call.
++            return False
++
++        return self.get_revision(dest) == name
++
++    def switch(self, dest, url, rev_options):
++        self.run_command(['config', 'remote.origin.url', url], cwd=dest)
++        cmd_args = ['checkout', '-q'] + rev_options.to_args()
++        self.run_command(cmd_args, cwd=dest)
++
++        self.update_submodules(dest)
++
++    def update(self, dest, rev_options):
++        # First fetch changes from the default remote
++        if self.get_git_version() >= parse_version('1.9.0'):
++            # fetch tags in addition to everything else
++            self.run_command(['fetch', '-q', '--tags'], cwd=dest)
++        else:
++            self.run_command(['fetch', '-q'], cwd=dest)
++        # Then reset to wanted revision (maybe even origin/master)
++        rev_options = self.check_rev_options(dest, rev_options)
++        cmd_args = ['reset', '--hard', '-q'] + rev_options.to_args()
++        self.run_command(cmd_args, cwd=dest)
++        #: update submodules
++        self.update_submodules(dest)
++
++    def obtain(self, dest):
++        url, rev = self.get_url_rev()
++        rev_options = self.make_rev_options(rev)
++        if self.check_destination(dest, url, rev_options):
++            rev_display = rev_options.to_display()
++            logger.info(
++                'Cloning %s%s to %s', url, rev_display, display_path(dest),
++            )
++            self.run_command(['clone', '-q', url, dest])
++
++            if rev:
++                rev_options = self.check_rev_options(dest, rev_options)
++                # Only do a checkout if the current commit id doesn't match
++                # the requested revision.
++                if not self.is_commit_id_equal(dest, rev_options.rev):
++                    rev = rev_options.rev
++                    # Only fetch the revision if it's a ref
++                    if rev.startswith('refs/'):
++                        self.run_command(
++                            ['fetch', '-q', url] + rev_options.to_args(),
++                            cwd=dest,
++                        )
++                        # Change the revision to the SHA of the ref we fetched
++                        rev = 'FETCH_HEAD'
++                    self.run_command(['checkout', '-q', rev], cwd=dest)
++
++            #: repo may contain submodules
++            self.update_submodules(dest)
++
++    def get_url(self, location):
++        """Return URL of the first remote encountered."""
++        remotes = self.run_command(
++            ['config', '--get-regexp', r'remote\..*\.url'],
++            show_stdout=False, cwd=location,
++        )
++        remotes = remotes.splitlines()
++        found_remote = remotes[0]
++        for remote in remotes:
++            if remote.startswith('remote.origin.url '):
++                found_remote = remote
++                break
++        url = found_remote.split(' ')[1]
++        return url.strip()
++
++    def get_revision(self, location):
++        current_rev = self.run_command(
++            ['rev-parse', 'HEAD'], show_stdout=False, cwd=location,
++        )
++        return current_rev.strip()
++
++    def _get_subdirectory(self, location):
++        """Return the relative path of setup.py to the git repo root."""
++        # find the repo root
++        git_dir = self.run_command(['rev-parse', '--git-dir'],
++                                   show_stdout=False, cwd=location).strip()
++        if not os.path.isabs(git_dir):
++            git_dir = os.path.join(location, git_dir)
++        root_dir = os.path.join(git_dir, '..')
++        # find setup.py
++        orig_location = location
++        while not os.path.exists(os.path.join(location, 'setup.py')):
++            last_location = location
++            location = os.path.dirname(location)
++            if location == last_location:
++                # We've traversed up to the root of the filesystem without
++                # finding setup.py
++                logger.warning(
++                    "Could not find setup.py for directory %s (tried all "
++                    "parent directories)",
++                    orig_location,
++                )
++                return None
++        # relative path of setup.py to repo root
++        if samefile(root_dir, location):
++            return None
++        return os.path.relpath(location, root_dir)
++
++    def get_src_requirement(self, dist, location):
++        repo = self.get_url(location)
++        if not repo.lower().startswith('git:'):
++            repo = 'git+' + repo
++        egg_project_name = dist.egg_name().split('-', 1)[0]
++        if not repo:
++            return None
++        current_rev = self.get_revision(location)
++        req = '%s@%s#egg=%s' % (repo, current_rev, egg_project_name)
++        subdirectory = self._get_subdirectory(location)
++        if subdirectory:
++            req += '&subdirectory=' + subdirectory
++        return req
++
++    def get_url_rev(self):
++        """
++        Prefixes stub URLs like 'user@hostname:user/repo.git' with 'ssh://'.
++        That's required because although they use SSH they sometimes doesn't
++        work with a ssh:// scheme (e.g. Github). But we need a scheme for
++        parsing. Hence we remove it again afterwards and return it as a stub.
++        """
++        if '://' not in self.url:
++            assert 'file:' not in self.url
++            self.url = self.url.replace('git+', 'git+ssh://')
++            url, rev = super(Git, self).get_url_rev()
++            url = url.replace('ssh://', '')
++        else:
++            url, rev = super(Git, self).get_url_rev()
++
++        return url, rev
++
++    def update_submodules(self, location):
++        if not os.path.exists(os.path.join(location, '.gitmodules')):
++            return
++        self.run_command(
++            ['submodule', 'update', '--init', '--recursive', '-q'],
++            cwd=location,
++        )
++
++    @classmethod
++    def controls_location(cls, location):
++        if super(Git, cls).controls_location(location):
++            return True
++        try:
++            r = cls().run_command(['rev-parse'],
++                                  cwd=location,
++                                  show_stdout=False,
++                                  on_returncode='ignore')
++            return not r
++        except BadCommand:
++            logger.debug("could not determine if %s is under git control "
++                         "because git is not available", location)
++            return False
++
++
++vcs.register(Git)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/mercurial.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/mercurial.py	(date 1573549700430)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/mercurial.py	(date 1573549700430)
+@@ -0,0 +1,105 @@
++from __future__ import absolute_import
++
++import logging
++import os
++
++from pip._vendor.six.moves import configparser
++
++from pip._internal.download import path_to_url
++from pip._internal.utils.misc import display_path
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.vcs import VersionControl, vcs
++
++logger = logging.getLogger(__name__)
++
++
++class Mercurial(VersionControl):
++    name = 'hg'
++    dirname = '.hg'
++    repo_name = 'clone'
++    schemes = ('hg', 'hg+http', 'hg+https', 'hg+ssh', 'hg+static-http')
++
++    def get_base_rev_args(self, rev):
++        return [rev]
++
++    def export(self, location):
++        """Export the Hg repository at the url to the destination location"""
++        with TempDirectory(kind="export") as temp_dir:
++            self.unpack(temp_dir.path)
++
++            self.run_command(
++                ['archive', location], show_stdout=False, cwd=temp_dir.path
++            )
++
++    def switch(self, dest, url, rev_options):
++        repo_config = os.path.join(dest, self.dirname, 'hgrc')
++        config = configparser.SafeConfigParser()
++        try:
++            config.read(repo_config)
++            config.set('paths', 'default', url)
++            with open(repo_config, 'w') as config_file:
++                config.write(config_file)
++        except (OSError, configparser.NoSectionError) as exc:
++            logger.warning(
++                'Could not switch Mercurial repository to %s: %s', url, exc,
++            )
++        else:
++            cmd_args = ['update', '-q'] + rev_options.to_args()
++            self.run_command(cmd_args, cwd=dest)
++
++    def update(self, dest, rev_options):
++        self.run_command(['pull', '-q'], cwd=dest)
++        cmd_args = ['update', '-q'] + rev_options.to_args()
++        self.run_command(cmd_args, cwd=dest)
++
++    def obtain(self, dest):
++        url, rev = self.get_url_rev()
++        rev_options = self.make_rev_options(rev)
++        if self.check_destination(dest, url, rev_options):
++            rev_display = rev_options.to_display()
++            logger.info(
++                'Cloning hg %s%s to %s',
++                url,
++                rev_display,
++                display_path(dest),
++            )
++            self.run_command(['clone', '--noupdate', '-q', url, dest])
++            cmd_args = ['update', '-q'] + rev_options.to_args()
++            self.run_command(cmd_args, cwd=dest)
++
++    def get_url(self, location):
++        url = self.run_command(
++            ['showconfig', 'paths.default'],
++            show_stdout=False, cwd=location).strip()
++        if self._is_local_repository(url):
++            url = path_to_url(url)
++        return url.strip()
++
++    def get_revision(self, location):
++        current_revision = self.run_command(
++            ['parents', '--template={rev}'],
++            show_stdout=False, cwd=location).strip()
++        return current_revision
++
++    def get_revision_hash(self, location):
++        current_rev_hash = self.run_command(
++            ['parents', '--template={node}'],
++            show_stdout=False, cwd=location).strip()
++        return current_rev_hash
++
++    def get_src_requirement(self, dist, location):
++        repo = self.get_url(location)
++        if not repo.lower().startswith('hg:'):
++            repo = 'hg+' + repo
++        egg_project_name = dist.egg_name().split('-', 1)[0]
++        if not repo:
++            return None
++        current_rev_hash = self.get_revision_hash(location)
++        return '%s@%s#egg=%s' % (repo, current_rev_hash, egg_project_name)
++
++    def is_commit_id_equal(self, dest, name):
++        """Always assume the versions don't match"""
++        return False
++
++
++vcs.register(Mercurial)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/bazaar.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/bazaar.py	(date 1573549700416)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/bazaar.py	(date 1573549700416)
+@@ -0,0 +1,113 @@
++from __future__ import absolute_import
++
++import logging
++import os
++
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++
++from pip._internal.download import path_to_url
++from pip._internal.utils.misc import display_path, rmtree
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.vcs import VersionControl, vcs
++
++logger = logging.getLogger(__name__)
++
++
++class Bazaar(VersionControl):
++    name = 'bzr'
++    dirname = '.bzr'
++    repo_name = 'branch'
++    schemes = (
++        'bzr', 'bzr+http', 'bzr+https', 'bzr+ssh', 'bzr+sftp', 'bzr+ftp',
++        'bzr+lp',
++    )
++
++    def __init__(self, url=None, *args, **kwargs):
++        super(Bazaar, self).__init__(url, *args, **kwargs)
++        # This is only needed for python <2.7.5
++        # Register lp but do not expose as a scheme to support bzr+lp.
++        if getattr(urllib_parse, 'uses_fragment', None):
++            urllib_parse.uses_fragment.extend(['lp'])
++
++    def get_base_rev_args(self, rev):
++        return ['-r', rev]
++
++    def export(self, location):
++        """
++        Export the Bazaar repository at the url to the destination location
++        """
++        # Remove the location to make sure Bazaar can export it correctly
++        if os.path.exists(location):
++            rmtree(location)
++
++        with TempDirectory(kind="export") as temp_dir:
++            self.unpack(temp_dir.path)
++
++            self.run_command(
++                ['export', location],
++                cwd=temp_dir.path, show_stdout=False,
++            )
++
++    def switch(self, dest, url, rev_options):
++        self.run_command(['switch', url], cwd=dest)
++
++    def update(self, dest, rev_options):
++        cmd_args = ['pull', '-q'] + rev_options.to_args()
++        self.run_command(cmd_args, cwd=dest)
++
++    def obtain(self, dest):
++        url, rev = self.get_url_rev()
++        rev_options = self.make_rev_options(rev)
++        if self.check_destination(dest, url, rev_options):
++            rev_display = rev_options.to_display()
++            logger.info(
++                'Checking out %s%s to %s',
++                url,
++                rev_display,
++                display_path(dest),
++            )
++            cmd_args = ['branch', '-q'] + rev_options.to_args() + [url, dest]
++            self.run_command(cmd_args)
++
++    def get_url_rev(self):
++        # hotfix the URL scheme after removing bzr+ from bzr+ssh:// readd it
++        url, rev = super(Bazaar, self).get_url_rev()
++        if url.startswith('ssh://'):
++            url = 'bzr+' + url
++        return url, rev
++
++    def get_url(self, location):
++        urls = self.run_command(['info'], show_stdout=False, cwd=location)
++        for line in urls.splitlines():
++            line = line.strip()
++            for x in ('checkout of branch: ',
++                      'parent branch: '):
++                if line.startswith(x):
++                    repo = line.split(x)[1]
++                    if self._is_local_repository(repo):
++                        return path_to_url(repo)
++                    return repo
++        return None
++
++    def get_revision(self, location):
++        revision = self.run_command(
++            ['revno'], show_stdout=False, cwd=location,
++        )
++        return revision.splitlines()[-1]
++
++    def get_src_requirement(self, dist, location):
++        repo = self.get_url(location)
++        if not repo:
++            return None
++        if not repo.lower().startswith('bzr:'):
++            repo = 'bzr+' + repo
++        egg_project_name = dist.egg_name().split('-', 1)[0]
++        current_rev = self.get_revision(location)
++        return '%s@%s#egg=%s' % (repo, current_rev, egg_project_name)
++
++    def is_commit_id_equal(self, dest, name):
++        """Always assume the versions don't match"""
++        return False
++
++
++vcs.register(Bazaar)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/subversion.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/subversion.py	(date 1573549700438)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/subversion.py	(date 1573549700438)
+@@ -0,0 +1,271 @@
++from __future__ import absolute_import
++
++import logging
++import os
++import re
++
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++
++from pip._internal.index import Link
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import display_path, rmtree
++from pip._internal.vcs import VersionControl, vcs
++
++_svn_xml_url_re = re.compile('url="([^"]+)"')
++_svn_rev_re = re.compile(r'committed-rev="(\d+)"')
++_svn_url_re = re.compile(r'URL: (.+)')
++_svn_revision_re = re.compile(r'Revision: (.+)')
++_svn_info_xml_rev_re = re.compile(r'\s*revision="(\d+)"')
++_svn_info_xml_url_re = re.compile(r'<url>(.*)</url>')
++
++
++logger = logging.getLogger(__name__)
++
++
++class Subversion(VersionControl):
++    name = 'svn'
++    dirname = '.svn'
++    repo_name = 'checkout'
++    schemes = ('svn', 'svn+ssh', 'svn+http', 'svn+https', 'svn+svn')
++
++    def get_base_rev_args(self, rev):
++        return ['-r', rev]
++
++    def get_info(self, location):
++        """Returns (url, revision), where both are strings"""
++        assert not location.rstrip('/').endswith(self.dirname), \
++            'Bad directory: %s' % location
++        output = self.run_command(
++            ['info', location],
++            show_stdout=False,
++            extra_environ={'LANG': 'C'},
++        )
++        match = _svn_url_re.search(output)
++        if not match:
++            logger.warning(
++                'Cannot determine URL of svn checkout %s',
++                display_path(location),
++            )
++            logger.debug('Output that cannot be parsed: \n%s', output)
++            return None, None
++        url = match.group(1).strip()
++        match = _svn_revision_re.search(output)
++        if not match:
++            logger.warning(
++                'Cannot determine revision of svn checkout %s',
++                display_path(location),
++            )
++            logger.debug('Output that cannot be parsed: \n%s', output)
++            return url, None
++        return url, match.group(1)
++
++    def export(self, location):
++        """Export the svn repository at the url to the destination location"""
++        url, rev = self.get_url_rev()
++        rev_options = get_rev_options(self, url, rev)
++        url = self.remove_auth_from_url(url)
++        logger.info('Exporting svn repository %s to %s', url, location)
++        with indent_log():
++            if os.path.exists(location):
++                # Subversion doesn't like to check out over an existing
++                # directory --force fixes this, but was only added in svn 1.5
++                rmtree(location)
++            cmd_args = ['export'] + rev_options.to_args() + [url, location]
++            self.run_command(cmd_args, show_stdout=False)
++
++    def switch(self, dest, url, rev_options):
++        cmd_args = ['switch'] + rev_options.to_args() + [url, dest]
++        self.run_command(cmd_args)
++
++    def update(self, dest, rev_options):
++        cmd_args = ['update'] + rev_options.to_args() + [dest]
++        self.run_command(cmd_args)
++
++    def obtain(self, dest):
++        url, rev = self.get_url_rev()
++        rev_options = get_rev_options(self, url, rev)
++        url = self.remove_auth_from_url(url)
++        if self.check_destination(dest, url, rev_options):
++            rev_display = rev_options.to_display()
++            logger.info(
++                'Checking out %s%s to %s',
++                url,
++                rev_display,
++                display_path(dest),
++            )
++            cmd_args = ['checkout', '-q'] + rev_options.to_args() + [url, dest]
++            self.run_command(cmd_args)
++
++    def get_location(self, dist, dependency_links):
++        for url in dependency_links:
++            egg_fragment = Link(url).egg_fragment
++            if not egg_fragment:
++                continue
++            if '-' in egg_fragment:
++                # FIXME: will this work when a package has - in the name?
++                key = '-'.join(egg_fragment.split('-')[:-1]).lower()
++            else:
++                key = egg_fragment
++            if key == dist.key:
++                return url.split('#', 1)[0]
++        return None
++
++    def get_revision(self, location):
++        """
++        Return the maximum revision for all files under a given location
++        """
++        # Note: taken from setuptools.command.egg_info
++        revision = 0
++
++        for base, dirs, files in os.walk(location):
++            if self.dirname not in dirs:
++                dirs[:] = []
++                continue    # no sense walking uncontrolled subdirs
++            dirs.remove(self.dirname)
++            entries_fn = os.path.join(base, self.dirname, 'entries')
++            if not os.path.exists(entries_fn):
++                # FIXME: should we warn?
++                continue
++
++            dirurl, localrev = self._get_svn_url_rev(base)
++
++            if base == location:
++                base = dirurl + '/'   # save the root url
++            elif not dirurl or not dirurl.startswith(base):
++                dirs[:] = []
++                continue    # not part of the same svn tree, skip it
++            revision = max(revision, localrev)
++        return revision
++
++    def get_url_rev(self):
++        # hotfix the URL scheme after removing svn+ from svn+ssh:// readd it
++        url, rev = super(Subversion, self).get_url_rev()
++        if url.startswith('ssh://'):
++            url = 'svn+' + url
++        return url, rev
++
++    def get_url(self, location):
++        # In cases where the source is in a subdirectory, not alongside
++        # setup.py we have to look up in the location until we find a real
++        # setup.py
++        orig_location = location
++        while not os.path.exists(os.path.join(location, 'setup.py')):
++            last_location = location
++            location = os.path.dirname(location)
++            if location == last_location:
++                # We've traversed up to the root of the filesystem without
++                # finding setup.py
++                logger.warning(
++                    "Could not find setup.py for directory %s (tried all "
++                    "parent directories)",
++                    orig_location,
++                )
++                return None
++
++        return self._get_svn_url_rev(location)[0]
++
++    def _get_svn_url_rev(self, location):
++        from pip._internal.exceptions import InstallationError
++
++        entries_path = os.path.join(location, self.dirname, 'entries')
++        if os.path.exists(entries_path):
++            with open(entries_path) as f:
++                data = f.read()
++        else:  # subversion >= 1.7 does not have the 'entries' file
++            data = ''
++
++        if (data.startswith('8') or
++                data.startswith('9') or
++                data.startswith('10')):
++            data = list(map(str.splitlines, data.split('\n\x0c\n')))
++            del data[0][0]  # get rid of the '8'
++            url = data[0][3]
++            revs = [int(d[9]) for d in data if len(d) > 9 and d[9]] + [0]
++        elif data.startswith('<?xml'):
++            match = _svn_xml_url_re.search(data)
++            if not match:
++                raise ValueError('Badly formatted data: %r' % data)
++            url = match.group(1)    # get repository URL
++            revs = [int(m.group(1)) for m in _svn_rev_re.finditer(data)] + [0]
++        else:
++            try:
++                # subversion >= 1.7
++                xml = self.run_command(
++                    ['info', '--xml', location],
++                    show_stdout=False,
++                )
++                url = _svn_info_xml_url_re.search(xml).group(1)
++                revs = [
++                    int(m.group(1)) for m in _svn_info_xml_rev_re.finditer(xml)
++                ]
++            except InstallationError:
++                url, revs = None, []
++
++        if revs:
++            rev = max(revs)
++        else:
++            rev = 0
++
++        return url, rev
++
++    def get_src_requirement(self, dist, location):
++        repo = self.get_url(location)
++        if repo is None:
++            return None
++        # FIXME: why not project name?
++        egg_project_name = dist.egg_name().split('-', 1)[0]
++        rev = self.get_revision(location)
++        return 'svn+%s@%s#egg=%s' % (repo, rev, egg_project_name)
++
++    def is_commit_id_equal(self, dest, name):
++        """Always assume the versions don't match"""
++        return False
++
++    @staticmethod
++    def remove_auth_from_url(url):
++        # Return a copy of url with 'username:password@' removed.
++        # username/pass params are passed to subversion through flags
++        # and are not recognized in the url.
++
++        # parsed url
++        purl = urllib_parse.urlsplit(url)
++        stripped_netloc = \
++            purl.netloc.split('@')[-1]
++
++        # stripped url
++        url_pieces = (
++            purl.scheme, stripped_netloc, purl.path, purl.query, purl.fragment
++        )
++        surl = urllib_parse.urlunsplit(url_pieces)
++        return surl
++
++
++def get_rev_options(vcs, url, rev):
++    """
++    Return a RevOptions object.
++    """
++    r = urllib_parse.urlsplit(url)
++    if hasattr(r, 'username'):
++        # >= Python-2.5
++        username, password = r.username, r.password
++    else:
++        netloc = r[1]
++        if '@' in netloc:
++            auth = netloc.split('@')[0]
++            if ':' in auth:
++                username, password = auth.split(':', 1)
++            else:
++                username, password = auth, None
++        else:
++            username, password = None, None
++
++    extra_args = []
++    if username:
++        extra_args += ['--username', username]
++    if password:
++        extra_args += ['--password', password]
++
++    return vcs.make_rev_options(rev, extra_args=extra_args)
++
++
++vcs.register(Subversion)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/__init__.py	(date 1573549700411)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/vcs/__init__.py	(date 1573549700411)
+@@ -0,0 +1,471 @@
++"""Handles all VCS (version control) support"""
++from __future__ import absolute_import
++
++import copy
++import errno
++import logging
++import os
++import shutil
++import sys
++
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++
++from pip._internal.exceptions import BadCommand
++from pip._internal.utils.misc import (
++    display_path, backup_dir, call_subprocess, rmtree, ask_path_exists,
++)
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++
++if MYPY_CHECK_RUNNING:
++    from typing import Dict, Optional, Tuple
++    from pip._internal.basecommand import Command
++
++__all__ = ['vcs', 'get_src_requirement']
++
++
++logger = logging.getLogger(__name__)
++
++
++class RevOptions(object):
++
++    """
++    Encapsulates a VCS-specific revision to install, along with any VCS
++    install options.
++
++    Instances of this class should be treated as if immutable.
++    """
++
++    def __init__(self, vcs, rev=None, extra_args=None):
++        """
++        Args:
++          vcs: a VersionControl object.
++          rev: the name of the revision to install.
++          extra_args: a list of extra options.
++        """
++        if extra_args is None:
++            extra_args = []
++
++        self.extra_args = extra_args
++        self.rev = rev
++        self.vcs = vcs
++
++    def __repr__(self):
++        return '<RevOptions {}: rev={!r}>'.format(self.vcs.name, self.rev)
++
++    @property
++    def arg_rev(self):
++        if self.rev is None:
++            return self.vcs.default_arg_rev
++
++        return self.rev
++
++    def to_args(self):
++        """
++        Return the VCS-specific command arguments.
++        """
++        args = []
++        rev = self.arg_rev
++        if rev is not None:
++            args += self.vcs.get_base_rev_args(rev)
++        args += self.extra_args
++
++        return args
++
++    def to_display(self):
++        if not self.rev:
++            return ''
++
++        return ' (to revision {})'.format(self.rev)
++
++    def make_new(self, rev):
++        """
++        Make a copy of the current instance, but with a new rev.
++
++        Args:
++          rev: the name of the revision for the new object.
++        """
++        return self.vcs.make_rev_options(rev, extra_args=self.extra_args)
++
++
++class VcsSupport(object):
++    _registry = {}  # type: Dict[str, Command]
++    schemes = ['ssh', 'git', 'hg', 'bzr', 'sftp', 'svn']
++
++    def __init__(self):
++        # Register more schemes with urlparse for various version control
++        # systems
++        urllib_parse.uses_netloc.extend(self.schemes)
++        # Python >= 2.7.4, 3.3 doesn't have uses_fragment
++        if getattr(urllib_parse, 'uses_fragment', None):
++            urllib_parse.uses_fragment.extend(self.schemes)
++        super(VcsSupport, self).__init__()
++
++    def __iter__(self):
++        return self._registry.__iter__()
++
++    @property
++    def backends(self):
++        return list(self._registry.values())
++
++    @property
++    def dirnames(self):
++        return [backend.dirname for backend in self.backends]
++
++    @property
++    def all_schemes(self):
++        schemes = []
++        for backend in self.backends:
++            schemes.extend(backend.schemes)
++        return schemes
++
++    def register(self, cls):
++        if not hasattr(cls, 'name'):
++            logger.warning('Cannot register VCS %s', cls.__name__)
++            return
++        if cls.name not in self._registry:
++            self._registry[cls.name] = cls
++            logger.debug('Registered VCS backend: %s', cls.name)
++
++    def unregister(self, cls=None, name=None):
++        if name in self._registry:
++            del self._registry[name]
++        elif cls in self._registry.values():
++            del self._registry[cls.name]
++        else:
++            logger.warning('Cannot unregister because no class or name given')
++
++    def get_backend_name(self, location):
++        """
++        Return the name of the version control backend if found at given
++        location, e.g. vcs.get_backend_name('/path/to/vcs/checkout')
++        """
++        for vc_type in self._registry.values():
++            if vc_type.controls_location(location):
++                logger.debug('Determine that %s uses VCS: %s',
++                             location, vc_type.name)
++                return vc_type.name
++        return None
++
++    def get_backend(self, name):
++        name = name.lower()
++        if name in self._registry:
++            return self._registry[name]
++
++    def get_backend_from_location(self, location):
++        vc_type = self.get_backend_name(location)
++        if vc_type:
++            return self.get_backend(vc_type)
++        return None
++
++
++vcs = VcsSupport()
++
++
++class VersionControl(object):
++    name = ''
++    dirname = ''
++    # List of supported schemes for this Version Control
++    schemes = ()  # type: Tuple[str, ...]
++    # Iterable of environment variable names to pass to call_subprocess().
++    unset_environ = ()  # type: Tuple[str, ...]
++    default_arg_rev = None  # type: Optional[str]
++
++    def __init__(self, url=None, *args, **kwargs):
++        self.url = url
++        super(VersionControl, self).__init__(*args, **kwargs)
++
++    def get_base_rev_args(self, rev):
++        """
++        Return the base revision arguments for a vcs command.
++
++        Args:
++          rev: the name of a revision to install.  Cannot be None.
++        """
++        raise NotImplementedError
++
++    def make_rev_options(self, rev=None, extra_args=None):
++        """
++        Return a RevOptions object.
++
++        Args:
++          rev: the name of a revision to install.
++          extra_args: a list of extra options.
++        """
++        return RevOptions(self, rev, extra_args=extra_args)
++
++    def _is_local_repository(self, repo):
++        """
++           posix absolute paths start with os.path.sep,
++           win32 ones start with drive (like c:\\folder)
++        """
++        drive, tail = os.path.splitdrive(repo)
++        return repo.startswith(os.path.sep) or drive
++
++    # See issue #1083 for why this method was introduced:
++    # https://github.com/pypa/pip/issues/1083
++    def translate_egg_surname(self, surname):
++        # For example, Django has branches of the form "stable/1.7.x".
++        return surname.replace('/', '_')
++
++    def export(self, location):
++        """
++        Export the repository at the url to the destination location
++        i.e. only download the files, without vcs informations
++        """
++        raise NotImplementedError
++
++    def get_url_rev(self):
++        """
++        Returns the correct repository URL and revision by parsing the given
++        repository URL
++        """
++        error_message = (
++            "Sorry, '%s' is a malformed VCS url. "
++            "The format is <vcs>+<protocol>://<url>, "
++            "e.g. svn+http://myrepo/svn/MyApp#egg=MyApp"
++        )
++        assert '+' in self.url, error_message % self.url
++        url = self.url.split('+', 1)[1]
++        scheme, netloc, path, query, frag = urllib_parse.urlsplit(url)
++        rev = None
++        if '@' in path:
++            path, rev = path.rsplit('@', 1)
++        url = urllib_parse.urlunsplit((scheme, netloc, path, query, ''))
++        return url, rev
++
++    def get_info(self, location):
++        """
++        Returns (url, revision), where both are strings
++        """
++        assert not location.rstrip('/').endswith(self.dirname), \
++            'Bad directory: %s' % location
++        return self.get_url(location), self.get_revision(location)
++
++    def normalize_url(self, url):
++        """
++        Normalize a URL for comparison by unquoting it and removing any
++        trailing slash.
++        """
++        return urllib_parse.unquote(url).rstrip('/')
++
++    def compare_urls(self, url1, url2):
++        """
++        Compare two repo URLs for identity, ignoring incidental differences.
++        """
++        return (self.normalize_url(url1) == self.normalize_url(url2))
++
++    def obtain(self, dest):
++        """
++        Called when installing or updating an editable package, takes the
++        source path of the checkout.
++        """
++        raise NotImplementedError
++
++    def switch(self, dest, url, rev_options):
++        """
++        Switch the repo at ``dest`` to point to ``URL``.
++
++        Args:
++          rev_options: a RevOptions object.
++        """
++        raise NotImplementedError
++
++    def update(self, dest, rev_options):
++        """
++        Update an already-existing repo to the given ``rev_options``.
++
++        Args:
++          rev_options: a RevOptions object.
++        """
++        raise NotImplementedError
++
++    def is_commit_id_equal(self, dest, name):
++        """
++        Return whether the id of the current commit equals the given name.
++
++        Args:
++          dest: the repository directory.
++          name: a string name.
++        """
++        raise NotImplementedError
++
++    def check_destination(self, dest, url, rev_options):
++        """
++        Prepare a location to receive a checkout/clone.
++
++        Return True if the location is ready for (and requires) a
++        checkout/clone, False otherwise.
++
++        Args:
++          rev_options: a RevOptions object.
++        """
++        checkout = True
++        prompt = False
++        rev_display = rev_options.to_display()
++        if os.path.exists(dest):
++            checkout = False
++            if os.path.exists(os.path.join(dest, self.dirname)):
++                existing_url = self.get_url(dest)
++                if self.compare_urls(existing_url, url):
++                    logger.debug(
++                        '%s in %s exists, and has correct URL (%s)',
++                        self.repo_name.title(),
++                        display_path(dest),
++                        url,
++                    )
++                    if not self.is_commit_id_equal(dest, rev_options.rev):
++                        logger.info(
++                            'Updating %s %s%s',
++                            display_path(dest),
++                            self.repo_name,
++                            rev_display,
++                        )
++                        self.update(dest, rev_options)
++                    else:
++                        logger.info(
++                            'Skipping because already up-to-date.')
++                else:
++                    logger.warning(
++                        '%s %s in %s exists with URL %s',
++                        self.name,
++                        self.repo_name,
++                        display_path(dest),
++                        existing_url,
++                    )
++                    prompt = ('(s)witch, (i)gnore, (w)ipe, (b)ackup ',
++                              ('s', 'i', 'w', 'b'))
++            else:
++                logger.warning(
++                    'Directory %s already exists, and is not a %s %s.',
++                    dest,
++                    self.name,
++                    self.repo_name,
++                )
++                prompt = ('(i)gnore, (w)ipe, (b)ackup ', ('i', 'w', 'b'))
++        if prompt:
++            logger.warning(
++                'The plan is to install the %s repository %s',
++                self.name,
++                url,
++            )
++            response = ask_path_exists('What to do?  %s' % prompt[0],
++                                       prompt[1])
++
++            if response == 's':
++                logger.info(
++                    'Switching %s %s to %s%s',
++                    self.repo_name,
++                    display_path(dest),
++                    url,
++                    rev_display,
++                )
++                self.switch(dest, url, rev_options)
++            elif response == 'i':
++                # do nothing
++                pass
++            elif response == 'w':
++                logger.warning('Deleting %s', display_path(dest))
++                rmtree(dest)
++                checkout = True
++            elif response == 'b':
++                dest_dir = backup_dir(dest)
++                logger.warning(
++                    'Backing up %s to %s', display_path(dest), dest_dir,
++                )
++                shutil.move(dest, dest_dir)
++                checkout = True
++            elif response == 'a':
++                sys.exit(-1)
++        return checkout
++
++    def unpack(self, location):
++        """
++        Clean up current location and download the url repository
++        (and vcs infos) into location
++        """
++        if os.path.exists(location):
++            rmtree(location)
++        self.obtain(location)
++
++    def get_src_requirement(self, dist, location):
++        """
++        Return a string representing the requirement needed to
++        redownload the files currently present in location, something
++        like:
++          {repository_url}@{revision}#egg={project_name}-{version_identifier}
++        """
++        raise NotImplementedError
++
++    def get_url(self, location):
++        """
++        Return the url used at location
++        Used in get_info or check_destination
++        """
++        raise NotImplementedError
++
++    def get_revision(self, location):
++        """
++        Return the current commit id of the files at the given location.
++        """
++        raise NotImplementedError
++
++    def run_command(self, cmd, show_stdout=True, cwd=None,
++                    on_returncode='raise',
++                    command_desc=None,
++                    extra_environ=None, spinner=None):
++        """
++        Run a VCS subcommand
++        This is simply a wrapper around call_subprocess that adds the VCS
++        command name, and checks that the VCS is available
++        """
++        cmd = [self.name] + cmd
++        try:
++            return call_subprocess(cmd, show_stdout, cwd,
++                                   on_returncode,
++                                   command_desc, extra_environ,
++                                   unset_environ=self.unset_environ,
++                                   spinner=spinner)
++        except OSError as e:
++            # errno.ENOENT = no such file or directory
++            # In other words, the VCS executable isn't available
++            if e.errno == errno.ENOENT:
++                raise BadCommand(
++                    'Cannot find command %r - do you have '
++                    '%r installed and in your '
++                    'PATH?' % (self.name, self.name))
++            else:
++                raise  # re-raise exception if a different error occurred
++
++    @classmethod
++    def controls_location(cls, location):
++        """
++        Check if a location is controlled by the vcs.
++        It is meant to be overridden to implement smarter detection
++        mechanisms for specific vcs.
++        """
++        logger.debug('Checking in %s for %s (%s)...',
++                     location, cls.dirname, cls.name)
++        path = os.path.join(location, cls.dirname)
++        return os.path.exists(path)
++
++
++def get_src_requirement(dist, location):
++    version_control = vcs.get_backend_from_location(location)
++    if version_control:
++        try:
++            return version_control().get_src_requirement(dist,
++                                                         location)
++        except BadCommand:
++            logger.warning(
++                'cannot determine version of editable source in %s '
++                '(%s command not found in path)',
++                location,
++                version_control.name,
++            )
++            return dist.as_requirement()
++    logger.warning(
++        'cannot determine version of editable source in %s (is not SVN '
++        'checkout, Git clone, Mercurial clone or Bazaar branch)',
++        location,
++    )
++    return dist.as_requirement()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/ui.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/ui.py	(date 1573549700401)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/ui.py	(date 1573549700401)
+@@ -0,0 +1,421 @@
++from __future__ import absolute_import, division
++
++import contextlib
++import itertools
++import logging
++import sys
++import time
++from signal import SIGINT, default_int_handler, signal
++
++from pip._vendor import six
++from pip._vendor.progress.bar import (
++    Bar, ChargingBar, FillingCirclesBar, FillingSquaresBar, IncrementalBar,
++    ShadyBar,
++)
++from pip._vendor.progress.helpers import HIDE_CURSOR, SHOW_CURSOR, WritelnMixin
++from pip._vendor.progress.spinner import Spinner
++
++from pip._internal.compat import WINDOWS
++from pip._internal.utils.logging import get_indentation
++from pip._internal.utils.misc import format_size
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++
++if MYPY_CHECK_RUNNING:
++    from typing import Any
++
++try:
++    from pip._vendor import colorama
++# Lots of different errors can come from this, including SystemError and
++# ImportError.
++except Exception:
++    colorama = None
++
++logger = logging.getLogger(__name__)
++
++
++def _select_progress_class(preferred, fallback):
++    encoding = getattr(preferred.file, "encoding", None)
++
++    # If we don't know what encoding this file is in, then we'll just assume
++    # that it doesn't support unicode and use the ASCII bar.
++    if not encoding:
++        return fallback
++
++    # Collect all of the possible characters we want to use with the preferred
++    # bar.
++    characters = [
++        getattr(preferred, "empty_fill", six.text_type()),
++        getattr(preferred, "fill", six.text_type()),
++    ]
++    characters += list(getattr(preferred, "phases", []))
++
++    # Try to decode the characters we're using for the bar using the encoding
++    # of the given file, if this works then we'll assume that we can use the
++    # fancier bar and if not we'll fall back to the plaintext bar.
++    try:
++        six.text_type().join(characters).encode(encoding)
++    except UnicodeEncodeError:
++        return fallback
++    else:
++        return preferred
++
++
++_BaseBar = _select_progress_class(IncrementalBar, Bar)  # type: Any
++
++
++class InterruptibleMixin(object):
++    """
++    Helper to ensure that self.finish() gets called on keyboard interrupt.
++
++    This allows downloads to be interrupted without leaving temporary state
++    (like hidden cursors) behind.
++
++    This class is similar to the progress library's existing SigIntMixin
++    helper, but as of version 1.2, that helper has the following problems:
++
++    1. It calls sys.exit().
++    2. It discards the existing SIGINT handler completely.
++    3. It leaves its own handler in place even after an uninterrupted finish,
++       which will have unexpected delayed effects if the user triggers an
++       unrelated keyboard interrupt some time after a progress-displaying
++       download has already completed, for example.
++    """
++
++    def __init__(self, *args, **kwargs):
++        """
++        Save the original SIGINT handler for later.
++        """
++        super(InterruptibleMixin, self).__init__(*args, **kwargs)
++
++        self.original_handler = signal(SIGINT, self.handle_sigint)
++
++        # If signal() returns None, the previous handler was not installed from
++        # Python, and we cannot restore it. This probably should not happen,
++        # but if it does, we must restore something sensible instead, at least.
++        # The least bad option should be Python's default SIGINT handler, which
++        # just raises KeyboardInterrupt.
++        if self.original_handler is None:
++            self.original_handler = default_int_handler
++
++    def finish(self):
++        """
++        Restore the original SIGINT handler after finishing.
++
++        This should happen regardless of whether the progress display finishes
++        normally, or gets interrupted.
++        """
++        super(InterruptibleMixin, self).finish()
++        signal(SIGINT, self.original_handler)
++
++    def handle_sigint(self, signum, frame):
++        """
++        Call self.finish() before delegating to the original SIGINT handler.
++
++        This handler should only be in place while the progress display is
++        active.
++        """
++        self.finish()
++        self.original_handler(signum, frame)
++
++
++class SilentBar(Bar):
++
++    def update(self):
++        pass
++
++
++class BlueEmojiBar(IncrementalBar):
++
++    suffix = "%(percent)d%%"
++    bar_prefix = " "
++    bar_suffix = " "
++    phases = (u"\U0001F539", u"\U0001F537", u"\U0001F535")  # type: Any
++
++
++class DownloadProgressMixin(object):
++
++    def __init__(self, *args, **kwargs):
++        super(DownloadProgressMixin, self).__init__(*args, **kwargs)
++        self.message = (" " * (get_indentation() + 2)) + self.message
++
++    @property
++    def downloaded(self):
++        return format_size(self.index)
++
++    @property
++    def download_speed(self):
++        # Avoid zero division errors...
++        if self.avg == 0.0:
++            return "..."
++        return format_size(1 / self.avg) + "/s"
++
++    @property
++    def pretty_eta(self):
++        if self.eta:
++            return "eta %s" % self.eta_td
++        return ""
++
++    def iter(self, it, n=1):
++        for x in it:
++            yield x
++            self.next(n)
++        self.finish()
++
++
++class WindowsMixin(object):
++
++    def __init__(self, *args, **kwargs):
++        # The Windows terminal does not support the hide/show cursor ANSI codes
++        # even with colorama. So we'll ensure that hide_cursor is False on
++        # Windows.
++        # This call neds to go before the super() call, so that hide_cursor
++        # is set in time. The base progress bar class writes the "hide cursor"
++        # code to the terminal in its init, so if we don't set this soon
++        # enough, we get a "hide" with no corresponding "show"...
++        if WINDOWS and self.hide_cursor:
++            self.hide_cursor = False
++
++        super(WindowsMixin, self).__init__(*args, **kwargs)
++
++        # Check if we are running on Windows and we have the colorama module,
++        # if we do then wrap our file with it.
++        if WINDOWS and colorama:
++            self.file = colorama.AnsiToWin32(self.file)
++            # The progress code expects to be able to call self.file.isatty()
++            # but the colorama.AnsiToWin32() object doesn't have that, so we'll
++            # add it.
++            self.file.isatty = lambda: self.file.wrapped.isatty()
++            # The progress code expects to be able to call self.file.flush()
++            # but the colorama.AnsiToWin32() object doesn't have that, so we'll
++            # add it.
++            self.file.flush = lambda: self.file.wrapped.flush()
++
++
++class BaseDownloadProgressBar(WindowsMixin, InterruptibleMixin,
++                              DownloadProgressMixin):
++
++    file = sys.stdout
++    message = "%(percent)d%%"
++    suffix = "%(downloaded)s %(download_speed)s %(pretty_eta)s"
++
++# NOTE: The "type: ignore" comments on the following classes are there to
++#       work around https://github.com/python/typing/issues/241
++
++
++class DefaultDownloadProgressBar(BaseDownloadProgressBar,
++                                 _BaseBar):  # type: ignore
++    pass
++
++
++class DownloadSilentBar(BaseDownloadProgressBar, SilentBar):  # type: ignore
++    pass
++
++
++class DownloadIncrementalBar(BaseDownloadProgressBar,  # type: ignore
++                             IncrementalBar):
++    pass
++
++
++class DownloadChargingBar(BaseDownloadProgressBar,  # type: ignore
++                          ChargingBar):
++    pass
++
++
++class DownloadShadyBar(BaseDownloadProgressBar, ShadyBar):  # type: ignore
++    pass
++
++
++class DownloadFillingSquaresBar(BaseDownloadProgressBar,  # type: ignore
++                                FillingSquaresBar):
++    pass
++
++
++class DownloadFillingCirclesBar(BaseDownloadProgressBar,  # type: ignore
++                                FillingCirclesBar):
++    pass
++
++
++class DownloadBlueEmojiProgressBar(BaseDownloadProgressBar,  # type: ignore
++                                   BlueEmojiBar):
++    pass
++
++
++class DownloadProgressSpinner(WindowsMixin, InterruptibleMixin,
++                              DownloadProgressMixin, WritelnMixin, Spinner):
++
++    file = sys.stdout
++    suffix = "%(downloaded)s %(download_speed)s"
++
++    def next_phase(self):
++        if not hasattr(self, "_phaser"):
++            self._phaser = itertools.cycle(self.phases)
++        return next(self._phaser)
++
++    def update(self):
++        message = self.message % self
++        phase = self.next_phase()
++        suffix = self.suffix % self
++        line = ''.join([
++            message,
++            " " if message else "",
++            phase,
++            " " if suffix else "",
++            suffix,
++        ])
++
++        self.writeln(line)
++
++
++BAR_TYPES = {
++    "off": (DownloadSilentBar, DownloadSilentBar),
++    "on": (DefaultDownloadProgressBar, DownloadProgressSpinner),
++    "ascii": (DownloadIncrementalBar, DownloadProgressSpinner),
++    "pretty": (DownloadFillingCirclesBar, DownloadProgressSpinner),
++    "emoji": (DownloadBlueEmojiProgressBar, DownloadProgressSpinner)
++}
++
++
++def DownloadProgressProvider(progress_bar, max=None):
++    if max is None or max == 0:
++        return BAR_TYPES[progress_bar][1]().iter
++    else:
++        return BAR_TYPES[progress_bar][0](max=max).iter
++
++
++################################################################
++# Generic "something is happening" spinners
++#
++# We don't even try using progress.spinner.Spinner here because it's actually
++# simpler to reimplement from scratch than to coerce their code into doing
++# what we need.
++################################################################
++
++@contextlib.contextmanager
++def hidden_cursor(file):
++    # The Windows terminal does not support the hide/show cursor ANSI codes,
++    # even via colorama. So don't even try.
++    if WINDOWS:
++        yield
++    # We don't want to clutter the output with control characters if we're
++    # writing to a file, or if the user is running with --quiet.
++    # See https://github.com/pypa/pip/issues/3418
++    elif not file.isatty() or logger.getEffectiveLevel() > logging.INFO:
++        yield
++    else:
++        file.write(HIDE_CURSOR)
++        try:
++            yield
++        finally:
++            file.write(SHOW_CURSOR)
++
++
++class RateLimiter(object):
++    def __init__(self, min_update_interval_seconds):
++        self._min_update_interval_seconds = min_update_interval_seconds
++        self._last_update = 0
++
++    def ready(self):
++        now = time.time()
++        delta = now - self._last_update
++        return delta >= self._min_update_interval_seconds
++
++    def reset(self):
++        self._last_update = time.time()
++
++
++class InteractiveSpinner(object):
++    def __init__(self, message, file=None, spin_chars="-\\|/",
++                 # Empirically, 8 updates/second looks nice
++                 min_update_interval_seconds=0.125):
++        self._message = message
++        if file is None:
++            file = sys.stdout
++        self._file = file
++        self._rate_limiter = RateLimiter(min_update_interval_seconds)
++        self._finished = False
++
++        self._spin_cycle = itertools.cycle(spin_chars)
++
++        self._file.write(" " * get_indentation() + self._message + " ... ")
++        self._width = 0
++
++    def _write(self, status):
++        assert not self._finished
++        # Erase what we wrote before by backspacing to the beginning, writing
++        # spaces to overwrite the old text, and then backspacing again
++        backup = "\b" * self._width
++        self._file.write(backup + " " * self._width + backup)
++        # Now we have a blank slate to add our status
++        self._file.write(status)
++        self._width = len(status)
++        self._file.flush()
++        self._rate_limiter.reset()
++
++    def spin(self):
++        if self._finished:
++            return
++        if not self._rate_limiter.ready():
++            return
++        self._write(next(self._spin_cycle))
++
++    def finish(self, final_status):
++        if self._finished:
++            return
++        self._write(final_status)
++        self._file.write("\n")
++        self._file.flush()
++        self._finished = True
++
++
++# Used for dumb terminals, non-interactive installs (no tty), etc.
++# We still print updates occasionally (once every 60 seconds by default) to
++# act as a keep-alive for systems like Travis-CI that take lack-of-output as
++# an indication that a task has frozen.
++class NonInteractiveSpinner(object):
++    def __init__(self, message, min_update_interval_seconds=60):
++        self._message = message
++        self._finished = False
++        self._rate_limiter = RateLimiter(min_update_interval_seconds)
++        self._update("started")
++
++    def _update(self, status):
++        assert not self._finished
++        self._rate_limiter.reset()
++        logger.info("%s: %s", self._message, status)
++
++    def spin(self):
++        if self._finished:
++            return
++        if not self._rate_limiter.ready():
++            return
++        self._update("still running...")
++
++    def finish(self, final_status):
++        if self._finished:
++            return
++        self._update("finished with status '%s'" % (final_status,))
++        self._finished = True
++
++
++@contextlib.contextmanager
++def open_spinner(message):
++    # Interactive spinner goes directly to sys.stdout rather than being routed
++    # through the logging system, but it acts like it has level INFO,
++    # i.e. it's only displayed if we're at level INFO or better.
++    # Non-interactive spinner goes through the logging system, so it is always
++    # in sync with logging configuration.
++    if sys.stdout.isatty() and logger.getEffectiveLevel() <= logging.INFO:
++        spinner = InteractiveSpinner(message)
++    else:
++        spinner = NonInteractiveSpinner(message)
++    try:
++        with hidden_cursor(sys.stdout):
++            yield spinner
++    except KeyboardInterrupt:
++        spinner.finish("canceled")
++        raise
++    except Exception:
++        spinner.finish("error")
++        raise
++    else:
++        spinner.finish("done")
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/temp_dir.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/temp_dir.py	(date 1573549700383)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/temp_dir.py	(date 1573549700383)
+@@ -0,0 +1,82 @@
++from __future__ import absolute_import
++
++import logging
++import os.path
++import tempfile
++
++from pip._internal.utils.misc import rmtree
++
++logger = logging.getLogger(__name__)
++
++
++class TempDirectory(object):
++    """Helper class that owns and cleans up a temporary directory.
++
++    This class can be used as a context manager or as an OO representation of a
++    temporary directory.
++
++    Attributes:
++        path
++            Location to the created temporary directory or None
++        delete
++            Whether the directory should be deleted when exiting
++            (when used as a contextmanager)
++
++    Methods:
++        create()
++            Creates a temporary directory and stores its path in the path
++            attribute.
++        cleanup()
++            Deletes the temporary directory and sets path attribute to None
++
++    When used as a context manager, a temporary directory is created on
++    entering the context and, if the delete attribute is True, on exiting the
++    context the created directory is deleted.
++    """
++
++    def __init__(self, path=None, delete=None, kind="temp"):
++        super(TempDirectory, self).__init__()
++
++        if path is None and delete is None:
++            # If we were not given an explicit directory, and we were not given
++            # an explicit delete option, then we'll default to deleting.
++            delete = True
++
++        self.path = path
++        self.delete = delete
++        self.kind = kind
++
++    def __repr__(self):
++        return "<{} {!r}>".format(self.__class__.__name__, self.path)
++
++    def __enter__(self):
++        self.create()
++        return self
++
++    def __exit__(self, exc, value, tb):
++        if self.delete:
++            self.cleanup()
++
++    def create(self):
++        """Create a temporary directory and store it's path in self.path
++        """
++        if self.path is not None:
++            logger.debug(
++                "Skipped creation of temporary directory: {}".format(self.path)
++            )
++            return
++        # We realpath here because some systems have their default tmpdir
++        # symlinked to another directory.  This tends to confuse build
++        # scripts, so we canonicalize the path by traversing potential
++        # symlinks here.
++        self.path = os.path.realpath(
++            tempfile.mkdtemp(prefix="pip-{}-".format(self.kind))
++        )
++        logger.debug("Created temporary directory: {}".format(self.path))
++
++    def cleanup(self):
++        """Remove the temporary directory created and reset state
++        """
++        if self.path is not None and os.path.exists(self.path):
++            rmtree(self.path)
++        self.path = None
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/glibc.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/glibc.py	(date 1573549700328)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/glibc.py	(date 1573549700328)
+@@ -0,0 +1,84 @@
++from __future__ import absolute_import
++
++import ctypes
++import re
++import warnings
++
++
++def glibc_version_string():
++    "Returns glibc version string, or None if not using glibc."
++
++    # ctypes.CDLL(None) internally calls dlopen(NULL), and as the dlopen
++    # manpage says, "If filename is NULL, then the returned handle is for the
++    # main program". This way we can let the linker do the work to figure out
++    # which libc our process is actually using.
++    process_namespace = ctypes.CDLL(None)
++    try:
++        gnu_get_libc_version = process_namespace.gnu_get_libc_version
++    except AttributeError:
++        # Symbol doesn't exist -> therefore, we are not linked to
++        # glibc.
++        return None
++
++    # Call gnu_get_libc_version, which returns a string like "2.5"
++    gnu_get_libc_version.restype = ctypes.c_char_p
++    version_str = gnu_get_libc_version()
++    # py2 / py3 compatibility:
++    if not isinstance(version_str, str):
++        version_str = version_str.decode("ascii")
++
++    return version_str
++
++
++# Separated out from have_compatible_glibc for easier unit testing
++def check_glibc_version(version_str, required_major, minimum_minor):
++    # Parse string and check against requested version.
++    #
++    # We use a regexp instead of str.split because we want to discard any
++    # random junk that might come after the minor version -- this might happen
++    # in patched/forked versions of glibc (e.g. Linaro's version of glibc
++    # uses version strings like "2.20-2014.11"). See gh-3588.
++    m = re.match(r"(?P<major>[0-9]+)\.(?P<minor>[0-9]+)", version_str)
++    if not m:
++        warnings.warn("Expected glibc version with 2 components major.minor,"
++                      " got: %s" % version_str, RuntimeWarning)
++        return False
++    return (int(m.group("major")) == required_major and
++            int(m.group("minor")) >= minimum_minor)
++
++
++def have_compatible_glibc(required_major, minimum_minor):
++    version_str = glibc_version_string()
++    if version_str is None:
++        return False
++    return check_glibc_version(version_str, required_major, minimum_minor)
++
++
++# platform.libc_ver regularly returns completely nonsensical glibc
++# versions. E.g. on my computer, platform says:
++#
++#   ~$ python2.7 -c 'import platform; print(platform.libc_ver())'
++#   ('glibc', '2.7')
++#   ~$ python3.5 -c 'import platform; print(platform.libc_ver())'
++#   ('glibc', '2.9')
++#
++# But the truth is:
++#
++#   ~$ ldd --version
++#   ldd (Debian GLIBC 2.22-11) 2.22
++#
++# This is unfortunate, because it means that the linehaul data on libc
++# versions that was generated by pip 8.1.2 and earlier is useless and
++# misleading. Solution: instead of using platform, use our code that actually
++# works.
++def libc_ver():
++    """Try to determine the glibc version
++
++    Returns a tuple of strings (lib, version) which default to empty strings
++    in case the lookup fails.
++    """
++    glibc_version = glibc_version_string()
++    if glibc_version is None:
++        return ("", "")
++    else:
++        return ("glibc", glibc_version)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/packaging.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/packaging.py	(date 1573549700366)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/packaging.py	(date 1573549700366)
+@@ -0,0 +1,70 @@
++from __future__ import absolute_import
++
++import logging
++import sys
++from email.parser import FeedParser  # type: ignore
++
++from pip._vendor import pkg_resources
++from pip._vendor.packaging import specifiers, version
++
++from pip._internal import exceptions
++
++logger = logging.getLogger(__name__)
++
++
++def check_requires_python(requires_python):
++    """
++    Check if the python version in use match the `requires_python` specifier.
++
++    Returns `True` if the version of python in use matches the requirement.
++    Returns `False` if the version of python in use does not matches the
++    requirement.
++
++    Raises an InvalidSpecifier if `requires_python` have an invalid format.
++    """
++    if requires_python is None:
++        # The package provides no information
++        return True
++    requires_python_specifier = specifiers.SpecifierSet(requires_python)
++
++    # We only use major.minor.micro
++    python_version = version.parse('.'.join(map(str, sys.version_info[:3])))
++    return python_version in requires_python_specifier
++
++
++def get_metadata(dist):
++    if (isinstance(dist, pkg_resources.DistInfoDistribution) and
++            dist.has_metadata('METADATA')):
++        return dist.get_metadata('METADATA')
++    elif dist.has_metadata('PKG-INFO'):
++        return dist.get_metadata('PKG-INFO')
++
++
++def check_dist_requires_python(dist):
++    metadata = get_metadata(dist)
++    feed_parser = FeedParser()
++    feed_parser.feed(metadata)
++    pkg_info_dict = feed_parser.close()
++    requires_python = pkg_info_dict.get('Requires-Python')
++    try:
++        if not check_requires_python(requires_python):
++            raise exceptions.UnsupportedPythonVersion(
++                "%s requires Python '%s' but the running Python is %s" % (
++                    dist.project_name,
++                    requires_python,
++                    '.'.join(map(str, sys.version_info[:3])),)
++            )
++    except specifiers.InvalidSpecifier as e:
++        logger.warning(
++            "Package %s has an invalid Requires-Python entry %s - %s",
++            dist.project_name, requires_python, e,
++        )
++        return
++
++
++def get_installer(dist):
++    if dist.has_metadata('INSTALLER'):
++        for line in dist.get_metadata_lines('INSTALLER'):
++            if line.strip():
++                return line.strip()
++    return ''
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/setuptools_build.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/setuptools_build.py	(date 1573549700376)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/setuptools_build.py	(date 1573549700376)
+@@ -0,0 +1,8 @@
++# Shim to wrap setup.py invocation with setuptools
++SETUPTOOLS_SHIM = (
++    "import setuptools, tokenize;__file__=%r;"
++    "f=getattr(tokenize, 'open', open)(__file__);"
++    "code=f.read().replace('\\r\\n', '\\n');"
++    "f.close();"
++    "exec(compile(code, __file__, 'exec'))"
++)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/appdirs.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/appdirs.py	(date 1573549700297)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/appdirs.py	(date 1573549700297)
+@@ -0,0 +1,258 @@
++"""
++This code was taken from https://github.com/ActiveState/appdirs and modified
++to suit our purposes.
++"""
++from __future__ import absolute_import
++
++import os
++import sys
++
++from pip._vendor.six import PY2, text_type
++
++from pip._internal.compat import WINDOWS, expanduser
++
++
++def user_cache_dir(appname):
++    r"""
++    Return full path to the user-specific cache dir for this application.
++
++        "appname" is the name of application.
++
++    Typical user cache directories are:
++        macOS:      ~/Library/Caches/<AppName>
++        Unix:       ~/.cache/<AppName> (XDG default)
++        Windows:    C:\Users\<username>\AppData\Local\<AppName>\Cache
++
++    On Windows the only suggestion in the MSDN docs is that local settings go
++    in the `CSIDL_LOCAL_APPDATA` directory. This is identical to the
++    non-roaming app data dir (the default returned by `user_data_dir`). Apps
++    typically put cache data somewhere *under* the given dir here. Some
++    examples:
++        ...\Mozilla\Firefox\Profiles\<ProfileName>\Cache
++        ...\Acme\SuperApp\Cache\1.0
++
++    OPINION: This function appends "Cache" to the `CSIDL_LOCAL_APPDATA` value.
++    """
++    if WINDOWS:
++        # Get the base path
++        path = os.path.normpath(_get_win_folder("CSIDL_LOCAL_APPDATA"))
++
++        # When using Python 2, return paths as bytes on Windows like we do on
++        # other operating systems. See helper function docs for more details.
++        if PY2 and isinstance(path, text_type):
++            path = _win_path_to_bytes(path)
++
++        # Add our app name and Cache directory to it
++        path = os.path.join(path, appname, "Cache")
++    elif sys.platform == "darwin":
++        # Get the base path
++        path = expanduser("~/Library/Caches")
++
++        # Add our app name to it
++        path = os.path.join(path, appname)
++    else:
++        # Get the base path
++        path = os.getenv("XDG_CACHE_HOME", expanduser("~/.cache"))
++
++        # Add our app name to it
++        path = os.path.join(path, appname)
++
++    return path
++
++
++def user_data_dir(appname, roaming=False):
++    r"""
++    Return full path to the user-specific data dir for this application.
++
++        "appname" is the name of application.
++            If None, just the system directory is returned.
++        "roaming" (boolean, default False) can be set True to use the Windows
++            roaming appdata directory. That means that for users on a Windows
++            network setup for roaming profiles, this user data will be
++            sync'd on login. See
++            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>
++            for a discussion of issues.
++
++    Typical user data directories are:
++        macOS:                  ~/Library/Application Support/<AppName>
++                                if it exists, else ~/.config/<AppName>
++        Unix:                   ~/.local/share/<AppName>    # or in
++                                $XDG_DATA_HOME, if defined
++        Win XP (not roaming):   C:\Documents and Settings\<username>\ ...
++                                ...Application Data\<AppName>
++        Win XP (roaming):       C:\Documents and Settings\<username>\Local ...
++                                ...Settings\Application Data\<AppName>
++        Win 7  (not roaming):   C:\\Users\<username>\AppData\Local\<AppName>
++        Win 7  (roaming):       C:\\Users\<username>\AppData\Roaming\<AppName>
++
++    For Unix, we follow the XDG spec and support $XDG_DATA_HOME.
++    That means, by default "~/.local/share/<AppName>".
++    """
++    if WINDOWS:
++        const = roaming and "CSIDL_APPDATA" or "CSIDL_LOCAL_APPDATA"
++        path = os.path.join(os.path.normpath(_get_win_folder(const)), appname)
++    elif sys.platform == "darwin":
++        path = os.path.join(
++            expanduser('~/Library/Application Support/'),
++            appname,
++        ) if os.path.isdir(os.path.join(
++            expanduser('~/Library/Application Support/'),
++            appname,
++        )
++        ) else os.path.join(
++            expanduser('~/.config/'),
++            appname,
++        )
++    else:
++        path = os.path.join(
++            os.getenv('XDG_DATA_HOME', expanduser("~/.local/share")),
++            appname,
++        )
++
++    return path
++
++
++def user_config_dir(appname, roaming=True):
++    """Return full path to the user-specific config dir for this application.
++
++        "appname" is the name of application.
++            If None, just the system directory is returned.
++        "roaming" (boolean, default True) can be set False to not use the
++            Windows roaming appdata directory. That means that for users on a
++            Windows network setup for roaming profiles, this user data will be
++            sync'd on login. See
++            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>
++            for a discussion of issues.
++
++    Typical user data directories are:
++        macOS:                  same as user_data_dir
++        Unix:                   ~/.config/<AppName>
++        Win *:                  same as user_data_dir
++
++    For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME.
++    That means, by default "~/.config/<AppName>".
++    """
++    if WINDOWS:
++        path = user_data_dir(appname, roaming=roaming)
++    elif sys.platform == "darwin":
++        path = user_data_dir(appname)
++    else:
++        path = os.getenv('XDG_CONFIG_HOME', expanduser("~/.config"))
++        path = os.path.join(path, appname)
++
++    return path
++
++
++# for the discussion regarding site_config_dirs locations
++# see <https://github.com/pypa/pip/issues/1733>
++def site_config_dirs(appname):
++    r"""Return a list of potential user-shared config dirs for this application.
++
++        "appname" is the name of application.
++
++    Typical user config directories are:
++        macOS:      /Library/Application Support/<AppName>/
++        Unix:       /etc or $XDG_CONFIG_DIRS[i]/<AppName>/ for each value in
++                    $XDG_CONFIG_DIRS
++        Win XP:     C:\Documents and Settings\All Users\Application ...
++                    ...Data\<AppName>\
++        Vista:      (Fail! "C:\ProgramData" is a hidden *system* directory
++                    on Vista.)
++        Win 7:      Hidden, but writeable on Win 7:
++                    C:\ProgramData\<AppName>\
++    """
++    if WINDOWS:
++        path = os.path.normpath(_get_win_folder("CSIDL_COMMON_APPDATA"))
++        pathlist = [os.path.join(path, appname)]
++    elif sys.platform == 'darwin':
++        pathlist = [os.path.join('/Library/Application Support', appname)]
++    else:
++        # try looking in $XDG_CONFIG_DIRS
++        xdg_config_dirs = os.getenv('XDG_CONFIG_DIRS', '/etc/xdg')
++        if xdg_config_dirs:
++            pathlist = [
++                os.path.join(expanduser(x), appname)
++                for x in xdg_config_dirs.split(os.pathsep)
++            ]
++        else:
++            pathlist = []
++
++        # always look in /etc directly as well
++        pathlist.append('/etc')
++
++    return pathlist
++
++
++# -- Windows support functions --
++
++def _get_win_folder_from_registry(csidl_name):
++    """
++    This is a fallback technique at best. I'm not sure if using the
++    registry for this guarantees us the correct answer for all CSIDL_*
++    names.
++    """
++    import _winreg
++
++    shell_folder_name = {
++        "CSIDL_APPDATA": "AppData",
++        "CSIDL_COMMON_APPDATA": "Common AppData",
++        "CSIDL_LOCAL_APPDATA": "Local AppData",
++    }[csidl_name]
++
++    key = _winreg.OpenKey(
++        _winreg.HKEY_CURRENT_USER,
++        r"Software\Microsoft\Windows\CurrentVersion\Explorer\Shell Folders"
++    )
++    directory, _type = _winreg.QueryValueEx(key, shell_folder_name)
++    return directory
++
++
++def _get_win_folder_with_ctypes(csidl_name):
++    csidl_const = {
++        "CSIDL_APPDATA": 26,
++        "CSIDL_COMMON_APPDATA": 35,
++        "CSIDL_LOCAL_APPDATA": 28,
++    }[csidl_name]
++
++    buf = ctypes.create_unicode_buffer(1024)
++    ctypes.windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)
++
++    # Downgrade to short path name if have highbit chars. See
++    # <http://bugs.activestate.com/show_bug.cgi?id=85099>.
++    has_high_char = False
++    for c in buf:
++        if ord(c) > 255:
++            has_high_char = True
++            break
++    if has_high_char:
++        buf2 = ctypes.create_unicode_buffer(1024)
++        if ctypes.windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):
++            buf = buf2
++
++    return buf.value
++
++
++if WINDOWS:
++    try:
++        import ctypes
++        _get_win_folder = _get_win_folder_with_ctypes
++    except ImportError:
++        _get_win_folder = _get_win_folder_from_registry
++
++
++def _win_path_to_bytes(path):
++    """Encode Windows paths to bytes. Only used on Python 2.
++
++    Motivation is to be consistent with other operating systems where paths
++    are also returned as bytes. This avoids problems mixing bytes and Unicode
++    elsewhere in the codebase. For more details and discussion see
++    <https://github.com/pypa/pip/issues/3463>.
++
++    If encoding using ASCII and MBCS fails, return the original Unicode path.
++    """
++    for encoding in ('ASCII', 'MBCS'):
++        try:
++            return path.encode(encoding)
++        except (UnicodeEncodeError, LookupError):
++            pass
++    return path
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/outdated.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/outdated.py	(date 1573549700358)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/outdated.py	(date 1573549700358)
+@@ -0,0 +1,163 @@
++from __future__ import absolute_import
++
++import datetime
++import json
++import logging
++import os.path
++import sys
++
++from pip._vendor import lockfile
++from pip._vendor.packaging import version as packaging_version
++
++from pip._internal.compat import WINDOWS
++from pip._internal.index import PackageFinder
++from pip._internal.locations import USER_CACHE_DIR, running_under_virtualenv
++from pip._internal.utils.filesystem import check_path_owner
++from pip._internal.utils.misc import ensure_dir, get_installed_version
++
++SELFCHECK_DATE_FMT = "%Y-%m-%dT%H:%M:%SZ"
++
++
++logger = logging.getLogger(__name__)
++
++
++class VirtualenvSelfCheckState(object):
++    def __init__(self):
++        self.statefile_path = os.path.join(sys.prefix, "pip-selfcheck.json")
++
++        # Load the existing state
++        try:
++            with open(self.statefile_path) as statefile:
++                self.state = json.load(statefile)
++        except (IOError, ValueError):
++            self.state = {}
++
++    def save(self, pypi_version, current_time):
++        # Attempt to write out our version check file
++        with open(self.statefile_path, "w") as statefile:
++            json.dump(
++                {
++                    "last_check": current_time.strftime(SELFCHECK_DATE_FMT),
++                    "pypi_version": pypi_version,
++                },
++                statefile,
++                sort_keys=True,
++                separators=(",", ":")
++            )
++
++
++class GlobalSelfCheckState(object):
++    def __init__(self):
++        self.statefile_path = os.path.join(USER_CACHE_DIR, "selfcheck.json")
++
++        # Load the existing state
++        try:
++            with open(self.statefile_path) as statefile:
++                self.state = json.load(statefile)[sys.prefix]
++        except (IOError, ValueError, KeyError):
++            self.state = {}
++
++    def save(self, pypi_version, current_time):
++        # Check to make sure that we own the directory
++        if not check_path_owner(os.path.dirname(self.statefile_path)):
++            return
++
++        # Now that we've ensured the directory is owned by this user, we'll go
++        # ahead and make sure that all our directories are created.
++        ensure_dir(os.path.dirname(self.statefile_path))
++
++        # Attempt to write out our version check file
++        with lockfile.LockFile(self.statefile_path):
++            if os.path.exists(self.statefile_path):
++                with open(self.statefile_path) as statefile:
++                    state = json.load(statefile)
++            else:
++                state = {}
++
++            state[sys.prefix] = {
++                "last_check": current_time.strftime(SELFCHECK_DATE_FMT),
++                "pypi_version": pypi_version,
++            }
++
++            with open(self.statefile_path, "w") as statefile:
++                json.dump(state, statefile, sort_keys=True,
++                          separators=(",", ":"))
++
++
++def load_selfcheck_statefile():
++    if running_under_virtualenv():
++        return VirtualenvSelfCheckState()
++    else:
++        return GlobalSelfCheckState()
++
++
++def pip_version_check(session, options):
++    """Check for an update for pip.
++
++    Limit the frequency of checks to once per week. State is stored either in
++    the active virtualenv or in the user's USER_CACHE_DIR keyed off the prefix
++    of the pip script path.
++    """
++    installed_version = get_installed_version("pip")
++    if not installed_version:
++        return
++
++    pip_version = packaging_version.parse(installed_version)
++    pypi_version = None
++
++    try:
++        state = load_selfcheck_statefile()
++
++        current_time = datetime.datetime.utcnow()
++        # Determine if we need to refresh the state
++        if "last_check" in state.state and "pypi_version" in state.state:
++            last_check = datetime.datetime.strptime(
++                state.state["last_check"],
++                SELFCHECK_DATE_FMT
++            )
++            if (current_time - last_check).total_seconds() < 7 * 24 * 60 * 60:
++                pypi_version = state.state["pypi_version"]
++
++        # Refresh the version if we need to or just see if we need to warn
++        if pypi_version is None:
++            # Lets use PackageFinder to see what the latest pip version is
++            finder = PackageFinder(
++                find_links=options.find_links,
++                index_urls=[options.index_url] + options.extra_index_urls,
++                allow_all_prereleases=False,  # Explicitly set to False
++                trusted_hosts=options.trusted_hosts,
++                process_dependency_links=options.process_dependency_links,
++                session=session,
++            )
++            all_candidates = finder.find_all_candidates("pip")
++            if not all_candidates:
++                return
++            pypi_version = str(
++                max(all_candidates, key=lambda c: c.version).version
++            )
++
++            # save that we've performed a check
++            state.save(pypi_version, current_time)
++
++        remote_version = packaging_version.parse(pypi_version)
++
++        # Determine if our pypi_version is older
++        if (pip_version < remote_version and
++                pip_version.base_version != remote_version.base_version):
++            # Advise "python -m pip" on Windows to avoid issues
++            # with overwriting pip.exe.
++            if WINDOWS:
++                pip_cmd = "python -m pip"
++            else:
++                pip_cmd = "pip"
++            logger.warning(
++                "You are using pip version %s, however version %s is "
++                "available.\nYou should consider upgrading via the "
++                "'%s install --upgrade pip' command.",
++                pip_version, pypi_version, pip_cmd
++            )
++    except Exception:
++        logger.debug(
++            "There was an error checking the latest version of pip",
++            exc_info=True,
++        )
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/typing.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/typing.py	(date 1573549700394)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/typing.py	(date 1573549700394)
+@@ -0,0 +1,29 @@
++"""For neatly implementing static typing in pip.
++
++`mypy` - the static type analysis tool we use - uses the `typing` module, which
++provides core functionality fundamental to mypy's functioning.
++
++Generally, `typing` would be imported at runtime and used in that fashion -
++it acts as a no-op at runtime and does not have any run-time overhead by
++design.
++
++As it turns out, `typing` is not vendorable - it uses separate sources for
++Python 2/Python 3. Thus, this codebase can not expect it to be present.
++To work around this, mypy allows the typing import to be behind a False-y
++optional to prevent it from running at runtime and type-comments can be used
++to remove the need for the types to be accessible directly during runtime.
++
++This module provides the False-y guard in a nicely named fashion so that a
++curious maintainer can reach here to read this.
++
++In pip, all static-typing related imports should be guarded as follows:
++
++    from pip.utils.typing import MYPY_CHECK_RUNNING
++
++    if MYPY_CHECK_RUNNING:
++        from typing import ...
++
++Ref: https://github.com/python/mypy/issues/3216
++"""
++
++MYPY_CHECK_RUNNING = False
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/logging.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/logging.py	(date 1573549700345)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/logging.py	(date 1573549700345)
+@@ -0,0 +1,132 @@
++from __future__ import absolute_import
++
++import contextlib
++import logging
++import logging.handlers
++import os
++
++from pip._internal.compat import WINDOWS
++from pip._internal.utils.misc import ensure_dir
++
++try:
++    import threading
++except ImportError:
++    import dummy_threading as threading  # type: ignore
++
++
++try:
++    from pip._vendor import colorama
++# Lots of different errors can come from this, including SystemError and
++# ImportError.
++except Exception:
++    colorama = None
++
++
++_log_state = threading.local()
++_log_state.indentation = 0
++
++
++@contextlib.contextmanager
++def indent_log(num=2):
++    """
++    A context manager which will cause the log output to be indented for any
++    log messages emitted inside it.
++    """
++    _log_state.indentation += num
++    try:
++        yield
++    finally:
++        _log_state.indentation -= num
++
++
++def get_indentation():
++    return getattr(_log_state, 'indentation', 0)
++
++
++class IndentingFormatter(logging.Formatter):
++
++    def format(self, record):
++        """
++        Calls the standard formatter, but will indent all of the log messages
++        by our current indentation level.
++        """
++        formatted = logging.Formatter.format(self, record)
++        formatted = "".join([
++            (" " * get_indentation()) + line
++            for line in formatted.splitlines(True)
++        ])
++        return formatted
++
++
++def _color_wrap(*colors):
++    def wrapped(inp):
++        return "".join(list(colors) + [inp, colorama.Style.RESET_ALL])
++    return wrapped
++
++
++class ColorizedStreamHandler(logging.StreamHandler):
++
++    # Don't build up a list of colors if we don't have colorama
++    if colorama:
++        COLORS = [
++            # This needs to be in order from highest logging level to lowest.
++            (logging.ERROR, _color_wrap(colorama.Fore.RED)),
++            (logging.WARNING, _color_wrap(colorama.Fore.YELLOW)),
++        ]
++    else:
++        COLORS = []
++
++    def __init__(self, stream=None, no_color=None):
++        logging.StreamHandler.__init__(self, stream)
++        self._no_color = no_color
++
++        if WINDOWS and colorama:
++            self.stream = colorama.AnsiToWin32(self.stream)
++
++    def should_color(self):
++        # Don't colorize things if we do not have colorama or if told not to
++        if not colorama or self._no_color:
++            return False
++
++        real_stream = (
++            self.stream if not isinstance(self.stream, colorama.AnsiToWin32)
++            else self.stream.wrapped
++        )
++
++        # If the stream is a tty we should color it
++        if hasattr(real_stream, "isatty") and real_stream.isatty():
++            return True
++
++        # If we have an ASNI term we should color it
++        if os.environ.get("TERM") == "ANSI":
++            return True
++
++        # If anything else we should not color it
++        return False
++
++    def format(self, record):
++        msg = logging.StreamHandler.format(self, record)
++
++        if self.should_color():
++            for level, color in self.COLORS:
++                if record.levelno >= level:
++                    msg = color(msg)
++                    break
++
++        return msg
++
++
++class BetterRotatingFileHandler(logging.handlers.RotatingFileHandler):
++
++    def _open(self):
++        ensure_dir(os.path.dirname(self.baseFilename))
++        return logging.handlers.RotatingFileHandler._open(self)
++
++
++class MaxLevelFilter(logging.Filter):
++
++    def __init__(self, level):
++        self.level = level
++
++    def filter(self, record):
++        return record.levelno < self.level
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/deprecation.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/deprecation.py	(date 1573549700305)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/deprecation.py	(date 1573549700305)
+@@ -0,0 +1,77 @@
++"""
++A module that implements tooling to enable easy warnings about deprecations.
++"""
++from __future__ import absolute_import
++
++import logging
++import warnings
++
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++
++if MYPY_CHECK_RUNNING:
++    from typing import Any
++
++
++class PipDeprecationWarning(Warning):
++    pass
++
++
++class Pending(object):
++    pass
++
++
++class RemovedInPip11Warning(PipDeprecationWarning):
++    pass
++
++
++class RemovedInPip12Warning(PipDeprecationWarning, Pending):
++    pass
++
++
++# Warnings <-> Logging Integration
++
++
++_warnings_showwarning = None  # type: Any
++
++
++def _showwarning(message, category, filename, lineno, file=None, line=None):
++    if file is not None:
++        if _warnings_showwarning is not None:
++            _warnings_showwarning(
++                message, category, filename, lineno, file, line,
++            )
++    else:
++        if issubclass(category, PipDeprecationWarning):
++            # We use a specially named logger which will handle all of the
++            # deprecation messages for pip.
++            logger = logging.getLogger("pip._internal.deprecations")
++
++            # This is purposely using the % formatter here instead of letting
++            # the logging module handle the interpolation. This is because we
++            # want it to appear as if someone typed this entire message out.
++            log_message = "DEPRECATION: %s" % message
++
++            # PipDeprecationWarnings that are Pending still have at least 2
++            # versions to go until they are removed so they can just be
++            # warnings.  Otherwise, they will be removed in the very next
++            # version of pip. We want these to be more obvious so we use the
++            # ERROR logging level.
++            if issubclass(category, Pending):
++                logger.warning(log_message)
++            else:
++                logger.error(log_message)
++        else:
++            _warnings_showwarning(
++                message, category, filename, lineno, file, line,
++            )
++
++
++def install_warning_logger():
++    # Enable our Deprecation Warnings
++    warnings.simplefilter("default", PipDeprecationWarning, append=True)
++
++    global _warnings_showwarning
++
++    if _warnings_showwarning is None:
++        _warnings_showwarning = warnings.showwarning
++        warnings.showwarning = _showwarning
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/encoding.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/encoding.py	(date 1573549700314)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/encoding.py	(date 1573549700314)
+@@ -0,0 +1,33 @@
++import codecs
++import locale
++import re
++import sys
++
++BOMS = [
++    (codecs.BOM_UTF8, 'utf8'),
++    (codecs.BOM_UTF16, 'utf16'),
++    (codecs.BOM_UTF16_BE, 'utf16-be'),
++    (codecs.BOM_UTF16_LE, 'utf16-le'),
++    (codecs.BOM_UTF32, 'utf32'),
++    (codecs.BOM_UTF32_BE, 'utf32-be'),
++    (codecs.BOM_UTF32_LE, 'utf32-le'),
++]
++
++ENCODING_RE = re.compile(br'coding[:=]\s*([-\w.]+)')
++
++
++def auto_decode(data):
++    """Check a bytes string for a BOM to correctly detect the encoding
++
++    Fallback to locale.getpreferredencoding(False) like open() on Python3"""
++    for bom, encoding in BOMS:
++        if data.startswith(bom):
++            return data[len(bom):].decode(encoding)
++    # Lets check the first two lines as in PEP263
++    for line in data.split(b'\n')[:2]:
++        if line[0:1] == b'#' and ENCODING_RE.search(line):
++            encoding = ENCODING_RE.search(line).groups()[0].decode('ascii')
++            return data.decode(encoding)
++    return data.decode(
++        locale.getpreferredencoding(False) or sys.getdefaultencoding(),
++    )
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/hashes.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/hashes.py	(date 1573549700336)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/hashes.py	(date 1573549700336)
+@@ -0,0 +1,94 @@
++from __future__ import absolute_import
++
++import hashlib
++
++from pip._vendor.six import iteritems, iterkeys, itervalues
++
++from pip._internal.exceptions import (
++    HashMismatch, HashMissing, InstallationError,
++)
++from pip._internal.utils.misc import read_chunks
++
++# The recommended hash algo of the moment. Change this whenever the state of
++# the art changes; it won't hurt backward compatibility.
++FAVORITE_HASH = 'sha256'
++
++
++# Names of hashlib algorithms allowed by the --hash option and ``pip hash``
++# Currently, those are the ones at least as collision-resistant as sha256.
++STRONG_HASHES = ['sha256', 'sha384', 'sha512']
++
++
++class Hashes(object):
++    """A wrapper that builds multiple hashes at once and checks them against
++    known-good values
++
++    """
++    def __init__(self, hashes=None):
++        """
++        :param hashes: A dict of algorithm names pointing to lists of allowed
++            hex digests
++        """
++        self._allowed = {} if hashes is None else hashes
++
++    def check_against_chunks(self, chunks):
++        """Check good hashes against ones built from iterable of chunks of
++        data.
++
++        Raise HashMismatch if none match.
++
++        """
++        gots = {}
++        for hash_name in iterkeys(self._allowed):
++            try:
++                gots[hash_name] = hashlib.new(hash_name)
++            except (ValueError, TypeError):
++                raise InstallationError('Unknown hash name: %s' % hash_name)
++
++        for chunk in chunks:
++            for hash in itervalues(gots):
++                hash.update(chunk)
++
++        for hash_name, got in iteritems(gots):
++            if got.hexdigest() in self._allowed[hash_name]:
++                return
++        self._raise(gots)
++
++    def _raise(self, gots):
++        raise HashMismatch(self._allowed, gots)
++
++    def check_against_file(self, file):
++        """Check good hashes against a file-like object
++
++        Raise HashMismatch if none match.
++
++        """
++        return self.check_against_chunks(read_chunks(file))
++
++    def check_against_path(self, path):
++        with open(path, 'rb') as file:
++            return self.check_against_file(file)
++
++    def __nonzero__(self):
++        """Return whether I know any known-good hashes."""
++        return bool(self._allowed)
++
++    def __bool__(self):
++        return self.__nonzero__()
++
++
++class MissingHashes(Hashes):
++    """A workalike for Hashes used when we're missing a hash for a requirement
++
++    It computes the actual hash of the requirement and raises a HashMissing
++    exception showing it to the user.
++
++    """
++    def __init__(self):
++        """Don't offer the ``hashes`` kwarg."""
++        # Pass our favorite hash in to generate a "gotten hash". With the
++        # empty list, it will never match, so an error will always raise.
++        super(MissingHashes, self).__init__(hashes={FAVORITE_HASH: []})
++
++    def _raise(self, gots):
++        raise HashMissing(gots[FAVORITE_HASH].hexdigest())
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/misc.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/misc.py	(date 1573549700351)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/misc.py	(date 1573549700351)
+@@ -0,0 +1,851 @@
++from __future__ import absolute_import
++
++import contextlib
++import errno
++import io
++import locale
++# we have a submodule named 'logging' which would shadow this if we used the
++# regular name:
++import logging as std_logging
++import os
++import posixpath
++import re
++import shutil
++import stat
++import subprocess
++import sys
++import tarfile
++import zipfile
++from collections import deque
++
++from pip._vendor import pkg_resources
++# NOTE: retrying is not annotated in typeshed as on 2017-07-17, which is
++#       why we ignore the type on this import.
++from pip._vendor.retrying import retry  # type: ignore
++from pip._vendor.six import PY2
++from pip._vendor.six.moves import input
++
++from pip._internal.compat import console_to_str, expanduser, stdlib_pkgs
++from pip._internal.exceptions import InstallationError
++from pip._internal.locations import (
++    running_under_virtualenv, site_packages, user_site, virtualenv_no_global,
++    write_delete_marker_file,
++)
++
++if PY2:
++    from io import BytesIO as StringIO
++else:
++    from io import StringIO
++
++__all__ = ['rmtree', 'display_path', 'backup_dir',
++           'ask', 'splitext',
++           'format_size', 'is_installable_dir',
++           'is_svn_page', 'file_contents',
++           'split_leading_dir', 'has_leading_dir',
++           'normalize_path',
++           'renames', 'get_prog',
++           'unzip_file', 'untar_file', 'unpack_file', 'call_subprocess',
++           'captured_stdout', 'ensure_dir',
++           'ARCHIVE_EXTENSIONS', 'SUPPORTED_EXTENSIONS',
++           'get_installed_version']
++
++
++logger = std_logging.getLogger(__name__)
++
++BZ2_EXTENSIONS = ('.tar.bz2', '.tbz')
++XZ_EXTENSIONS = ('.tar.xz', '.txz', '.tlz', '.tar.lz', '.tar.lzma')
++ZIP_EXTENSIONS = ('.zip', '.whl')
++TAR_EXTENSIONS = ('.tar.gz', '.tgz', '.tar')
++ARCHIVE_EXTENSIONS = (
++    ZIP_EXTENSIONS + BZ2_EXTENSIONS + TAR_EXTENSIONS + XZ_EXTENSIONS)
++SUPPORTED_EXTENSIONS = ZIP_EXTENSIONS + TAR_EXTENSIONS
++try:
++    import bz2  # noqa
++    SUPPORTED_EXTENSIONS += BZ2_EXTENSIONS
++except ImportError:
++    logger.debug('bz2 module is not available')
++
++try:
++    # Only for Python 3.3+
++    import lzma  # noqa
++    SUPPORTED_EXTENSIONS += XZ_EXTENSIONS
++except ImportError:
++    logger.debug('lzma module is not available')
++
++
++def import_or_raise(pkg_or_module_string, ExceptionType, *args, **kwargs):
++    try:
++        return __import__(pkg_or_module_string)
++    except ImportError:
++        raise ExceptionType(*args, **kwargs)
++
++
++def ensure_dir(path):
++    """os.path.makedirs without EEXIST."""
++    try:
++        os.makedirs(path)
++    except OSError as e:
++        if e.errno != errno.EEXIST:
++            raise
++
++
++def get_prog():
++    try:
++        prog = os.path.basename(sys.argv[0])
++        if prog in ('__main__.py', '-c'):
++            return "%s -m pip" % sys.executable
++        else:
++            return prog
++    except (AttributeError, TypeError, IndexError):
++        pass
++    return 'pip'
++
++
++# Retry every half second for up to 3 seconds
++@retry(stop_max_delay=3000, wait_fixed=500)
++def rmtree(dir, ignore_errors=False):
++    shutil.rmtree(dir, ignore_errors=ignore_errors,
++                  onerror=rmtree_errorhandler)
++
++
++def rmtree_errorhandler(func, path, exc_info):
++    """On Windows, the files in .svn are read-only, so when rmtree() tries to
++    remove them, an exception is thrown.  We catch that here, remove the
++    read-only attribute, and hopefully continue without problems."""
++    # if file type currently read only
++    if os.stat(path).st_mode & stat.S_IREAD:
++        # convert to read/write
++        os.chmod(path, stat.S_IWRITE)
++        # use the original function to repeat the operation
++        func(path)
++        return
++    else:
++        raise
++
++
++def display_path(path):
++    """Gives the display value for a given path, making it relative to cwd
++    if possible."""
++    path = os.path.normcase(os.path.abspath(path))
++    if sys.version_info[0] == 2:
++        path = path.decode(sys.getfilesystemencoding(), 'replace')
++        path = path.encode(sys.getdefaultencoding(), 'replace')
++    if path.startswith(os.getcwd() + os.path.sep):
++        path = '.' + path[len(os.getcwd()):]
++    return path
++
++
++def backup_dir(dir, ext='.bak'):
++    """Figure out the name of a directory to back up the given dir to
++    (adding .bak, .bak2, etc)"""
++    n = 1
++    extension = ext
++    while os.path.exists(dir + extension):
++        n += 1
++        extension = ext + str(n)
++    return dir + extension
++
++
++def ask_path_exists(message, options):
++    for action in os.environ.get('PIP_EXISTS_ACTION', '').split():
++        if action in options:
++            return action
++    return ask(message, options)
++
++
++def ask(message, options):
++    """Ask the message interactively, with the given possible responses"""
++    while 1:
++        if os.environ.get('PIP_NO_INPUT'):
++            raise Exception(
++                'No input was expected ($PIP_NO_INPUT set); question: %s' %
++                message
++            )
++        response = input(message)
++        response = response.strip().lower()
++        if response not in options:
++            print(
++                'Your response (%r) was not one of the expected responses: '
++                '%s' % (response, ', '.join(options))
++            )
++        else:
++            return response
++
++
++def format_size(bytes):
++    if bytes > 1000 * 1000:
++        return '%.1fMB' % (bytes / 1000.0 / 1000)
++    elif bytes > 10 * 1000:
++        return '%ikB' % (bytes / 1000)
++    elif bytes > 1000:
++        return '%.1fkB' % (bytes / 1000.0)
++    else:
++        return '%ibytes' % bytes
++
++
++def is_installable_dir(path):
++    """Return True if `path` is a directory containing a setup.py file."""
++    if not os.path.isdir(path):
++        return False
++    setup_py = os.path.join(path, 'setup.py')
++    if os.path.isfile(setup_py):
++        return True
++    return False
++
++
++def is_svn_page(html):
++    """
++    Returns true if the page appears to be the index page of an svn repository
++    """
++    return (re.search(r'<title>[^<]*Revision \d+:', html) and
++            re.search(r'Powered by (?:<a[^>]*?>)?Subversion', html, re.I))
++
++
++def file_contents(filename):
++    with open(filename, 'rb') as fp:
++        return fp.read().decode('utf-8')
++
++
++def read_chunks(file, size=io.DEFAULT_BUFFER_SIZE):
++    """Yield pieces of data from a file-like object until EOF."""
++    while True:
++        chunk = file.read(size)
++        if not chunk:
++            break
++        yield chunk
++
++
++def split_leading_dir(path):
++    path = path.lstrip('/').lstrip('\\')
++    if '/' in path and (('\\' in path and path.find('/') < path.find('\\')) or
++                        '\\' not in path):
++        return path.split('/', 1)
++    elif '\\' in path:
++        return path.split('\\', 1)
++    else:
++        return path, ''
++
++
++def has_leading_dir(paths):
++    """Returns true if all the paths have the same leading path name
++    (i.e., everything is in one subdirectory in an archive)"""
++    common_prefix = None
++    for path in paths:
++        prefix, rest = split_leading_dir(path)
++        if not prefix:
++            return False
++        elif common_prefix is None:
++            common_prefix = prefix
++        elif prefix != common_prefix:
++            return False
++    return True
++
++
++def normalize_path(path, resolve_symlinks=True):
++    """
++    Convert a path to its canonical, case-normalized, absolute version.
++
++    """
++    path = expanduser(path)
++    if resolve_symlinks:
++        path = os.path.realpath(path)
++    else:
++        path = os.path.abspath(path)
++    return os.path.normcase(path)
++
++
++def splitext(path):
++    """Like os.path.splitext, but take off .tar too"""
++    base, ext = posixpath.splitext(path)
++    if base.lower().endswith('.tar'):
++        ext = base[-4:] + ext
++        base = base[:-4]
++    return base, ext
++
++
++def renames(old, new):
++    """Like os.renames(), but handles renaming across devices."""
++    # Implementation borrowed from os.renames().
++    head, tail = os.path.split(new)
++    if head and tail and not os.path.exists(head):
++        os.makedirs(head)
++
++    shutil.move(old, new)
++
++    head, tail = os.path.split(old)
++    if head and tail:
++        try:
++            os.removedirs(head)
++        except OSError:
++            pass
++
++
++def is_local(path):
++    """
++    Return True if path is within sys.prefix, if we're running in a virtualenv.
++
++    If we're not in a virtualenv, all paths are considered "local."
++
++    """
++    if not running_under_virtualenv():
++        return True
++    return normalize_path(path).startswith(normalize_path(sys.prefix))
++
++
++def dist_is_local(dist):
++    """
++    Return True if given Distribution object is installed locally
++    (i.e. within current virtualenv).
++
++    Always True if we're not in a virtualenv.
++
++    """
++    return is_local(dist_location(dist))
++
++
++def dist_in_usersite(dist):
++    """
++    Return True if given Distribution is installed in user site.
++    """
++    norm_path = normalize_path(dist_location(dist))
++    return norm_path.startswith(normalize_path(user_site))
++
++
++def dist_in_site_packages(dist):
++    """
++    Return True if given Distribution is installed in
++    sysconfig.get_python_lib().
++    """
++    return normalize_path(
++        dist_location(dist)
++    ).startswith(normalize_path(site_packages))
++
++
++def dist_is_editable(dist):
++    """Is distribution an editable install?"""
++    for path_item in sys.path:
++        egg_link = os.path.join(path_item, dist.project_name + '.egg-link')
++        if os.path.isfile(egg_link):
++            return True
++    return False
++
++
++def get_installed_distributions(local_only=True,
++                                skip=stdlib_pkgs,
++                                include_editables=True,
++                                editables_only=False,
++                                user_only=False):
++    """
++    Return a list of installed Distribution objects.
++
++    If ``local_only`` is True (default), only return installations
++    local to the current virtualenv, if in a virtualenv.
++
++    ``skip`` argument is an iterable of lower-case project names to
++    ignore; defaults to stdlib_pkgs
++
++    If ``include_editables`` is False, don't report editables.
++
++    If ``editables_only`` is True , only report editables.
++
++    If ``user_only`` is True , only report installations in the user
++    site directory.
++
++    """
++    if local_only:
++        local_test = dist_is_local
++    else:
++        def local_test(d):
++            return True
++
++    if include_editables:
++        def editable_test(d):
++            return True
++    else:
++        def editable_test(d):
++            return not dist_is_editable(d)
++
++    if editables_only:
++        def editables_only_test(d):
++            return dist_is_editable(d)
++    else:
++        def editables_only_test(d):
++            return True
++
++    if user_only:
++        user_test = dist_in_usersite
++    else:
++        def user_test(d):
++            return True
++
++    return [d for d in pkg_resources.working_set
++            if local_test(d) and
++            d.key not in skip and
++            editable_test(d) and
++            editables_only_test(d) and
++            user_test(d)
++            ]
++
++
++def egg_link_path(dist):
++    """
++    Return the path for the .egg-link file if it exists, otherwise, None.
++
++    There's 3 scenarios:
++    1) not in a virtualenv
++       try to find in site.USER_SITE, then site_packages
++    2) in a no-global virtualenv
++       try to find in site_packages
++    3) in a yes-global virtualenv
++       try to find in site_packages, then site.USER_SITE
++       (don't look in global location)
++
++    For #1 and #3, there could be odd cases, where there's an egg-link in 2
++    locations.
++
++    This method will just return the first one found.
++    """
++    sites = []
++    if running_under_virtualenv():
++        if virtualenv_no_global():
++            sites.append(site_packages)
++        else:
++            sites.append(site_packages)
++            if user_site:
++                sites.append(user_site)
++    else:
++        if user_site:
++            sites.append(user_site)
++        sites.append(site_packages)
++
++    for site in sites:
++        egglink = os.path.join(site, dist.project_name) + '.egg-link'
++        if os.path.isfile(egglink):
++            return egglink
++
++
++def dist_location(dist):
++    """
++    Get the site-packages location of this distribution. Generally
++    this is dist.location, except in the case of develop-installed
++    packages, where dist.location is the source code location, and we
++    want to know where the egg-link file is.
++
++    """
++    egg_link = egg_link_path(dist)
++    if egg_link:
++        return egg_link
++    return dist.location
++
++
++def current_umask():
++    """Get the current umask which involves having to set it temporarily."""
++    mask = os.umask(0)
++    os.umask(mask)
++    return mask
++
++
++def unzip_file(filename, location, flatten=True):
++    """
++    Unzip the file (with path `filename`) to the destination `location`.  All
++    files are written based on system defaults and umask (i.e. permissions are
++    not preserved), except that regular file members with any execute
++    permissions (user, group, or world) have "chmod +x" applied after being
++    written. Note that for windows, any execute changes using os.chmod are
++    no-ops per the python docs.
++    """
++    ensure_dir(location)
++    zipfp = open(filename, 'rb')
++    try:
++        zip = zipfile.ZipFile(zipfp, allowZip64=True)
++        leading = has_leading_dir(zip.namelist()) and flatten
++        for info in zip.infolist():
++            name = info.filename
++            data = zip.read(name)
++            fn = name
++            if leading:
++                fn = split_leading_dir(name)[1]
++            fn = os.path.join(location, fn)
++            dir = os.path.dirname(fn)
++            if fn.endswith('/') or fn.endswith('\\'):
++                # A directory
++                ensure_dir(fn)
++            else:
++                ensure_dir(dir)
++                fp = open(fn, 'wb')
++                try:
++                    fp.write(data)
++                finally:
++                    fp.close()
++                    mode = info.external_attr >> 16
++                    # if mode and regular file and any execute permissions for
++                    # user/group/world?
++                    if mode and stat.S_ISREG(mode) and mode & 0o111:
++                        # make dest file have execute for user/group/world
++                        # (chmod +x) no-op on windows per python docs
++                        os.chmod(fn, (0o777 - current_umask() | 0o111))
++    finally:
++        zipfp.close()
++
++
++def untar_file(filename, location):
++    """
++    Untar the file (with path `filename`) to the destination `location`.
++    All files are written based on system defaults and umask (i.e. permissions
++    are not preserved), except that regular file members with any execute
++    permissions (user, group, or world) have "chmod +x" applied after being
++    written.  Note that for windows, any execute changes using os.chmod are
++    no-ops per the python docs.
++    """
++    ensure_dir(location)
++    if filename.lower().endswith('.gz') or filename.lower().endswith('.tgz'):
++        mode = 'r:gz'
++    elif filename.lower().endswith(BZ2_EXTENSIONS):
++        mode = 'r:bz2'
++    elif filename.lower().endswith(XZ_EXTENSIONS):
++        mode = 'r:xz'
++    elif filename.lower().endswith('.tar'):
++        mode = 'r'
++    else:
++        logger.warning(
++            'Cannot determine compression type for file %s', filename,
++        )
++        mode = 'r:*'
++    tar = tarfile.open(filename, mode)
++    try:
++        # note: python<=2.5 doesn't seem to know about pax headers, filter them
++        leading = has_leading_dir([
++            member.name for member in tar.getmembers()
++            if member.name != 'pax_global_header'
++        ])
++        for member in tar.getmembers():
++            fn = member.name
++            if fn == 'pax_global_header':
++                continue
++            if leading:
++                fn = split_leading_dir(fn)[1]
++            path = os.path.join(location, fn)
++            if member.isdir():
++                ensure_dir(path)
++            elif member.issym():
++                try:
++                    tar._extract_member(member, path)
++                except Exception as exc:
++                    # Some corrupt tar files seem to produce this
++                    # (specifically bad symlinks)
++                    logger.warning(
++                        'In the tar file %s the member %s is invalid: %s',
++                        filename, member.name, exc,
++                    )
++                    continue
++            else:
++                try:
++                    fp = tar.extractfile(member)
++                except (KeyError, AttributeError) as exc:
++                    # Some corrupt tar files seem to produce this
++                    # (specifically bad symlinks)
++                    logger.warning(
++                        'In the tar file %s the member %s is invalid: %s',
++                        filename, member.name, exc,
++                    )
++                    continue
++                ensure_dir(os.path.dirname(path))
++                with open(path, 'wb') as destfp:
++                    shutil.copyfileobj(fp, destfp)
++                fp.close()
++                # Update the timestamp (useful for cython compiled files)
++                tar.utime(member, path)
++                # member have any execute permissions for user/group/world?
++                if member.mode & 0o111:
++                    # make dest file have execute for user/group/world
++                    # no-op on windows per python docs
++                    os.chmod(path, (0o777 - current_umask() | 0o111))
++    finally:
++        tar.close()
++
++
++def unpack_file(filename, location, content_type, link):
++    filename = os.path.realpath(filename)
++    if (content_type == 'application/zip' or
++            filename.lower().endswith(ZIP_EXTENSIONS) or
++            zipfile.is_zipfile(filename)):
++        unzip_file(
++            filename,
++            location,
++            flatten=not filename.endswith('.whl')
++        )
++    elif (content_type == 'application/x-gzip' or
++            tarfile.is_tarfile(filename) or
++            filename.lower().endswith(
++                TAR_EXTENSIONS + BZ2_EXTENSIONS + XZ_EXTENSIONS)):
++        untar_file(filename, location)
++    elif (content_type and content_type.startswith('text/html') and
++            is_svn_page(file_contents(filename))):
++        # We don't really care about this
++        from pip._internal.vcs.subversion import Subversion
++        Subversion('svn+' + link.url).unpack(location)
++    else:
++        # FIXME: handle?
++        # FIXME: magic signatures?
++        logger.critical(
++            'Cannot unpack file %s (downloaded from %s, content-type: %s); '
++            'cannot detect archive format',
++            filename, location, content_type,
++        )
++        raise InstallationError(
++            'Cannot determine archive format of %s' % location
++        )
++
++
++def call_subprocess(cmd, show_stdout=True, cwd=None,
++                    on_returncode='raise',
++                    command_desc=None,
++                    extra_environ=None, unset_environ=None, spinner=None):
++    """
++    Args:
++      unset_environ: an iterable of environment variable names to unset
++        prior to calling subprocess.Popen().
++    """
++    if unset_environ is None:
++        unset_environ = []
++    # This function's handling of subprocess output is confusing and I
++    # previously broke it terribly, so as penance I will write a long comment
++    # explaining things.
++    #
++    # The obvious thing that affects output is the show_stdout=
++    # kwarg. show_stdout=True means, let the subprocess write directly to our
++    # stdout. Even though it is nominally the default, it is almost never used
++    # inside pip (and should not be used in new code without a very good
++    # reason); as of 2016-02-22 it is only used in a few places inside the VCS
++    # wrapper code. Ideally we should get rid of it entirely, because it
++    # creates a lot of complexity here for a rarely used feature.
++    #
++    # Most places in pip set show_stdout=False. What this means is:
++    # - We connect the child stdout to a pipe, which we read.
++    # - By default, we hide the output but show a spinner -- unless the
++    #   subprocess exits with an error, in which case we show the output.
++    # - If the --verbose option was passed (= loglevel is DEBUG), then we show
++    #   the output unconditionally. (But in this case we don't want to show
++    #   the output a second time if it turns out that there was an error.)
++    #
++    # stderr is always merged with stdout (even if show_stdout=True).
++    if show_stdout:
++        stdout = None
++    else:
++        stdout = subprocess.PIPE
++    if command_desc is None:
++        cmd_parts = []
++        for part in cmd:
++            if ' ' in part or '\n' in part or '"' in part or "'" in part:
++                part = '"%s"' % part.replace('"', '\\"')
++            cmd_parts.append(part)
++        command_desc = ' '.join(cmd_parts)
++    logger.debug("Running command %s", command_desc)
++    env = os.environ.copy()
++    if extra_environ:
++        env.update(extra_environ)
++    for name in unset_environ:
++        env.pop(name, None)
++    try:
++        proc = subprocess.Popen(
++            cmd, stderr=subprocess.STDOUT, stdin=subprocess.PIPE,
++            stdout=stdout, cwd=cwd, env=env,
++        )
++        proc.stdin.close()
++    except Exception as exc:
++        logger.critical(
++            "Error %s while executing command %s", exc, command_desc,
++        )
++        raise
++    all_output = []
++    if stdout is not None:
++        while True:
++            line = console_to_str(proc.stdout.readline())
++            if not line:
++                break
++            line = line.rstrip()
++            all_output.append(line + '\n')
++            if logger.getEffectiveLevel() <= std_logging.DEBUG:
++                # Show the line immediately
++                logger.debug(line)
++            else:
++                # Update the spinner
++                if spinner is not None:
++                    spinner.spin()
++    try:
++        proc.wait()
++    finally:
++        if proc.stdout:
++            proc.stdout.close()
++    if spinner is not None:
++        if proc.returncode:
++            spinner.finish("error")
++        else:
++            spinner.finish("done")
++    if proc.returncode:
++        if on_returncode == 'raise':
++            if (logger.getEffectiveLevel() > std_logging.DEBUG and
++                    not show_stdout):
++                logger.info(
++                    'Complete output from command %s:', command_desc,
++                )
++                logger.info(
++                    ''.join(all_output) +
++                    '\n----------------------------------------'
++                )
++            raise InstallationError(
++                'Command "%s" failed with error code %s in %s'
++                % (command_desc, proc.returncode, cwd))
++        elif on_returncode == 'warn':
++            logger.warning(
++                'Command "%s" had error code %s in %s',
++                command_desc, proc.returncode, cwd,
++            )
++        elif on_returncode == 'ignore':
++            pass
++        else:
++            raise ValueError('Invalid value: on_returncode=%s' %
++                             repr(on_returncode))
++    if not show_stdout:
++        return ''.join(all_output)
++
++
++def read_text_file(filename):
++    """Return the contents of *filename*.
++
++    Try to decode the file contents with utf-8, the preferred system encoding
++    (e.g., cp1252 on some Windows machines), and latin1, in that order.
++    Decoding a byte string with latin1 will never raise an error. In the worst
++    case, the returned string will contain some garbage characters.
++
++    """
++    with open(filename, 'rb') as fp:
++        data = fp.read()
++
++    encodings = ['utf-8', locale.getpreferredencoding(False), 'latin1']
++    for enc in encodings:
++        try:
++            data = data.decode(enc)
++        except UnicodeDecodeError:
++            continue
++        break
++
++    assert type(data) != bytes  # Latin1 should have worked.
++    return data
++
++
++def _make_build_dir(build_dir):
++    os.makedirs(build_dir)
++    write_delete_marker_file(build_dir)
++
++
++class FakeFile(object):
++    """Wrap a list of lines in an object with readline() to make
++    ConfigParser happy."""
++    def __init__(self, lines):
++        self._gen = (l for l in lines)
++
++    def readline(self):
++        try:
++            try:
++                return next(self._gen)
++            except NameError:
++                return self._gen.next()
++        except StopIteration:
++            return ''
++
++    def __iter__(self):
++        return self._gen
++
++
++class StreamWrapper(StringIO):
++
++    @classmethod
++    def from_stream(cls, orig_stream):
++        cls.orig_stream = orig_stream
++        return cls()
++
++    # compileall.compile_dir() needs stdout.encoding to print to stdout
++    @property
++    def encoding(self):
++        return self.orig_stream.encoding
++
++
++@contextlib.contextmanager
++def captured_output(stream_name):
++    """Return a context manager used by captured_stdout/stdin/stderr
++    that temporarily replaces the sys stream *stream_name* with a StringIO.
++
++    Taken from Lib/support/__init__.py in the CPython repo.
++    """
++    orig_stdout = getattr(sys, stream_name)
++    setattr(sys, stream_name, StreamWrapper.from_stream(orig_stdout))
++    try:
++        yield getattr(sys, stream_name)
++    finally:
++        setattr(sys, stream_name, orig_stdout)
++
++
++def captured_stdout():
++    """Capture the output of sys.stdout:
++
++       with captured_stdout() as stdout:
++           print('hello')
++       self.assertEqual(stdout.getvalue(), 'hello\n')
++
++    Taken from Lib/support/__init__.py in the CPython repo.
++    """
++    return captured_output('stdout')
++
++
++class cached_property(object):
++    """A property that is only computed once per instance and then replaces
++       itself with an ordinary attribute. Deleting the attribute resets the
++       property.
++
++       Source: https://github.com/bottlepy/bottle/blob/0.11.5/bottle.py#L175
++    """
++
++    def __init__(self, func):
++        self.__doc__ = getattr(func, '__doc__')
++        self.func = func
++
++    def __get__(self, obj, cls):
++        if obj is None:
++            # We're being accessed from the class itself, not from an object
++            return self
++        value = obj.__dict__[self.func.__name__] = self.func(obj)
++        return value
++
++
++def get_installed_version(dist_name, lookup_dirs=None):
++    """Get the installed version of dist_name avoiding pkg_resources cache"""
++    # Create a requirement that we'll look for inside of setuptools.
++    req = pkg_resources.Requirement.parse(dist_name)
++
++    # We want to avoid having this cached, so we need to construct a new
++    # working set each time.
++    if lookup_dirs is None:
++        working_set = pkg_resources.WorkingSet()
++    else:
++        working_set = pkg_resources.WorkingSet(lookup_dirs)
++
++    # Get the installed distribution from our working set
++    dist = working_set.find(req)
++
++    # Check to see if we got an installed distribution or not, if we did
++    # we want to return it's version.
++    return dist.version if dist else None
++
++
++def consume(iterator):
++    """Consume an iterable at C speed."""
++    deque(iterator, maxlen=0)
++
++
++# Simulates an enum
++def enum(*sequential, **named):
++    enums = dict(zip(sequential, range(len(sequential))), **named)
++    reverse = {value: key for key, value in enums.items()}
++    enums['reverse_mapping'] = reverse
++    return type('Enum', (), enums)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/filesystem.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/filesystem.py	(date 1573549700321)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/filesystem.py	(date 1573549700321)
+@@ -0,0 +1,28 @@
++import os
++import os.path
++
++from pip._internal.compat import get_path_uid
++
++
++def check_path_owner(path):
++    # If we don't have a way to check the effective uid of this process, then
++    # we'll just assume that we own the directory.
++    if not hasattr(os, "geteuid"):
++        return True
++
++    previous = None
++    while path != previous:
++        if os.path.lexists(path):
++            # Check if path is writable by current user.
++            if os.geteuid() == 0:
++                # Special handling for root user in order to handle properly
++                # cases where users use sudo without -H flag.
++                try:
++                    path_uid = get_path_uid(path)
++                except OSError:
++                    return False
++                return path_uid == 0
++            else:
++                return os.access(path, os.W_OK)
++        else:
++            previous, path = path, os.path.dirname(path)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/__init__.py	(date 1573549700291)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/utils/__init__.py	(date 1573549700291)
+@@ -0,0 +1,0 @@
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/models/index.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/models/index.py	(date 1573549700221)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/models/index.py	(date 1573549700221)
+@@ -0,0 +1,15 @@
++from pip._vendor.six.moves.urllib import parse as urllib_parse
++
++
++class Index(object):
++    def __init__(self, url):
++        self.url = url
++        self.netloc = urllib_parse.urlsplit(url).netloc
++        self.simple_url = self.url_to_path('simple')
++        self.pypi_url = self.url_to_path('pypi')
++
++    def url_to_path(self, path):
++        return urllib_parse.urljoin(self.url, path)
++
++
++PyPI = Index('https://pypi.org/')
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/models/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/models/__init__.py	(date 1573549700215)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/models/__init__.py	(date 1573549700215)
+@@ -0,0 +1,4 @@
++from pip._internal.models.index import Index, PyPI
++
++
++__all__ = ["Index", "PyPI"]
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/download.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/download.py	(date 1573549700101)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/download.py	(date 1573549700101)
+@@ -0,0 +1,233 @@
++from __future__ import absolute_import
++
++import logging
++import os
++
++from pip._internal import cmdoptions
++from pip._internal.basecommand import RequirementCommand
++from pip._internal.exceptions import CommandError
++from pip._internal.index import FormatControl
++from pip._internal.operations.prepare import RequirementPreparer
++from pip._internal.req import RequirementSet
++from pip._internal.resolve import Resolver
++from pip._internal.utils.filesystem import check_path_owner
++from pip._internal.utils.misc import ensure_dir, normalize_path
++from pip._internal.utils.temp_dir import TempDirectory
++
++logger = logging.getLogger(__name__)
++
++
++class DownloadCommand(RequirementCommand):
++    """
++    Download packages from:
++
++    - PyPI (and other indexes) using requirement specifiers.
++    - VCS project urls.
++    - Local project directories.
++    - Local or remote source archives.
++
++    pip also supports downloading from "requirements files", which provide
++    an easy way to specify a whole environment to be downloaded.
++    """
++    name = 'download'
++
++    usage = """
++      %prog [options] <requirement specifier> [package-index-options] ...
++      %prog [options] -r <requirements file> [package-index-options] ...
++      %prog [options] <vcs project url> ...
++      %prog [options] <local project path> ...
++      %prog [options] <archive url/path> ..."""
++
++    summary = 'Download packages.'
++
++    def __init__(self, *args, **kw):
++        super(DownloadCommand, self).__init__(*args, **kw)
++
++        cmd_opts = self.cmd_opts
++
++        cmd_opts.add_option(cmdoptions.constraints())
++        cmd_opts.add_option(cmdoptions.requirements())
++        cmd_opts.add_option(cmdoptions.build_dir())
++        cmd_opts.add_option(cmdoptions.no_deps())
++        cmd_opts.add_option(cmdoptions.global_options())
++        cmd_opts.add_option(cmdoptions.no_binary())
++        cmd_opts.add_option(cmdoptions.only_binary())
++        cmd_opts.add_option(cmdoptions.src())
++        cmd_opts.add_option(cmdoptions.pre())
++        cmd_opts.add_option(cmdoptions.no_clean())
++        cmd_opts.add_option(cmdoptions.require_hashes())
++        cmd_opts.add_option(cmdoptions.progress_bar())
++        cmd_opts.add_option(cmdoptions.no_build_isolation())
++
++        cmd_opts.add_option(
++            '-d', '--dest', '--destination-dir', '--destination-directory',
++            dest='download_dir',
++            metavar='dir',
++            default=os.curdir,
++            help=("Download packages into <dir>."),
++        )
++
++        cmd_opts.add_option(
++            '--platform',
++            dest='platform',
++            metavar='platform',
++            default=None,
++            help=("Only download wheels compatible with <platform>. "
++                  "Defaults to the platform of the running system."),
++        )
++
++        cmd_opts.add_option(
++            '--python-version',
++            dest='python_version',
++            metavar='python_version',
++            default=None,
++            help=("Only download wheels compatible with Python "
++                  "interpreter version <version>. If not specified, then the "
++                  "current system interpreter minor version is used. A major "
++                  "version (e.g. '2') can be specified to match all "
++                  "minor revs of that major version.  A minor version "
++                  "(e.g. '34') can also be specified."),
++        )
++
++        cmd_opts.add_option(
++            '--implementation',
++            dest='implementation',
++            metavar='implementation',
++            default=None,
++            help=("Only download wheels compatible with Python "
++                  "implementation <implementation>, e.g. 'pp', 'jy', 'cp', "
++                  " or 'ip'. If not specified, then the current "
++                  "interpreter implementation is used.  Use 'py' to force "
++                  "implementation-agnostic wheels."),
++        )
++
++        cmd_opts.add_option(
++            '--abi',
++            dest='abi',
++            metavar='abi',
++            default=None,
++            help=("Only download wheels compatible with Python "
++                  "abi <abi>, e.g. 'pypy_41'.  If not specified, then the "
++                  "current interpreter abi tag is used.  Generally "
++                  "you will need to specify --implementation, "
++                  "--platform, and --python-version when using "
++                  "this option."),
++        )
++
++        index_opts = cmdoptions.make_option_group(
++            cmdoptions.index_group,
++            self.parser,
++        )
++
++        self.parser.insert_option_group(0, index_opts)
++        self.parser.insert_option_group(0, cmd_opts)
++
++    def run(self, options, args):
++        options.ignore_installed = True
++        # editable doesn't really make sense for `pip download`, but the bowels
++        # of the RequirementSet code require that property.
++        options.editables = []
++
++        if options.python_version:
++            python_versions = [options.python_version]
++        else:
++            python_versions = None
++
++        dist_restriction_set = any([
++            options.python_version,
++            options.platform,
++            options.abi,
++            options.implementation,
++        ])
++        binary_only = FormatControl(set(), {':all:'})
++        no_sdist_dependencies = (
++            options.format_control != binary_only and
++            not options.ignore_dependencies
++        )
++        if dist_restriction_set and no_sdist_dependencies:
++            raise CommandError(
++                "When restricting platform and interpreter constraints using "
++                "--python-version, --platform, --abi, or --implementation, "
++                "either --no-deps must be set, or --only-binary=:all: must be "
++                "set and --no-binary must not be set (or must be set to "
++                ":none:)."
++            )
++
++        options.src_dir = os.path.abspath(options.src_dir)
++        options.download_dir = normalize_path(options.download_dir)
++
++        ensure_dir(options.download_dir)
++
++        with self._build_session(options) as session:
++            finder = self._build_package_finder(
++                options=options,
++                session=session,
++                platform=options.platform,
++                python_versions=python_versions,
++                abi=options.abi,
++                implementation=options.implementation,
++            )
++            build_delete = (not (options.no_clean or options.build_dir))
++            if options.cache_dir and not check_path_owner(options.cache_dir):
++                logger.warning(
++                    "The directory '%s' or its parent directory is not owned "
++                    "by the current user and caching wheels has been "
++                    "disabled. check the permissions and owner of that "
++                    "directory. If executing pip with sudo, you may want "
++                    "sudo's -H flag.",
++                    options.cache_dir,
++                )
++                options.cache_dir = None
++
++            with TempDirectory(
++                options.build_dir, delete=build_delete, kind="download"
++            ) as directory:
++
++                requirement_set = RequirementSet(
++                    require_hashes=options.require_hashes,
++                )
++                self.populate_requirement_set(
++                    requirement_set,
++                    args,
++                    options,
++                    finder,
++                    session,
++                    self.name,
++                    None
++                )
++
++                preparer = RequirementPreparer(
++                    build_dir=directory.path,
++                    src_dir=options.src_dir,
++                    download_dir=options.download_dir,
++                    wheel_download_dir=None,
++                    progress_bar=options.progress_bar,
++                    build_isolation=options.build_isolation,
++                )
++
++                resolver = Resolver(
++                    preparer=preparer,
++                    finder=finder,
++                    session=session,
++                    wheel_cache=None,
++                    use_user_site=False,
++                    upgrade_strategy="to-satisfy-only",
++                    force_reinstall=False,
++                    ignore_dependencies=options.ignore_dependencies,
++                    ignore_requires_python=False,
++                    ignore_installed=True,
++                    isolated=options.isolated_mode,
++                )
++                resolver.resolve(requirement_set)
++
++                downloaded = ' '.join([
++                    req.name for req in requirement_set.successfully_downloaded
++                ])
++                if downloaded:
++                    logger.info('Successfully downloaded %s', downloaded)
++
++                # Clean up
++                if not options.no_clean:
++                    requirement_set.cleanup_files()
++
++        return requirement_set
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/completion.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/completion.py	(date 1573549700086)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/completion.py	(date 1573549700086)
+@@ -0,0 +1,94 @@
++from __future__ import absolute_import
++
++import sys
++import textwrap
++
++from pip._internal.basecommand import Command
++from pip._internal.utils.misc import get_prog
++
++BASE_COMPLETION = """
++# pip %(shell)s completion start%(script)s# pip %(shell)s completion end
++"""
++
++COMPLETION_SCRIPTS = {
++    'bash': """
++        _pip_completion()
++        {
++            COMPREPLY=( $( COMP_WORDS="${COMP_WORDS[*]}" \\
++                           COMP_CWORD=$COMP_CWORD \\
++                           PIP_AUTO_COMPLETE=1 $1 ) )
++        }
++        complete -o default -F _pip_completion %(prog)s
++    """,
++    'zsh': """
++        function _pip_completion {
++          local words cword
++          read -Ac words
++          read -cn cword
++          reply=( $( COMP_WORDS="$words[*]" \\
++                     COMP_CWORD=$(( cword-1 )) \\
++                     PIP_AUTO_COMPLETE=1 $words[1] ) )
++        }
++        compctl -K _pip_completion %(prog)s
++    """,
++    'fish': """
++        function __fish_complete_pip
++            set -lx COMP_WORDS (commandline -o) ""
++            set -lx COMP_CWORD ( \\
++                math (contains -i -- (commandline -t) $COMP_WORDS)-1 \\
++            )
++            set -lx PIP_AUTO_COMPLETE 1
++            string split \\  -- (eval $COMP_WORDS[1])
++        end
++        complete -fa "(__fish_complete_pip)" -c %(prog)s
++    """,
++}
++
++
++class CompletionCommand(Command):
++    """A helper command to be used for command completion."""
++    name = 'completion'
++    summary = 'A helper command used for command completion.'
++    ignore_require_venv = True
++
++    def __init__(self, *args, **kw):
++        super(CompletionCommand, self).__init__(*args, **kw)
++
++        cmd_opts = self.cmd_opts
++
++        cmd_opts.add_option(
++            '--bash', '-b',
++            action='store_const',
++            const='bash',
++            dest='shell',
++            help='Emit completion code for bash')
++        cmd_opts.add_option(
++            '--zsh', '-z',
++            action='store_const',
++            const='zsh',
++            dest='shell',
++            help='Emit completion code for zsh')
++        cmd_opts.add_option(
++            '--fish', '-f',
++            action='store_const',
++            const='fish',
++            dest='shell',
++            help='Emit completion code for fish')
++
++        self.parser.insert_option_group(0, cmd_opts)
++
++    def run(self, options, args):
++        """Prints the completion code of the given shell"""
++        shells = COMPLETION_SCRIPTS.keys()
++        shell_options = ['--' + shell for shell in sorted(shells)]
++        if options.shell in shells:
++            script = textwrap.dedent(
++                COMPLETION_SCRIPTS.get(options.shell, '') % {
++                    'prog': get_prog(),
++                }
++            )
++            print(BASE_COMPLETION % {'script': script, 'shell': options.shell})
++        else:
++            sys.stderr.write(
++                'ERROR: You must pass %s\n' % ' or '.join(shell_options)
++            )
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/show.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/show.py	(date 1573549700192)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/show.py	(date 1573549700192)
+@@ -0,0 +1,164 @@
++from __future__ import absolute_import
++
++import logging
++import os
++from email.parser import FeedParser  # type: ignore
++
++from pip._vendor import pkg_resources
++from pip._vendor.packaging.utils import canonicalize_name
++
++from pip._internal.basecommand import Command
++from pip._internal.status_codes import ERROR, SUCCESS
++
++logger = logging.getLogger(__name__)
++
++
++class ShowCommand(Command):
++    """Show information about one or more installed packages."""
++    name = 'show'
++    usage = """
++      %prog [options] <package> ..."""
++    summary = 'Show information about installed packages.'
++    ignore_require_venv = True
++
++    def __init__(self, *args, **kw):
++        super(ShowCommand, self).__init__(*args, **kw)
++        self.cmd_opts.add_option(
++            '-f', '--files',
++            dest='files',
++            action='store_true',
++            default=False,
++            help='Show the full list of installed files for each package.')
++
++        self.parser.insert_option_group(0, self.cmd_opts)
++
++    def run(self, options, args):
++        if not args:
++            logger.warning('ERROR: Please provide a package name or names.')
++            return ERROR
++        query = args
++
++        results = search_packages_info(query)
++        if not print_results(
++                results, list_files=options.files, verbose=options.verbose):
++            return ERROR
++        return SUCCESS
++
++
++def search_packages_info(query):
++    """
++    Gather details from installed distributions. Print distribution name,
++    version, location, and installed files. Installed files requires a
++    pip generated 'installed-files.txt' in the distributions '.egg-info'
++    directory.
++    """
++    installed = {}
++    for p in pkg_resources.working_set:
++        installed[canonicalize_name(p.project_name)] = p
++
++    query_names = [canonicalize_name(name) for name in query]
++
++    for dist in [installed[pkg] for pkg in query_names if pkg in installed]:
++        package = {
++            'name': dist.project_name,
++            'version': dist.version,
++            'location': dist.location,
++            'requires': [dep.project_name for dep in dist.requires()],
++        }
++        file_list = None
++        metadata = None
++        if isinstance(dist, pkg_resources.DistInfoDistribution):
++            # RECORDs should be part of .dist-info metadatas
++            if dist.has_metadata('RECORD'):
++                lines = dist.get_metadata_lines('RECORD')
++                paths = [l.split(',')[0] for l in lines]
++                paths = [os.path.join(dist.location, p) for p in paths]
++                file_list = [os.path.relpath(p, dist.location) for p in paths]
++
++            if dist.has_metadata('METADATA'):
++                metadata = dist.get_metadata('METADATA')
++        else:
++            # Otherwise use pip's log for .egg-info's
++            if dist.has_metadata('installed-files.txt'):
++                paths = dist.get_metadata_lines('installed-files.txt')
++                paths = [os.path.join(dist.egg_info, p) for p in paths]
++                file_list = [os.path.relpath(p, dist.location) for p in paths]
++
++            if dist.has_metadata('PKG-INFO'):
++                metadata = dist.get_metadata('PKG-INFO')
++
++        if dist.has_metadata('entry_points.txt'):
++            entry_points = dist.get_metadata_lines('entry_points.txt')
++            package['entry_points'] = entry_points
++
++        if dist.has_metadata('INSTALLER'):
++            for line in dist.get_metadata_lines('INSTALLER'):
++                if line.strip():
++                    package['installer'] = line.strip()
++                    break
++
++        # @todo: Should pkg_resources.Distribution have a
++        # `get_pkg_info` method?
++        feed_parser = FeedParser()
++        feed_parser.feed(metadata)
++        pkg_info_dict = feed_parser.close()
++        for key in ('metadata-version', 'summary',
++                    'home-page', 'author', 'author-email', 'license'):
++            package[key] = pkg_info_dict.get(key)
++
++        # It looks like FeedParser cannot deal with repeated headers
++        classifiers = []
++        for line in metadata.splitlines():
++            if line.startswith('Classifier: '):
++                classifiers.append(line[len('Classifier: '):])
++        package['classifiers'] = classifiers
++
++        if file_list:
++            package['files'] = sorted(file_list)
++        yield package
++
++
++def print_results(distributions, list_files=False, verbose=False):
++    """
++    Print the informations from installed distributions found.
++    """
++    results_printed = False
++    for i, dist in enumerate(distributions):
++        results_printed = True
++        if i > 0:
++            logger.info("---")
++
++        name = dist.get('name', '')
++        required_by = [
++            pkg.project_name for pkg in pkg_resources.working_set
++            if name in [required.name for required in pkg.requires()]
++        ]
++
++        logger.info("Name: %s", name)
++        logger.info("Version: %s", dist.get('version', ''))
++        logger.info("Summary: %s", dist.get('summary', ''))
++        logger.info("Home-page: %s", dist.get('home-page', ''))
++        logger.info("Author: %s", dist.get('author', ''))
++        logger.info("Author-email: %s", dist.get('author-email', ''))
++        logger.info("License: %s", dist.get('license', ''))
++        logger.info("Location: %s", dist.get('location', ''))
++        logger.info("Requires: %s", ', '.join(dist.get('requires', [])))
++        logger.info("Required-by: %s", ', '.join(required_by))
++
++        if verbose:
++            logger.info("Metadata-Version: %s",
++                        dist.get('metadata-version', ''))
++            logger.info("Installer: %s", dist.get('installer', ''))
++            logger.info("Classifiers:")
++            for classifier in dist.get('classifiers', []):
++                logger.info("  %s", classifier)
++            logger.info("Entry-points:")
++            for entry in dist.get('entry_points', []):
++                logger.info("  %s", entry.strip())
++        if list_files:
++            logger.info("Files:")
++            for line in dist.get('files', []):
++                logger.info("  %s", line.strip())
++            if "files" not in dist:
++                logger.info("Cannot locate installed-files.txt")
++    return results_printed
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/list.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/list.py	(date 1573549700178)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/list.py	(date 1573549700178)
+@@ -0,0 +1,343 @@
++from __future__ import absolute_import
++
++import json
++import logging
++import warnings
++
++from pip._vendor import six
++from pip._vendor.six.moves import zip_longest
++
++from pip._internal.basecommand import Command
++from pip._internal.cmdoptions import index_group, make_option_group
++from pip._internal.exceptions import CommandError
++from pip._internal.index import PackageFinder
++from pip._internal.utils.deprecation import RemovedInPip11Warning
++from pip._internal.utils.misc import (
++    dist_is_editable, get_installed_distributions,
++)
++from pip._internal.utils.packaging import get_installer
++
++logger = logging.getLogger(__name__)
++
++
++class ListCommand(Command):
++    """
++    List installed packages, including editables.
++
++    Packages are listed in a case-insensitive sorted order.
++    """
++    name = 'list'
++    usage = """
++      %prog [options]"""
++    summary = 'List installed packages.'
++
++    def __init__(self, *args, **kw):
++        super(ListCommand, self).__init__(*args, **kw)
++
++        cmd_opts = self.cmd_opts
++
++        cmd_opts.add_option(
++            '-o', '--outdated',
++            action='store_true',
++            default=False,
++            help='List outdated packages')
++        cmd_opts.add_option(
++            '-u', '--uptodate',
++            action='store_true',
++            default=False,
++            help='List uptodate packages')
++        cmd_opts.add_option(
++            '-e', '--editable',
++            action='store_true',
++            default=False,
++            help='List editable projects.')
++        cmd_opts.add_option(
++            '-l', '--local',
++            action='store_true',
++            default=False,
++            help=('If in a virtualenv that has global access, do not list '
++                  'globally-installed packages.'),
++        )
++        self.cmd_opts.add_option(
++            '--user',
++            dest='user',
++            action='store_true',
++            default=False,
++            help='Only output packages installed in user-site.')
++
++        cmd_opts.add_option(
++            '--pre',
++            action='store_true',
++            default=False,
++            help=("Include pre-release and development versions. By default, "
++                  "pip only finds stable versions."),
++        )
++
++        cmd_opts.add_option(
++            '--format',
++            action='store',
++            dest='list_format',
++            default="columns",
++            choices=('legacy', 'columns', 'freeze', 'json'),
++            help="Select the output format among: columns (default), freeze, "
++                 "json, or legacy.",
++        )
++
++        cmd_opts.add_option(
++            '--not-required',
++            action='store_true',
++            dest='not_required',
++            help="List packages that are not dependencies of "
++                 "installed packages.",
++        )
++
++        cmd_opts.add_option(
++            '--exclude-editable',
++            action='store_false',
++            dest='include_editable',
++            help='Exclude editable package from output.',
++        )
++        cmd_opts.add_option(
++            '--include-editable',
++            action='store_true',
++            dest='include_editable',
++            help='Include editable package from output.',
++            default=True,
++        )
++        index_opts = make_option_group(index_group, self.parser)
++
++        self.parser.insert_option_group(0, index_opts)
++        self.parser.insert_option_group(0, cmd_opts)
++
++    def _build_package_finder(self, options, index_urls, session):
++        """
++        Create a package finder appropriate to this list command.
++        """
++        return PackageFinder(
++            find_links=options.find_links,
++            index_urls=index_urls,
++            allow_all_prereleases=options.pre,
++            trusted_hosts=options.trusted_hosts,
++            process_dependency_links=options.process_dependency_links,
++            session=session,
++        )
++
++    def run(self, options, args):
++        if options.list_format == "legacy":
++            warnings.warn(
++                "The legacy format has been deprecated and will be removed "
++                "in the future.",
++                RemovedInPip11Warning,
++            )
++
++        if options.outdated and options.uptodate:
++            raise CommandError(
++                "Options --outdated and --uptodate cannot be combined.")
++
++        packages = get_installed_distributions(
++            local_only=options.local,
++            user_only=options.user,
++            editables_only=options.editable,
++            include_editables=options.include_editable,
++        )
++
++        if options.outdated:
++            packages = self.get_outdated(packages, options)
++        elif options.uptodate:
++            packages = self.get_uptodate(packages, options)
++
++        if options.not_required:
++            packages = self.get_not_required(packages, options)
++
++        self.output_package_listing(packages, options)
++
++    def get_outdated(self, packages, options):
++        return [
++            dist for dist in self.iter_packages_latest_infos(packages, options)
++            if dist.latest_version > dist.parsed_version
++        ]
++
++    def get_uptodate(self, packages, options):
++        return [
++            dist for dist in self.iter_packages_latest_infos(packages, options)
++            if dist.latest_version == dist.parsed_version
++        ]
++
++    def get_not_required(self, packages, options):
++        dep_keys = set()
++        for dist in packages:
++            dep_keys.update(requirement.key for requirement in dist.requires())
++        return {pkg for pkg in packages if pkg.key not in dep_keys}
++
++    def iter_packages_latest_infos(self, packages, options):
++        index_urls = [options.index_url] + options.extra_index_urls
++        if options.no_index:
++            logger.debug('Ignoring indexes: %s', ','.join(index_urls))
++            index_urls = []
++
++        dependency_links = []
++        for dist in packages:
++            if dist.has_metadata('dependency_links.txt'):
++                dependency_links.extend(
++                    dist.get_metadata_lines('dependency_links.txt'),
++                )
++
++        with self._build_session(options) as session:
++            finder = self._build_package_finder(options, index_urls, session)
++            finder.add_dependency_links(dependency_links)
++
++            for dist in packages:
++                typ = 'unknown'
++                all_candidates = finder.find_all_candidates(dist.key)
++                if not options.pre:
++                    # Remove prereleases
++                    all_candidates = [candidate for candidate in all_candidates
++                                      if not candidate.version.is_prerelease]
++
++                if not all_candidates:
++                    continue
++                best_candidate = max(all_candidates,
++                                     key=finder._candidate_sort_key)
++                remote_version = best_candidate.version
++                if best_candidate.location.is_wheel:
++                    typ = 'wheel'
++                else:
++                    typ = 'sdist'
++                # This is dirty but makes the rest of the code much cleaner
++                dist.latest_version = remote_version
++                dist.latest_filetype = typ
++                yield dist
++
++    def output_legacy(self, dist, options):
++        if options.verbose >= 1:
++            return '%s (%s, %s, %s)' % (
++                dist.project_name,
++                dist.version,
++                dist.location,
++                get_installer(dist),
++            )
++        elif dist_is_editable(dist):
++            return '%s (%s, %s)' % (
++                dist.project_name,
++                dist.version,
++                dist.location,
++            )
++        else:
++            return '%s (%s)' % (dist.project_name, dist.version)
++
++    def output_legacy_latest(self, dist, options):
++        return '%s - Latest: %s [%s]' % (
++            self.output_legacy(dist, options),
++            dist.latest_version,
++            dist.latest_filetype,
++        )
++
++    def output_package_listing(self, packages, options):
++        packages = sorted(
++            packages,
++            key=lambda dist: dist.project_name.lower(),
++        )
++        if options.list_format == 'columns' and packages:
++            data, header = format_for_columns(packages, options)
++            self.output_package_listing_columns(data, header)
++        elif options.list_format == 'freeze':
++            for dist in packages:
++                if options.verbose >= 1:
++                    logger.info("%s==%s (%s)", dist.project_name,
++                                dist.version, dist.location)
++                else:
++                    logger.info("%s==%s", dist.project_name, dist.version)
++        elif options.list_format == 'json':
++            logger.info(format_for_json(packages, options))
++        elif options.list_format == "legacy":
++            for dist in packages:
++                if options.outdated:
++                    logger.info(self.output_legacy_latest(dist, options))
++                else:
++                    logger.info(self.output_legacy(dist, options))
++
++    def output_package_listing_columns(self, data, header):
++        # insert the header first: we need to know the size of column names
++        if len(data) > 0:
++            data.insert(0, header)
++
++        pkg_strings, sizes = tabulate(data)
++
++        # Create and add a separator.
++        if len(data) > 0:
++            pkg_strings.insert(1, " ".join(map(lambda x: '-' * x, sizes)))
++
++        for val in pkg_strings:
++            logger.info(val)
++
++
++def tabulate(vals):
++    # From pfmoore on GitHub:
++    # https://github.com/pypa/pip/issues/3651#issuecomment-216932564
++    assert len(vals) > 0
++
++    sizes = [0] * max(len(x) for x in vals)
++    for row in vals:
++        sizes = [max(s, len(str(c))) for s, c in zip_longest(sizes, row)]
++
++    result = []
++    for row in vals:
++        display = " ".join([str(c).ljust(s) if c is not None else ''
++                            for s, c in zip_longest(sizes, row)])
++        result.append(display)
++
++    return result, sizes
++
++
++def format_for_columns(pkgs, options):
++    """
++    Convert the package data into something usable
++    by output_package_listing_columns.
++    """
++    running_outdated = options.outdated
++    # Adjust the header for the `pip list --outdated` case.
++    if running_outdated:
++        header = ["Package", "Version", "Latest", "Type"]
++    else:
++        header = ["Package", "Version"]
++
++    data = []
++    if options.verbose >= 1 or any(dist_is_editable(x) for x in pkgs):
++        header.append("Location")
++    if options.verbose >= 1:
++        header.append("Installer")
++
++    for proj in pkgs:
++        # if we're working on the 'outdated' list, separate out the
++        # latest_version and type
++        row = [proj.project_name, proj.version]
++
++        if running_outdated:
++            row.append(proj.latest_version)
++            row.append(proj.latest_filetype)
++
++        if options.verbose >= 1 or dist_is_editable(proj):
++            row.append(proj.location)
++        if options.verbose >= 1:
++            row.append(get_installer(proj))
++
++        data.append(row)
++
++    return data, header
++
++
++def format_for_json(packages, options):
++    data = []
++    for dist in packages:
++        info = {
++            'name': dist.project_name,
++            'version': six.text_type(dist.version),
++        }
++        if options.verbose >= 1:
++            info['location'] = dist.location
++            info['installer'] = get_installer(dist)
++        if options.outdated:
++            info['latest_version'] = six.text_type(dist.latest_version)
++            info['latest_filetype'] = dist.latest_filetype
++        data.append(info)
++    return json.dumps(data)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/help.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/help.py	(date 1573549700164)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/help.py	(date 1573549700164)
+@@ -0,0 +1,36 @@
++from __future__ import absolute_import
++
++from pip._internal.basecommand import SUCCESS, Command
++from pip._internal.exceptions import CommandError
++
++
++class HelpCommand(Command):
++    """Show help for commands"""
++    name = 'help'
++    usage = """
++      %prog <command>"""
++    summary = 'Show help for commands.'
++    ignore_require_venv = True
++
++    def run(self, options, args):
++        from pip._internal.commands import commands_dict, get_similar_commands
++
++        try:
++            # 'pip help' with no args is handled by pip.__init__.parseopt()
++            cmd_name = args[0]  # the command we need help for
++        except IndexError:
++            return SUCCESS
++
++        if cmd_name not in commands_dict:
++            guess = get_similar_commands(cmd_name)
++
++            msg = ['unknown command "%s"' % cmd_name]
++            if guess:
++                msg.append('maybe you meant "%s"' % guess)
++
++            raise CommandError(' - '.join(msg))
++
++        command = commands_dict[cmd_name]()
++        command.parser.print_help()
++
++        return SUCCESS
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/wheel.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/wheel.py	(date 1573549700206)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/wheel.py	(date 1573549700206)
+@@ -0,0 +1,179 @@
++# -*- coding: utf-8 -*-
++from __future__ import absolute_import
++
++import logging
++import os
++
++from pip._internal import cmdoptions
++from pip._internal.basecommand import RequirementCommand
++from pip._internal.cache import WheelCache
++from pip._internal.exceptions import CommandError, PreviousBuildDirError
++from pip._internal.operations.prepare import RequirementPreparer
++from pip._internal.req import RequirementSet
++from pip._internal.resolve import Resolver
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.wheel import WheelBuilder
++
++logger = logging.getLogger(__name__)
++
++
++class WheelCommand(RequirementCommand):
++    """
++    Build Wheel archives for your requirements and dependencies.
++
++    Wheel is a built-package format, and offers the advantage of not
++    recompiling your software during every install. For more details, see the
++    wheel docs: https://wheel.readthedocs.io/en/latest/
++
++    Requirements: setuptools>=0.8, and wheel.
++
++    'pip wheel' uses the bdist_wheel setuptools extension from the wheel
++    package to build individual wheels.
++
++    """
++
++    name = 'wheel'
++    usage = """
++      %prog [options] <requirement specifier> ...
++      %prog [options] -r <requirements file> ...
++      %prog [options] [-e] <vcs project url> ...
++      %prog [options] [-e] <local project path> ...
++      %prog [options] <archive url/path> ..."""
++
++    summary = 'Build wheels from your requirements.'
++
++    def __init__(self, *args, **kw):
++        super(WheelCommand, self).__init__(*args, **kw)
++
++        cmd_opts = self.cmd_opts
++
++        cmd_opts.add_option(
++            '-w', '--wheel-dir',
++            dest='wheel_dir',
++            metavar='dir',
++            default=os.curdir,
++            help=("Build wheels into <dir>, where the default is the "
++                  "current working directory."),
++        )
++        cmd_opts.add_option(cmdoptions.no_binary())
++        cmd_opts.add_option(cmdoptions.only_binary())
++        cmd_opts.add_option(
++            '--build-option',
++            dest='build_options',
++            metavar='options',
++            action='append',
++            help="Extra arguments to be supplied to 'setup.py bdist_wheel'.",
++        )
++        cmd_opts.add_option(cmdoptions.no_build_isolation())
++        cmd_opts.add_option(cmdoptions.constraints())
++        cmd_opts.add_option(cmdoptions.editable())
++        cmd_opts.add_option(cmdoptions.requirements())
++        cmd_opts.add_option(cmdoptions.src())
++        cmd_opts.add_option(cmdoptions.ignore_requires_python())
++        cmd_opts.add_option(cmdoptions.no_deps())
++        cmd_opts.add_option(cmdoptions.build_dir())
++        cmd_opts.add_option(cmdoptions.progress_bar())
++
++        cmd_opts.add_option(
++            '--global-option',
++            dest='global_options',
++            action='append',
++            metavar='options',
++            help="Extra global options to be supplied to the setup.py "
++            "call before the 'bdist_wheel' command.")
++
++        cmd_opts.add_option(
++            '--pre',
++            action='store_true',
++            default=False,
++            help=("Include pre-release and development versions. By default, "
++                  "pip only finds stable versions."),
++        )
++
++        cmd_opts.add_option(cmdoptions.no_clean())
++        cmd_opts.add_option(cmdoptions.require_hashes())
++
++        index_opts = cmdoptions.make_option_group(
++            cmdoptions.index_group,
++            self.parser,
++        )
++
++        self.parser.insert_option_group(0, index_opts)
++        self.parser.insert_option_group(0, cmd_opts)
++
++    def run(self, options, args):
++        cmdoptions.check_install_build_global(options)
++
++        index_urls = [options.index_url] + options.extra_index_urls
++        if options.no_index:
++            logger.debug('Ignoring indexes: %s', ','.join(index_urls))
++            index_urls = []
++
++        if options.build_dir:
++            options.build_dir = os.path.abspath(options.build_dir)
++
++        options.src_dir = os.path.abspath(options.src_dir)
++
++        with self._build_session(options) as session:
++            finder = self._build_package_finder(options, session)
++            build_delete = (not (options.no_clean or options.build_dir))
++            wheel_cache = WheelCache(options.cache_dir, options.format_control)
++
++            with TempDirectory(
++                options.build_dir, delete=build_delete, kind="wheel"
++            ) as directory:
++                requirement_set = RequirementSet(
++                    require_hashes=options.require_hashes,
++                )
++
++                try:
++                    self.populate_requirement_set(
++                        requirement_set, args, options, finder, session,
++                        self.name, wheel_cache
++                    )
++
++                    preparer = RequirementPreparer(
++                        build_dir=directory.path,
++                        src_dir=options.src_dir,
++                        download_dir=None,
++                        wheel_download_dir=options.wheel_dir,
++                        progress_bar=options.progress_bar,
++                        build_isolation=options.build_isolation,
++                    )
++
++                    resolver = Resolver(
++                        preparer=preparer,
++                        finder=finder,
++                        session=session,
++                        wheel_cache=wheel_cache,
++                        use_user_site=False,
++                        upgrade_strategy="to-satisfy-only",
++                        force_reinstall=False,
++                        ignore_dependencies=options.ignore_dependencies,
++                        ignore_requires_python=options.ignore_requires_python,
++                        ignore_installed=True,
++                        isolated=options.isolated_mode,
++                    )
++                    resolver.resolve(requirement_set)
++
++                    # build wheels
++                    wb = WheelBuilder(
++                        finder, preparer, wheel_cache,
++                        build_options=options.build_options or [],
++                        global_options=options.global_options or [],
++                        no_clean=options.no_clean,
++                    )
++                    wheels_built_successfully = wb.build(
++                        requirement_set.requirements.values(), session=session,
++                    )
++                    if not wheels_built_successfully:
++                        raise CommandError(
++                            "Failed to build one or more wheels"
++                        )
++                except PreviousBuildDirError:
++                    options.no_clean = True
++                    raise
++                finally:
++                    if not options.no_clean:
++                        requirement_set.cleanup_files()
++                        wheel_cache.cleanup()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/search.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/search.py	(date 1573549700185)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/search.py	(date 1573549700185)
+@@ -0,0 +1,135 @@
++from __future__ import absolute_import
++
++import logging
++import sys
++import textwrap
++from collections import OrderedDict
++
++from pip._vendor import pkg_resources
++from pip._vendor.packaging.version import parse as parse_version
++# NOTE: XMLRPC Client is not annotated in typeshed as on 2017-07-17, which is
++#       why we ignore the type on this import
++from pip._vendor.six.moves import xmlrpc_client  # type: ignore
++
++from pip._internal.basecommand import SUCCESS, Command
++from pip._internal.compat import get_terminal_size
++from pip._internal.download import PipXmlrpcTransport
++from pip._internal.exceptions import CommandError
++from pip._internal.models import PyPI
++from pip._internal.status_codes import NO_MATCHES_FOUND
++from pip._internal.utils.logging import indent_log
++
++logger = logging.getLogger(__name__)
++
++
++class SearchCommand(Command):
++    """Search for PyPI packages whose name or summary contains <query>."""
++    name = 'search'
++    usage = """
++      %prog [options] <query>"""
++    summary = 'Search PyPI for packages.'
++    ignore_require_venv = True
++
++    def __init__(self, *args, **kw):
++        super(SearchCommand, self).__init__(*args, **kw)
++        self.cmd_opts.add_option(
++            '-i', '--index',
++            dest='index',
++            metavar='URL',
++            default=PyPI.pypi_url,
++            help='Base URL of Python Package Index (default %default)')
++
++        self.parser.insert_option_group(0, self.cmd_opts)
++
++    def run(self, options, args):
++        if not args:
++            raise CommandError('Missing required argument (search query).')
++        query = args
++        pypi_hits = self.search(query, options)
++        hits = transform_hits(pypi_hits)
++
++        terminal_width = None
++        if sys.stdout.isatty():
++            terminal_width = get_terminal_size()[0]
++
++        print_results(hits, terminal_width=terminal_width)
++        if pypi_hits:
++            return SUCCESS
++        return NO_MATCHES_FOUND
++
++    def search(self, query, options):
++        index_url = options.index
++        with self._build_session(options) as session:
++            transport = PipXmlrpcTransport(index_url, session)
++            pypi = xmlrpc_client.ServerProxy(index_url, transport)
++            hits = pypi.search({'name': query, 'summary': query}, 'or')
++            return hits
++
++
++def transform_hits(hits):
++    """
++    The list from pypi is really a list of versions. We want a list of
++    packages with the list of versions stored inline. This converts the
++    list from pypi into one we can use.
++    """
++    packages = OrderedDict()
++    for hit in hits:
++        name = hit['name']
++        summary = hit['summary']
++        version = hit['version']
++
++        if name not in packages.keys():
++            packages[name] = {
++                'name': name,
++                'summary': summary,
++                'versions': [version],
++            }
++        else:
++            packages[name]['versions'].append(version)
++
++            # if this is the highest version, replace summary and score
++            if version == highest_version(packages[name]['versions']):
++                packages[name]['summary'] = summary
++
++    return list(packages.values())
++
++
++def print_results(hits, name_column_width=None, terminal_width=None):
++    if not hits:
++        return
++    if name_column_width is None:
++        name_column_width = max([
++            len(hit['name']) + len(highest_version(hit.get('versions', ['-'])))
++            for hit in hits
++        ]) + 4
++
++    installed_packages = [p.project_name for p in pkg_resources.working_set]
++    for hit in hits:
++        name = hit['name']
++        summary = hit['summary'] or ''
++        latest = highest_version(hit.get('versions', ['-']))
++        if terminal_width is not None:
++            target_width = terminal_width - name_column_width - 5
++            if target_width > 10:
++                # wrap and indent summary to fit terminal
++                summary = textwrap.wrap(summary, target_width)
++                summary = ('\n' + ' ' * (name_column_width + 3)).join(summary)
++
++        line = '%-*s - %s' % (name_column_width,
++                              '%s (%s)' % (name, latest), summary)
++        try:
++            logger.info(line)
++            if name in installed_packages:
++                dist = pkg_resources.get_distribution(name)
++                with indent_log():
++                    if dist.version == latest:
++                        logger.info('INSTALLED: %s (latest)', dist.version)
++                    else:
++                        logger.info('INSTALLED: %s', dist.version)
++                        logger.info('LATEST:    %s', latest)
++        except UnicodeEncodeError:
++            pass
++
++
++def highest_version(versions):
++    return max(versions, key=parse_version)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/check.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/check.py	(date 1573549700081)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/check.py	(date 1573549700081)
+@@ -0,0 +1,42 @@
++import logging
++
++from pip._internal.basecommand import Command
++from pip._internal.operations.check import (
++    check_package_set, create_package_set_from_installed,
++)
++from pip._internal.utils.misc import get_installed_distributions
++
++logger = logging.getLogger(__name__)
++
++
++class CheckCommand(Command):
++    """Verify installed packages have compatible dependencies."""
++    name = 'check'
++    usage = """
++      %prog [options]"""
++    summary = 'Verify installed packages have compatible dependencies.'
++
++    def run(self, options, args):
++        package_set = create_package_set_from_installed()
++        missing, conflicting = check_package_set(package_set)
++
++        for project_name in missing:
++            version = package_set[project_name].version
++            for dependency in missing[project_name]:
++                logger.info(
++                    "%s %s requires %s, which is not installed.",
++                    project_name, version, dependency[0],
++                )
++
++        for project_name in conflicting:
++            version = package_set[project_name].version
++            for dep_name, dep_version, req in conflicting[project_name]:
++                logger.info(
++                    "%s %s has requirement %s, but you have %s %s.",
++                    project_name, version, req, dep_name, dep_version,
++                )
++
++        if missing or conflicting:
++            return 1
++        else:
++            logger.info("No broken requirements found.")
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/uninstall.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/uninstall.py	(date 1573549700199)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/uninstall.py	(date 1573549700199)
+@@ -0,0 +1,71 @@
++from __future__ import absolute_import
++
++from pip._vendor.packaging.utils import canonicalize_name
++
++from pip._internal.basecommand import Command
++from pip._internal.exceptions import InstallationError
++from pip._internal.req import InstallRequirement, parse_requirements
++
++
++class UninstallCommand(Command):
++    """
++    Uninstall packages.
++
++    pip is able to uninstall most installed packages. Known exceptions are:
++
++    - Pure distutils packages installed with ``python setup.py install``, which
++      leave behind no metadata to determine what files were installed.
++    - Script wrappers installed by ``python setup.py develop``.
++    """
++    name = 'uninstall'
++    usage = """
++      %prog [options] <package> ...
++      %prog [options] -r <requirements file> ..."""
++    summary = 'Uninstall packages.'
++
++    def __init__(self, *args, **kw):
++        super(UninstallCommand, self).__init__(*args, **kw)
++        self.cmd_opts.add_option(
++            '-r', '--requirement',
++            dest='requirements',
++            action='append',
++            default=[],
++            metavar='file',
++            help='Uninstall all the packages listed in the given requirements '
++                 'file.  This option can be used multiple times.',
++        )
++        self.cmd_opts.add_option(
++            '-y', '--yes',
++            dest='yes',
++            action='store_true',
++            help="Don't ask for confirmation of uninstall deletions.")
++
++        self.parser.insert_option_group(0, self.cmd_opts)
++
++    def run(self, options, args):
++        with self._build_session(options) as session:
++            reqs_to_uninstall = {}
++            for name in args:
++                req = InstallRequirement.from_line(
++                    name, isolated=options.isolated_mode,
++                )
++                if req.name:
++                    reqs_to_uninstall[canonicalize_name(req.name)] = req
++            for filename in options.requirements:
++                for req in parse_requirements(
++                        filename,
++                        options=options,
++                        session=session):
++                    if req.name:
++                        reqs_to_uninstall[canonicalize_name(req.name)] = req
++            if not reqs_to_uninstall:
++                raise InstallationError(
++                    'You must give at least one requirement to %(name)s (see '
++                    '"pip help %(name)s")' % dict(name=self.name)
++                )
++            for req in reqs_to_uninstall.values():
++                uninstall_pathset = req.uninstall(
++                    auto_confirm=options.yes, verbose=self.verbosity > 0,
++                )
++                if uninstall_pathset:
++                    uninstall_pathset.commit()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/install.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/install.py	(date 1573549700172)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/install.py	(date 1573549700172)
+@@ -0,0 +1,502 @@
++from __future__ import absolute_import
++
++import errno
++import logging
++import operator
++import os
++import shutil
++from optparse import SUPPRESS_HELP
++
++from pip._internal import cmdoptions
++from pip._internal.basecommand import RequirementCommand
++from pip._internal.cache import WheelCache
++from pip._internal.exceptions import (
++    CommandError, InstallationError, PreviousBuildDirError,
++)
++from pip._internal.locations import distutils_scheme, virtualenv_no_global
++from pip._internal.operations.check import check_install_conflicts
++from pip._internal.operations.prepare import RequirementPreparer
++from pip._internal.req import RequirementSet, install_given_reqs
++from pip._internal.resolve import Resolver
++from pip._internal.status_codes import ERROR
++from pip._internal.utils.filesystem import check_path_owner
++from pip._internal.utils.misc import ensure_dir, get_installed_version
++from pip._internal.utils.temp_dir import TempDirectory
++from pip._internal.wheel import WheelBuilder
++
++try:
++    import wheel
++except ImportError:
++    wheel = None
++
++
++logger = logging.getLogger(__name__)
++
++
++class InstallCommand(RequirementCommand):
++    """
++    Install packages from:
++
++    - PyPI (and other indexes) using requirement specifiers.
++    - VCS project urls.
++    - Local project directories.
++    - Local or remote source archives.
++
++    pip also supports installing from "requirements files", which provide
++    an easy way to specify a whole environment to be installed.
++    """
++    name = 'install'
++
++    usage = """
++      %prog [options] <requirement specifier> [package-index-options] ...
++      %prog [options] -r <requirements file> [package-index-options] ...
++      %prog [options] [-e] <vcs project url> ...
++      %prog [options] [-e] <local project path> ...
++      %prog [options] <archive url/path> ..."""
++
++    summary = 'Install packages.'
++
++    def __init__(self, *args, **kw):
++        super(InstallCommand, self).__init__(*args, **kw)
++
++        cmd_opts = self.cmd_opts
++
++        cmd_opts.add_option(cmdoptions.requirements())
++        cmd_opts.add_option(cmdoptions.constraints())
++        cmd_opts.add_option(cmdoptions.no_deps())
++        cmd_opts.add_option(cmdoptions.pre())
++
++        cmd_opts.add_option(cmdoptions.editable())
++        cmd_opts.add_option(
++            '-t', '--target',
++            dest='target_dir',
++            metavar='dir',
++            default=None,
++            help='Install packages into <dir>. '
++                 'By default this will not replace existing files/folders in '
++                 '<dir>. Use --upgrade to replace existing packages in <dir> '
++                 'with new versions.'
++        )
++        cmd_opts.add_option(
++            '--user',
++            dest='use_user_site',
++            action='store_true',
++            help="Install to the Python user install directory for your "
++                 "platform. Typically ~/.local/, or %APPDATA%\\Python on "
++                 "Windows. (See the Python documentation for site.USER_BASE "
++                 "for full details.)")
++        cmd_opts.add_option(
++            '--no-user',
++            dest='use_user_site',
++            action='store_false',
++            help=SUPPRESS_HELP)
++        cmd_opts.add_option(
++            '--root',
++            dest='root_path',
++            metavar='dir',
++            default=None,
++            help="Install everything relative to this alternate root "
++                 "directory.")
++        cmd_opts.add_option(
++            '--prefix',
++            dest='prefix_path',
++            metavar='dir',
++            default=None,
++            help="Installation prefix where lib, bin and other top-level "
++                 "folders are placed")
++
++        cmd_opts.add_option(cmdoptions.build_dir())
++
++        cmd_opts.add_option(cmdoptions.src())
++
++        cmd_opts.add_option(
++            '-U', '--upgrade',
++            dest='upgrade',
++            action='store_true',
++            help='Upgrade all specified packages to the newest available '
++                 'version. The handling of dependencies depends on the '
++                 'upgrade-strategy used.'
++        )
++
++        cmd_opts.add_option(
++            '--upgrade-strategy',
++            dest='upgrade_strategy',
++            default='only-if-needed',
++            choices=['only-if-needed', 'eager'],
++            help='Determines how dependency upgrading should be handled '
++                 '[default: %default]. '
++                 '"eager" - dependencies are upgraded regardless of '
++                 'whether the currently installed version satisfies the '
++                 'requirements of the upgraded package(s). '
++                 '"only-if-needed" -  are upgraded only when they do not '
++                 'satisfy the requirements of the upgraded package(s).'
++        )
++
++        cmd_opts.add_option(
++            '--force-reinstall',
++            dest='force_reinstall',
++            action='store_true',
++            help='Reinstall all packages even if they are already '
++                 'up-to-date.')
++
++        cmd_opts.add_option(
++            '-I', '--ignore-installed',
++            dest='ignore_installed',
++            action='store_true',
++            help='Ignore the installed packages (reinstalling instead).')
++
++        cmd_opts.add_option(cmdoptions.ignore_requires_python())
++        cmd_opts.add_option(cmdoptions.no_build_isolation())
++
++        cmd_opts.add_option(cmdoptions.install_options())
++        cmd_opts.add_option(cmdoptions.global_options())
++
++        cmd_opts.add_option(
++            "--compile",
++            action="store_true",
++            dest="compile",
++            default=True,
++            help="Compile Python source files to bytecode",
++        )
++
++        cmd_opts.add_option(
++            "--no-compile",
++            action="store_false",
++            dest="compile",
++            help="Do not compile Python source files to bytecode",
++        )
++
++        cmd_opts.add_option(
++            "--no-warn-script-location",
++            action="store_false",
++            dest="warn_script_location",
++            default=True,
++            help="Do not warn when installing scripts outside PATH",
++        )
++        cmd_opts.add_option(
++            "--no-warn-conflicts",
++            action="store_false",
++            dest="warn_about_conflicts",
++            default=True,
++            help="Do not warn about broken dependencies",
++        )
++
++        cmd_opts.add_option(cmdoptions.no_binary())
++        cmd_opts.add_option(cmdoptions.only_binary())
++        cmd_opts.add_option(cmdoptions.no_clean())
++        cmd_opts.add_option(cmdoptions.require_hashes())
++        cmd_opts.add_option(cmdoptions.progress_bar())
++
++        index_opts = cmdoptions.make_option_group(
++            cmdoptions.index_group,
++            self.parser,
++        )
++
++        self.parser.insert_option_group(0, index_opts)
++        self.parser.insert_option_group(0, cmd_opts)
++
++    def run(self, options, args):
++        cmdoptions.check_install_build_global(options)
++
++        upgrade_strategy = "to-satisfy-only"
++        if options.upgrade:
++            upgrade_strategy = options.upgrade_strategy
++
++        if options.build_dir:
++            options.build_dir = os.path.abspath(options.build_dir)
++
++        options.src_dir = os.path.abspath(options.src_dir)
++        install_options = options.install_options or []
++        if options.use_user_site:
++            if options.prefix_path:
++                raise CommandError(
++                    "Can not combine '--user' and '--prefix' as they imply "
++                    "different installation locations"
++                )
++            if virtualenv_no_global():
++                raise InstallationError(
++                    "Can not perform a '--user' install. User site-packages "
++                    "are not visible in this virtualenv."
++                )
++            install_options.append('--user')
++            install_options.append('--prefix=')
++
++        target_temp_dir = TempDirectory(kind="target")
++        if options.target_dir:
++            options.ignore_installed = True
++            options.target_dir = os.path.abspath(options.target_dir)
++            if (os.path.exists(options.target_dir) and not
++                    os.path.isdir(options.target_dir)):
++                raise CommandError(
++                    "Target path exists but is not a directory, will not "
++                    "continue."
++                )
++
++            # Create a target directory for using with the target option
++            target_temp_dir.create()
++            install_options.append('--home=' + target_temp_dir.path)
++
++        global_options = options.global_options or []
++
++        with self._build_session(options) as session:
++            finder = self._build_package_finder(options, session)
++            build_delete = (not (options.no_clean or options.build_dir))
++            wheel_cache = WheelCache(options.cache_dir, options.format_control)
++
++            if options.cache_dir and not check_path_owner(options.cache_dir):
++                logger.warning(
++                    "The directory '%s' or its parent directory is not owned "
++                    "by the current user and caching wheels has been "
++                    "disabled. check the permissions and owner of that "
++                    "directory. If executing pip with sudo, you may want "
++                    "sudo's -H flag.",
++                    options.cache_dir,
++                )
++                options.cache_dir = None
++
++            with TempDirectory(
++                options.build_dir, delete=build_delete, kind="install"
++            ) as directory:
++                requirement_set = RequirementSet(
++                    require_hashes=options.require_hashes,
++                )
++
++                try:
++                    self.populate_requirement_set(
++                        requirement_set, args, options, finder, session,
++                        self.name, wheel_cache
++                    )
++                    preparer = RequirementPreparer(
++                        build_dir=directory.path,
++                        src_dir=options.src_dir,
++                        download_dir=None,
++                        wheel_download_dir=None,
++                        progress_bar=options.progress_bar,
++                        build_isolation=options.build_isolation,
++                    )
++
++                    resolver = Resolver(
++                        preparer=preparer,
++                        finder=finder,
++                        session=session,
++                        wheel_cache=wheel_cache,
++                        use_user_site=options.use_user_site,
++                        upgrade_strategy=upgrade_strategy,
++                        force_reinstall=options.force_reinstall,
++                        ignore_dependencies=options.ignore_dependencies,
++                        ignore_requires_python=options.ignore_requires_python,
++                        ignore_installed=options.ignore_installed,
++                        isolated=options.isolated_mode,
++                    )
++                    resolver.resolve(requirement_set)
++
++                    # If caching is disabled or wheel is not installed don't
++                    # try to build wheels.
++                    if wheel and options.cache_dir:
++                        # build wheels before install.
++                        wb = WheelBuilder(
++                            finder, preparer, wheel_cache,
++                            build_options=[], global_options=[],
++                        )
++                        # Ignore the result: a failed wheel will be
++                        # installed from the sdist/vcs whatever.
++                        wb.build(
++                            requirement_set.requirements.values(),
++                            session=session, autobuilding=True
++                        )
++
++                    to_install = resolver.get_installation_order(
++                        requirement_set
++                    )
++
++                    # Consistency Checking of the package set we're installing.
++                    should_warn_about_conflicts = (
++                        not options.ignore_dependencies and
++                        options.warn_about_conflicts
++                    )
++                    if should_warn_about_conflicts:
++                        self._warn_about_conflicts(to_install)
++
++                    # Don't warn about script install locations if
++                    # --target has been specified
++                    warn_script_location = options.warn_script_location
++                    if options.target_dir:
++                        warn_script_location = False
++
++                    installed = install_given_reqs(
++                        to_install,
++                        install_options,
++                        global_options,
++                        root=options.root_path,
++                        home=target_temp_dir.path,
++                        prefix=options.prefix_path,
++                        pycompile=options.compile,
++                        warn_script_location=warn_script_location,
++                        use_user_site=options.use_user_site,
++                    )
++
++                    possible_lib_locations = get_lib_location_guesses(
++                        user=options.use_user_site,
++                        home=target_temp_dir.path,
++                        root=options.root_path,
++                        prefix=options.prefix_path,
++                        isolated=options.isolated_mode,
++                    )
++                    reqs = sorted(installed, key=operator.attrgetter('name'))
++                    items = []
++                    for req in reqs:
++                        item = req.name
++                        try:
++                            installed_version = get_installed_version(
++                                req.name, possible_lib_locations
++                            )
++                            if installed_version:
++                                item += '-' + installed_version
++                        except Exception:
++                            pass
++                        items.append(item)
++                    installed = ' '.join(items)
++                    if installed:
++                        logger.info('Successfully installed %s', installed)
++                except EnvironmentError as error:
++                    show_traceback = (self.verbosity >= 1)
++
++                    message = create_env_error_message(
++                        error, show_traceback, options.use_user_site,
++                    )
++                    logger.error(message, exc_info=show_traceback)
++
++                    return ERROR
++                except PreviousBuildDirError:
++                    options.no_clean = True
++                    raise
++                finally:
++                    # Clean up
++                    if not options.no_clean:
++                        requirement_set.cleanup_files()
++                        wheel_cache.cleanup()
++
++        if options.target_dir:
++            self._handle_target_dir(
++                options.target_dir, target_temp_dir, options.upgrade
++            )
++        return requirement_set
++
++    def _handle_target_dir(self, target_dir, target_temp_dir, upgrade):
++        ensure_dir(target_dir)
++
++        # Checking both purelib and platlib directories for installed
++        # packages to be moved to target directory
++        lib_dir_list = []
++
++        with target_temp_dir:
++            # Checking both purelib and platlib directories for installed
++            # packages to be moved to target directory
++            scheme = distutils_scheme('', home=target_temp_dir.path)
++            purelib_dir = scheme['purelib']
++            platlib_dir = scheme['platlib']
++            data_dir = scheme['data']
++
++            if os.path.exists(purelib_dir):
++                lib_dir_list.append(purelib_dir)
++            if os.path.exists(platlib_dir) and platlib_dir != purelib_dir:
++                lib_dir_list.append(platlib_dir)
++            if os.path.exists(data_dir):
++                lib_dir_list.append(data_dir)
++
++            for lib_dir in lib_dir_list:
++                for item in os.listdir(lib_dir):
++                    if lib_dir == data_dir:
++                        ddir = os.path.join(data_dir, item)
++                        if any(s.startswith(ddir) for s in lib_dir_list[:-1]):
++                            continue
++                    target_item_dir = os.path.join(target_dir, item)
++                    if os.path.exists(target_item_dir):
++                        if not upgrade:
++                            logger.warning(
++                                'Target directory %s already exists. Specify '
++                                '--upgrade to force replacement.',
++                                target_item_dir
++                            )
++                            continue
++                        if os.path.islink(target_item_dir):
++                            logger.warning(
++                                'Target directory %s already exists and is '
++                                'a link. Pip will not automatically replace '
++                                'links, please remove if replacement is '
++                                'desired.',
++                                target_item_dir
++                            )
++                            continue
++                        if os.path.isdir(target_item_dir):
++                            shutil.rmtree(target_item_dir)
++                        else:
++                            os.remove(target_item_dir)
++
++                    shutil.move(
++                        os.path.join(lib_dir, item),
++                        target_item_dir
++                    )
++
++    def _warn_about_conflicts(self, to_install):
++        package_set, _dep_info = check_install_conflicts(to_install)
++        missing, conflicting = _dep_info
++
++        # NOTE: There is some duplication here from pip check
++        for project_name in missing:
++            version = package_set[project_name][0]
++            for dependency in missing[project_name]:
++                logger.critical(
++                    "%s %s requires %s, which is not installed.",
++                    project_name, version, dependency[1],
++                )
++
++        for project_name in conflicting:
++            version = package_set[project_name][0]
++            for dep_name, dep_version, req in conflicting[project_name]:
++                logger.critical(
++                    "%s %s has requirement %s, but you'll have %s %s which is "
++                    "incompatible.",
++                    project_name, version, req, dep_name, dep_version,
++                )
++
++
++def get_lib_location_guesses(*args, **kwargs):
++    scheme = distutils_scheme('', *args, **kwargs)
++    return [scheme['purelib'], scheme['platlib']]
++
++
++def create_env_error_message(error, show_traceback, using_user_site):
++    """Format an error message for an EnvironmentError
++
++    It may occur anytime during the execution of the install command.
++    """
++    parts = []
++
++    # Mention the error if we are not going to show a traceback
++    parts.append("Could not install packages due to an EnvironmentError")
++    if not show_traceback:
++        parts.append(": ")
++        parts.append(str(error))
++    else:
++        parts.append(".")
++
++    # Spilt the error indication from a helper message (if any)
++    parts[-1] += "\n"
++
++    # Suggest useful actions to the user:
++    #  (1) using user site-packages or (2) verifying the permissions
++    if error.errno == errno.EACCES:
++        user_option_part = "Consider using the `--user` option"
++        permissions_part = "Check the permissions"
++
++        if not using_user_site:
++            parts.extend([
++                user_option_part, " or ",
++                permissions_part.lower(),
++            ])
++        else:
++            parts.append(permissions_part)
++        parts.append(".\n")
++
++    return "".join(parts).strip() + "\n"
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/configuration.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/configuration.py	(date 1573549700094)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/configuration.py	(date 1573549700094)
+@@ -0,0 +1,227 @@
++import logging
++import os
++import subprocess
++
++from pip._internal.basecommand import Command
++from pip._internal.configuration import Configuration, kinds
++from pip._internal.exceptions import PipError
++from pip._internal.locations import venv_config_file
++from pip._internal.status_codes import ERROR, SUCCESS
++from pip._internal.utils.misc import get_prog
++
++logger = logging.getLogger(__name__)
++
++
++class ConfigurationCommand(Command):
++    """Manage local and global configuration.
++
++        Subcommands:
++
++        list: List the active configuration (or from the file specified)
++        edit: Edit the configuration file in an editor
++        get: Get the value associated with name
++        set: Set the name=value
++        unset: Unset the value associated with name
++
++        If none of --user, --global and --venv are passed, a virtual
++        environment configuration file is used if one is active and the file
++        exists. Otherwise, all modifications happen on the to the user file by
++        default.
++    """
++
++    name = 'config'
++    usage = """
++        %prog [<file-option>] list
++        %prog [<file-option>] [--editor <editor-path>] edit
++
++        %prog [<file-option>] get name
++        %prog [<file-option>] set name value
++        %prog [<file-option>] unset name
++    """
++
++    summary = "Manage local and global configuration."
++
++    def __init__(self, *args, **kwargs):
++        super(ConfigurationCommand, self).__init__(*args, **kwargs)
++
++        self.configuration = None
++
++        self.cmd_opts.add_option(
++            '--editor',
++            dest='editor',
++            action='store',
++            default=None,
++            help=(
++                'Editor to use to edit the file. Uses VISUAL or EDITOR '
++                'environment variables if not provided.'
++            )
++        )
++
++        self.cmd_opts.add_option(
++            '--global',
++            dest='global_file',
++            action='store_true',
++            default=False,
++            help='Use the system-wide configuration file only'
++        )
++
++        self.cmd_opts.add_option(
++            '--user',
++            dest='user_file',
++            action='store_true',
++            default=False,
++            help='Use the user configuration file only'
++        )
++
++        self.cmd_opts.add_option(
++            '--venv',
++            dest='venv_file',
++            action='store_true',
++            default=False,
++            help='Use the virtualenv configuration file only'
++        )
++
++        self.parser.insert_option_group(0, self.cmd_opts)
++
++    def run(self, options, args):
++        handlers = {
++            "list": self.list_values,
++            "edit": self.open_in_editor,
++            "get": self.get_name,
++            "set": self.set_name_value,
++            "unset": self.unset_name
++        }
++
++        # Determine action
++        if not args or args[0] not in handlers:
++            logger.error("Need an action ({}) to perform.".format(
++                ", ".join(sorted(handlers)))
++            )
++            return ERROR
++
++        action = args[0]
++
++        # Determine which configuration files are to be loaded
++        #    Depends on whether the command is modifying.
++        try:
++            load_only = self._determine_file(
++                options, need_value=(action in ["get", "set", "unset", "edit"])
++            )
++        except PipError as e:
++            logger.error(e.args[0])
++            return ERROR
++
++        # Load a new configuration
++        self.configuration = Configuration(
++            isolated=options.isolated_mode, load_only=load_only
++        )
++        self.configuration.load()
++
++        # Error handling happens here, not in the action-handlers.
++        try:
++            handlers[action](options, args[1:])
++        except PipError as e:
++            logger.error(e.args[0])
++            return ERROR
++
++        return SUCCESS
++
++    def _determine_file(self, options, need_value):
++        file_options = {
++            kinds.USER: options.user_file,
++            kinds.GLOBAL: options.global_file,
++            kinds.VENV: options.venv_file
++        }
++
++        if sum(file_options.values()) == 0:
++            if not need_value:
++                return None
++            # Default to user, unless there's a virtualenv file.
++            elif os.path.exists(venv_config_file):
++                return kinds.VENV
++            else:
++                return kinds.USER
++        elif sum(file_options.values()) == 1:
++            # There's probably a better expression for this.
++            return [key for key in file_options if file_options[key]][0]
++
++        raise PipError(
++            "Need exactly one file to operate upon "
++            "(--user, --venv, --global) to perform."
++        )
++
++    def list_values(self, options, args):
++        self._get_n_args(args, "list", n=0)
++
++        for key, value in sorted(self.configuration.items()):
++            logger.info("%s=%r", key, value)
++
++    def get_name(self, options, args):
++        key = self._get_n_args(args, "get [name]", n=1)
++        value = self.configuration.get_value(key)
++
++        logger.info("%s", value)
++
++    def set_name_value(self, options, args):
++        key, value = self._get_n_args(args, "set [name] [value]", n=2)
++        self.configuration.set_value(key, value)
++
++        self._save_configuration()
++
++    def unset_name(self, options, args):
++        key = self._get_n_args(args, "unset [name]", n=1)
++        self.configuration.unset_value(key)
++
++        self._save_configuration()
++
++    def open_in_editor(self, options, args):
++        editor = self._determine_editor(options)
++
++        fname = self.configuration.get_file_to_edit()
++        if fname is None:
++            raise PipError("Could not determine appropriate file.")
++
++        try:
++            subprocess.check_call([editor, fname])
++        except subprocess.CalledProcessError as e:
++            raise PipError(
++                "Editor Subprocess exited with exit code {}"
++                .format(e.returncode)
++            )
++
++    def _get_n_args(self, args, example, n):
++        """Helper to make sure the command got the right number of arguments
++        """
++        if len(args) != n:
++            msg = (
++                'Got unexpected number of arguments, expected {}. '
++                '(example: "{} config {}")'
++            ).format(n, get_prog(), example)
++            raise PipError(msg)
++
++        if n == 1:
++            return args[0]
++        else:
++            return args
++
++    def _save_configuration(self):
++        # We successfully ran a modifying command. Need to save the
++        # configuration.
++        try:
++            self.configuration.save()
++        except Exception:
++            logger.error(
++                "Unable to save configuration. Please report this as a bug.",
++                exc_info=1
++            )
++            raise PipError("Internal Error.")
++
++    def _determine_editor(self, options):
++        if options.editor is not None:
++            return options.editor
++        elif "VISUAL" in os.environ:
++            return os.environ["VISUAL"]
++        elif "EDITOR" in os.environ:
++            return os.environ["EDITOR"]
++        else:
++            raise PipError("Could not determine editor to use.")
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/hash.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/hash.py	(date 1573549700158)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/hash.py	(date 1573549700158)
+@@ -0,0 +1,57 @@
++from __future__ import absolute_import
++
++import hashlib
++import logging
++import sys
++
++from pip._internal.basecommand import Command
++from pip._internal.status_codes import ERROR
++from pip._internal.utils.hashes import FAVORITE_HASH, STRONG_HASHES
++from pip._internal.utils.misc import read_chunks
++
++logger = logging.getLogger(__name__)
++
++
++class HashCommand(Command):
++    """
++    Compute a hash of a local package archive.
++
++    These can be used with --hash in a requirements file to do repeatable
++    installs.
++
++    """
++    name = 'hash'
++    usage = '%prog [options] <file> ...'
++    summary = 'Compute hashes of package archives.'
++    ignore_require_venv = True
++
++    def __init__(self, *args, **kw):
++        super(HashCommand, self).__init__(*args, **kw)
++        self.cmd_opts.add_option(
++            '-a', '--algorithm',
++            dest='algorithm',
++            choices=STRONG_HASHES,
++            action='store',
++            default=FAVORITE_HASH,
++            help='The hash algorithm to use: one of %s' %
++                 ', '.join(STRONG_HASHES))
++        self.parser.insert_option_group(0, self.cmd_opts)
++
++    def run(self, options, args):
++        if not args:
++            self.parser.print_usage(sys.stderr)
++            return ERROR
++
++        algorithm = options.algorithm
++        for path in args:
++            logger.info('%s:\n--hash=%s:%s',
++                        path, algorithm, _hash_of_file(path, algorithm))
++
++
++def _hash_of_file(path, algorithm):
++    """Return the hash digest of a file."""
++    with open(path, 'rb') as archive:
++        hash = hashlib.new(algorithm)
++        for chunk in read_chunks(archive):
++            hash.update(chunk)
++    return hash.hexdigest()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/freeze.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/freeze.py	(date 1573549700107)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/freeze.py	(date 1573549700107)
+@@ -0,0 +1,96 @@
++from __future__ import absolute_import
++
++import sys
++
++from pip._internal import index
++from pip._internal.basecommand import Command
++from pip._internal.cache import WheelCache
++from pip._internal.compat import stdlib_pkgs
++from pip._internal.operations.freeze import freeze
++
++DEV_PKGS = {'pip', 'setuptools', 'distribute', 'wheel'}
++
++
++class FreezeCommand(Command):
++    """
++    Output installed packages in requirements format.
++
++    packages are listed in a case-insensitive sorted order.
++    """
++    name = 'freeze'
++    usage = """
++      %prog [options]"""
++    summary = 'Output installed packages in requirements format.'
++    log_streams = ("ext://sys.stderr", "ext://sys.stderr")
++
++    def __init__(self, *args, **kw):
++        super(FreezeCommand, self).__init__(*args, **kw)
++
++        self.cmd_opts.add_option(
++            '-r', '--requirement',
++            dest='requirements',
++            action='append',
++            default=[],
++            metavar='file',
++            help="Use the order in the given requirements file and its "
++                 "comments when generating output. This option can be "
++                 "used multiple times.")
++        self.cmd_opts.add_option(
++            '-f', '--find-links',
++            dest='find_links',
++            action='append',
++            default=[],
++            metavar='URL',
++            help='URL for finding packages, which will be added to the '
++                 'output.')
++        self.cmd_opts.add_option(
++            '-l', '--local',
++            dest='local',
++            action='store_true',
++            default=False,
++            help='If in a virtualenv that has global access, do not output '
++                 'globally-installed packages.')
++        self.cmd_opts.add_option(
++            '--user',
++            dest='user',
++            action='store_true',
++            default=False,
++            help='Only output packages installed in user-site.')
++        self.cmd_opts.add_option(
++            '--all',
++            dest='freeze_all',
++            action='store_true',
++            help='Do not skip these packages in the output:'
++                 ' %s' % ', '.join(DEV_PKGS))
++        self.cmd_opts.add_option(
++            '--exclude-editable',
++            dest='exclude_editable',
++            action='store_true',
++            help='Exclude editable package from output.')
++
++        self.parser.insert_option_group(0, self.cmd_opts)
++
++    def run(self, options, args):
++        format_control = index.FormatControl(set(), set())
++        wheel_cache = WheelCache(options.cache_dir, format_control)
++        skip = set(stdlib_pkgs)
++        if not options.freeze_all:
++            skip.update(DEV_PKGS)
++
++        freeze_kwargs = dict(
++            requirement=options.requirements,
++            find_links=options.find_links,
++            local_only=options.local,
++            user_only=options.user,
++            skip_regex=options.skip_requirements_regex,
++            isolated=options.isolated_mode,
++            wheel_cache=wheel_cache,
++            skip=skip,
++            exclude_editable=options.exclude_editable,
++        )
++
++        try:
++            for line in freeze(**freeze_kwargs):
++                sys.stdout.write(line + '\n')
++        finally:
++            wheel_cache.cleanup()
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/__init__.py	(date 1573549700073)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/commands/__init__.py	(date 1573549700073)
+@@ -0,0 +1,79 @@
++"""
++Package containing all pip commands
++"""
++from __future__ import absolute_import
++
++from pip._internal.commands.completion import CompletionCommand
++from pip._internal.commands.configuration import ConfigurationCommand
++from pip._internal.commands.download import DownloadCommand
++from pip._internal.commands.freeze import FreezeCommand
++from pip._internal.commands.hash import HashCommand
++from pip._internal.commands.help import HelpCommand
++from pip._internal.commands.list import ListCommand
++from pip._internal.commands.check import CheckCommand
++from pip._internal.commands.search import SearchCommand
++from pip._internal.commands.show import ShowCommand
++from pip._internal.commands.install import InstallCommand
++from pip._internal.commands.uninstall import UninstallCommand
++from pip._internal.commands.wheel import WheelCommand
++
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++
++if MYPY_CHECK_RUNNING:
++    from typing import List, Type
++    from pip._internal.basecommand import Command
++
++commands_order = [
++    InstallCommand,
++    DownloadCommand,
++    UninstallCommand,
++    FreezeCommand,
++    ListCommand,
++    ShowCommand,
++    CheckCommand,
++    ConfigurationCommand,
++    SearchCommand,
++    WheelCommand,
++    HashCommand,
++    CompletionCommand,
++    HelpCommand,
++]  # type: List[Type[Command]]
++
++commands_dict = {c.name: c for c in commands_order}
++
++
++def get_summaries(ordered=True):
++    """Yields sorted (command name, command summary) tuples."""
++
++    if ordered:
++        cmditems = _sort_commands(commands_dict, commands_order)
++    else:
++        cmditems = commands_dict.items()
++
++    for name, command_class in cmditems:
++        yield (name, command_class.summary)
++
++
++def get_similar_commands(name):
++    """Command name auto-correct."""
++    from difflib import get_close_matches
++
++    name = name.lower()
++
++    close_commands = get_close_matches(name, commands_dict.keys())
++
++    if close_commands:
++        return close_commands[0]
++    else:
++        return False
++
++
++def _sort_commands(cmddict, order):
++    def keyfn(key):
++        try:
++            return order.index(key[1])
++        except ValueError:
++            # unordered items should come last
++            return 0xff
++
++    return sorted(cmddict.items(), key=keyfn)
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/check.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/check.py	(date 1573549700235)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/check.py	(date 1573549700235)
+@@ -0,0 +1,106 @@
++"""Validation of dependencies of packages
++"""
++
++from collections import namedtuple
++
++from pip._vendor.packaging.utils import canonicalize_name
++
++from pip._internal.operations.prepare import make_abstract_dist
++
++from pip._internal.utils.misc import get_installed_distributions
++from pip._internal.utils.typing import MYPY_CHECK_RUNNING
++
++if MYPY_CHECK_RUNNING:
++    from pip._internal.req.req_install import InstallRequirement
++    from typing import Any, Dict, Iterator, Set, Tuple, List
++
++    # Shorthands
++    PackageSet = Dict[str, 'PackageDetails']
++    Missing = Tuple[str, Any]
++    Conflicting = Tuple[str, str, Any]
++
++    MissingDict = Dict[str, List[Missing]]
++    ConflictingDict = Dict[str, List[Conflicting]]
++    CheckResult = Tuple[MissingDict, ConflictingDict]
++
++PackageDetails = namedtuple('PackageDetails', ['version', 'requires'])
++
++
++def create_package_set_from_installed(**kwargs):
++    # type: (**Any) -> PackageSet
++    """Converts a list of distributions into a PackageSet.
++    """
++    # Default to using all packages installed on the system
++    if kwargs == {}:
++        kwargs = {"local_only": False, "skip": ()}
++    retval = {}
++    for dist in get_installed_distributions(**kwargs):
++        name = canonicalize_name(dist.project_name)
++        retval[name] = PackageDetails(dist.version, dist.requires())
++    return retval
++
++
++def check_package_set(package_set):
++    # type: (PackageSet) -> CheckResult
++    """Check if a package set is consistent
++    """
++    missing = dict()
++    conflicting = dict()
++
++    for package_name in package_set:
++        # Info about dependencies of package_name
++        missing_deps = set()  # type: Set[Missing]
++        conflicting_deps = set()  # type: Set[Conflicting]
++
++        for req in package_set[package_name].requires:
++            name = canonicalize_name(req.project_name)  # type: str
++
++            # Check if it's missing
++            if name not in package_set:
++                missed = True
++                if req.marker is not None:
++                    missed = req.marker.evaluate()
++                if missed:
++                    missing_deps.add((name, req))
++                continue
++
++            # Check if there's a conflict
++            version = package_set[name].version  # type: str
++            if not req.specifier.contains(version, prereleases=True):
++                conflicting_deps.add((name, version, req))
++
++        def str_key(x):
++            return str(x)
++
++        if missing_deps:
++            missing[package_name] = sorted(missing_deps, key=str_key)
++        if conflicting_deps:
++            conflicting[package_name] = sorted(conflicting_deps, key=str_key)
++
++    return missing, conflicting
++
++
++def check_install_conflicts(to_install):
++    # type: (List[InstallRequirement]) -> Tuple[PackageSet, CheckResult]
++    """For checking if the dependency graph would be consistent after \
++    installing given requirements
++    """
++    # Start from the current state
++    state = create_package_set_from_installed()
++    _simulate_installation_of(to_install, state)
++    return state, check_package_set(state)
++
++
++# NOTE from @pradyunsg
++# This required a minor update in dependency link handling logic over at
++# operations.prepare.IsSDist.dist() to get it working
++def _simulate_installation_of(to_install, state):
++    # type: (List[InstallRequirement], PackageSet) -> None
++    """Computes the version of packages after installing to_install.
++    """
++
++    # Modify it as installing requirement_set would (assuming no errors)
++    for inst_req in to_install:
++        dist = make_abstract_dist(inst_req).dist(finder=None)
++        name = canonicalize_name(dist.key)
++        state[name] = PackageDetails(dist.version, dist.requires())
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/prepare.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/prepare.py	(date 1573549700249)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/prepare.py	(date 1573549700249)
+@@ -0,0 +1,380 @@
++"""Prepares a distribution for installation
++"""
++
++import itertools
++import logging
++import os
++import sys
++from copy import copy
++
++from pip._vendor import pkg_resources, requests
++
++from pip._internal.build_env import NoOpBuildEnvironment
++from pip._internal.compat import expanduser
++from pip._internal.download import (
++    is_dir_url, is_file_url, is_vcs_url, unpack_url, url_to_path,
++)
++from pip._internal.exceptions import (
++    DirectoryUrlHashUnsupported, HashUnpinned, InstallationError,
++    PreviousBuildDirError, VcsHashUnsupported,
++)
++from pip._internal.index import FormatControl
++from pip._internal.req.req_install import InstallRequirement
++from pip._internal.utils.hashes import MissingHashes
++from pip._internal.utils.logging import indent_log
++from pip._internal.utils.misc import (
++    call_subprocess, display_path, normalize_path,
++)
++from pip._internal.utils.ui import open_spinner
++from pip._internal.vcs import vcs
++
++logger = logging.getLogger(__name__)
++
++
++def make_abstract_dist(req):
++    """Factory to make an abstract dist object.
++
++    Preconditions: Either an editable req with a source_dir, or satisfied_by or
++    a wheel link, or a non-editable req with a source_dir.
++
++    :return: A concrete DistAbstraction.
++    """
++    if req.editable:
++        return IsSDist(req)
++    elif req.link and req.link.is_wheel:
++        return IsWheel(req)
++    else:
++        return IsSDist(req)
++
++
++def _install_build_reqs(finder, prefix, build_requirements):
++    # NOTE: What follows is not a very good thing.
++    #       Eventually, this should move into the BuildEnvironment class and
++    #       that should handle all the isolation and sub-process invocation.
++    finder = copy(finder)
++    finder.format_control = FormatControl(set(), set([":all:"]))
++    urls = [
++        finder.find_requirement(
++            InstallRequirement.from_line(r), upgrade=False).url
++        for r in build_requirements
++    ]
++    args = [
++        sys.executable, '-m', 'pip', 'install', '--ignore-installed',
++        '--no-user', '--prefix', prefix,
++    ] + list(urls)
++
++    with open_spinner("Installing build dependencies") as spinner:
++        call_subprocess(args, show_stdout=False, spinner=spinner)
++
++
++class DistAbstraction(object):
++    """Abstracts out the wheel vs non-wheel Resolver.resolve() logic.
++
++    The requirements for anything installable are as follows:
++     - we must be able to determine the requirement name
++       (or we can't correctly handle the non-upgrade case).
++     - we must be able to generate a list of run-time dependencies
++       without installing any additional packages (or we would
++       have to either burn time by doing temporary isolated installs
++       or alternatively violate pips 'don't start installing unless
++       all requirements are available' rule - neither of which are
++       desirable).
++     - for packages with setup requirements, we must also be able
++       to determine their requirements without installing additional
++       packages (for the same reason as run-time dependencies)
++     - we must be able to create a Distribution object exposing the
++       above metadata.
++    """
++
++    def __init__(self, req):
++        self.req = req
++
++    def dist(self, finder):
++        """Return a setuptools Dist object."""
++        raise NotImplementedError(self.dist)
++
++    def prep_for_dist(self, finder):
++        """Ensure that we can get a Dist for this requirement."""
++        raise NotImplementedError(self.dist)
++
++
++class IsWheel(DistAbstraction):
++
++    def dist(self, finder):
++        return list(pkg_resources.find_distributions(
++            self.req.source_dir))[0]
++
++    def prep_for_dist(self, finder, build_isolation):
++        # FIXME:https://github.com/pypa/pip/issues/1112
++        pass
++
++
++class IsSDist(DistAbstraction):
++
++    def dist(self, finder):
++        dist = self.req.get_dist()
++        # FIXME: shouldn't be globally added.
++        if finder and dist.has_metadata('dependency_links.txt'):
++            finder.add_dependency_links(
++                dist.get_metadata_lines('dependency_links.txt')
++            )
++        return dist
++
++    def prep_for_dist(self, finder, build_isolation):
++        # Before calling "setup.py egg_info", we need to set-up the build
++        # environment.
++        build_requirements, isolate = self.req.get_pep_518_info()
++        should_isolate = build_isolation and isolate
++
++        minimum_requirements = ('setuptools', 'wheel')
++        missing_requirements = set(minimum_requirements) - set(
++            pkg_resources.Requirement(r).key
++            for r in build_requirements
++        )
++        if missing_requirements:
++            def format_reqs(rs):
++                return ' and '.join(map(repr, sorted(rs)))
++            logger.warning(
++                "Missing build time requirements in pyproject.toml for %s: "
++                "%s.", self.req, format_reqs(missing_requirements)
++            )
++            logger.warning(
++                "This version of pip does not implement PEP 517 so it cannot "
++                "build a wheel without %s.", format_reqs(minimum_requirements)
++            )
++
++        if should_isolate:
++            with self.req.build_env:
++                pass
++            _install_build_reqs(finder, self.req.build_env.path,
++                                build_requirements)
++        else:
++            self.req.build_env = NoOpBuildEnvironment(no_clean=False)
++
++        self.req.run_egg_info()
++        self.req.assert_source_matches_version()
++
++
++class Installed(DistAbstraction):
++
++    def dist(self, finder):
++        return self.req.satisfied_by
++
++    def prep_for_dist(self, finder):
++        pass
++
++
++class RequirementPreparer(object):
++    """Prepares a Requirement
++    """
++
++    def __init__(self, build_dir, download_dir, src_dir, wheel_download_dir,
++                 progress_bar, build_isolation):
++        super(RequirementPreparer, self).__init__()
++
++        self.src_dir = src_dir
++        self.build_dir = build_dir
++
++        # Where still packed archives should be written to. If None, they are
++        # not saved, and are deleted immediately after unpacking.
++        self.download_dir = download_dir
++
++        # Where still-packed .whl files should be written to. If None, they are
++        # written to the download_dir parameter. Separate to download_dir to
++        # permit only keeping wheel archives for pip wheel.
++        if wheel_download_dir:
++            wheel_download_dir = normalize_path(wheel_download_dir)
++        self.wheel_download_dir = wheel_download_dir
++
++        # NOTE
++        # download_dir and wheel_download_dir overlap semantically and may
++        # be combined if we're willing to have non-wheel archives present in
++        # the wheelhouse output by 'pip wheel'.
++
++        self.progress_bar = progress_bar
++
++        # Is build isolation allowed?
++        self.build_isolation = build_isolation
++
++    @property
++    def _download_should_save(self):
++        # TODO: Modify to reduce indentation needed
++        if self.download_dir:
++            self.download_dir = expanduser(self.download_dir)
++            if os.path.exists(self.download_dir):
++                return True
++            else:
++                logger.critical('Could not find download directory')
++                raise InstallationError(
++                    "Could not find or access download directory '%s'"
++                    % display_path(self.download_dir))
++        return False
++
++    def prepare_linked_requirement(self, req, session, finder,
++                                   upgrade_allowed, require_hashes):
++        """Prepare a requirement that would be obtained from req.link
++        """
++        # TODO: Breakup into smaller functions
++        if req.link and req.link.scheme == 'file':
++            path = url_to_path(req.link.url)
++            logger.info('Processing %s', display_path(path))
++        else:
++            logger.info('Collecting %s', req)
++
++        with indent_log():
++            # @@ if filesystem packages are not marked
++            # editable in a req, a non deterministic error
++            # occurs when the script attempts to unpack the
++            # build directory
++            req.ensure_has_source_dir(self.build_dir)
++            # If a checkout exists, it's unwise to keep going.  version
++            # inconsistencies are logged later, but do not fail the
++            # installation.
++            # FIXME: this won't upgrade when there's an existing
++            # package unpacked in `req.source_dir`
++            # package unpacked in `req.source_dir`
++            if os.path.exists(os.path.join(req.source_dir, 'setup.py')):
++                raise PreviousBuildDirError(
++                    "pip can't proceed with requirements '%s' due to a"
++                    " pre-existing build directory (%s). This is "
++                    "likely due to a previous installation that failed"
++                    ". pip is being responsible and not assuming it "
++                    "can delete this. Please delete it and try again."
++                    % (req, req.source_dir)
++                )
++            req.populate_link(finder, upgrade_allowed, require_hashes)
++
++            # We can't hit this spot and have populate_link return None.
++            # req.satisfied_by is None here (because we're
++            # guarded) and upgrade has no impact except when satisfied_by
++            # is not None.
++            # Then inside find_requirement existing_applicable -> False
++            # If no new versions are found, DistributionNotFound is raised,
++            # otherwise a result is guaranteed.
++            assert req.link
++            link = req.link
++
++            # Now that we have the real link, we can tell what kind of
++            # requirements we have and raise some more informative errors
++            # than otherwise. (For example, we can raise VcsHashUnsupported
++            # for a VCS URL rather than HashMissing.)
++            if require_hashes:
++                # We could check these first 2 conditions inside
++                # unpack_url and save repetition of conditions, but then
++                # we would report less-useful error messages for
++                # unhashable requirements, complaining that there's no
++                # hash provided.
++                if is_vcs_url(link):
++                    raise VcsHashUnsupported()
++                elif is_file_url(link) and is_dir_url(link):
++                    raise DirectoryUrlHashUnsupported()
++                if not req.original_link and not req.is_pinned:
++                    # Unpinned packages are asking for trouble when a new
++                    # version is uploaded. This isn't a security check, but
++                    # it saves users a surprising hash mismatch in the
++                    # future.
++                    #
++                    # file:/// URLs aren't pinnable, so don't complain
++                    # about them not being pinned.
++                    raise HashUnpinned()
++
++            hashes = req.hashes(trust_internet=not require_hashes)
++            if require_hashes and not hashes:
++                # Known-good hashes are missing for this requirement, so
++                # shim it with a facade object that will provoke hash
++                # computation and then raise a HashMissing exception
++                # showing the user what the hash should be.
++                hashes = MissingHashes()
++
++            try:
++                download_dir = self.download_dir
++                # We always delete unpacked sdists after pip ran.
++                autodelete_unpacked = True
++                if req.link.is_wheel and self.wheel_download_dir:
++                    # when doing 'pip wheel` we download wheels to a
++                    # dedicated dir.
++                    download_dir = self.wheel_download_dir
++                if req.link.is_wheel:
++                    if download_dir:
++                        # When downloading, we only unpack wheels to get
++                        # metadata.
++                        autodelete_unpacked = True
++                    else:
++                        # When installing a wheel, we use the unpacked
++                        # wheel.
++                        autodelete_unpacked = False
++                unpack_url(
++                    req.link, req.source_dir,
++                    download_dir, autodelete_unpacked,
++                    session=session, hashes=hashes,
++                    progress_bar=self.progress_bar
++                )
++            except requests.HTTPError as exc:
++                logger.critical(
++                    'Could not install requirement %s because of error %s',
++                    req,
++                    exc,
++                )
++                raise InstallationError(
++                    'Could not install requirement %s because of HTTP '
++                    'error %s for URL %s' %
++                    (req, exc, req.link)
++                )
++            abstract_dist = make_abstract_dist(req)
++            abstract_dist.prep_for_dist(finder, self.build_isolation)
++            if self._download_should_save:
++                # Make a .zip of the source_dir we already created.
++                if req.link.scheme in vcs.all_schemes:
++                    req.archive(self.download_dir)
++        return abstract_dist
++
++    def prepare_editable_requirement(self, req, require_hashes, use_user_site,
++                                     finder):
++        """Prepare an editable requirement
++        """
++        assert req.editable, "cannot prepare a non-editable req as editable"
++
++        logger.info('Obtaining %s', req)
++
++        with indent_log():
++            if require_hashes:
++                raise InstallationError(
++                    'The editable requirement %s cannot be installed when '
++                    'requiring hashes, because there is no single file to '
++                    'hash.' % req
++                )
++            req.ensure_has_source_dir(self.src_dir)
++            req.update_editable(not self._download_should_save)
++
++            abstract_dist = make_abstract_dist(req)
++            abstract_dist.prep_for_dist(finder, self.build_isolation)
++
++            if self._download_should_save:
++                req.archive(self.download_dir)
++            req.check_if_exists(use_user_site)
++
++        return abstract_dist
++
++    def prepare_installed_requirement(self, req, require_hashes, skip_reason):
++        """Prepare an already-installed requirement
++        """
++        assert req.satisfied_by, "req should have been satisfied but isn't"
++        assert skip_reason is not None, (
++            "did not get skip reason skipped but req.satisfied_by "
++            "is set to %r" % (req.satisfied_by,)
++        )
++        logger.info(
++            'Requirement %s: %s (%s)',
++            skip_reason, req, req.satisfied_by.version
++        )
++        with indent_log():
++            if require_hashes:
++                logger.debug(
++                    'Since it is already installed, we are trusting this '
++                    'package without checking its hash. To ensure a '
++                    'completely repeatable environment, install into an '
++                    'empty virtualenv.'
++                )
++            abstract_dist = Installed(req)
++
++        return abstract_dist
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/freeze.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/freeze.py	(date 1573549700240)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/freeze.py	(date 1573549700240)
+@@ -0,0 +1,252 @@
++from __future__ import absolute_import
++
++import collections
++import logging
++import os
++import re
++import warnings
++
++from pip._vendor import pkg_resources, six
++from pip._vendor.packaging.utils import canonicalize_name
++from pip._vendor.pkg_resources import RequirementParseError
++
++from pip._internal.exceptions import InstallationError
++from pip._internal.req import InstallRequirement
++from pip._internal.req.req_file import COMMENT_RE
++from pip._internal.utils.deprecation import RemovedInPip11Warning
++from pip._internal.utils.misc import (
++    dist_is_editable, get_installed_distributions,
++)
++
++logger = logging.getLogger(__name__)
++
++
++def freeze(
++        requirement=None,
++        find_links=None, local_only=None, user_only=None, skip_regex=None,
++        isolated=False,
++        wheel_cache=None,
++        exclude_editable=False,
++        skip=()):
++    find_links = find_links or []
++    skip_match = None
++
++    if skip_regex:
++        skip_match = re.compile(skip_regex).search
++
++    dependency_links = []
++
++    for dist in pkg_resources.working_set:
++        if dist.has_metadata('dependency_links.txt'):
++            dependency_links.extend(
++                dist.get_metadata_lines('dependency_links.txt')
++            )
++    for link in find_links:
++        if '#egg=' in link:
++            dependency_links.append(link)
++    for link in find_links:
++        yield '-f %s' % link
++    installations = {}
++    for dist in get_installed_distributions(local_only=local_only,
++                                            skip=(),
++                                            user_only=user_only):
++        try:
++            req = FrozenRequirement.from_dist(
++                dist,
++                dependency_links
++            )
++        except RequirementParseError:
++            logger.warning(
++                "Could not parse requirement: %s",
++                dist.project_name
++            )
++            continue
++        if exclude_editable and req.editable:
++            continue
++        installations[req.name] = req
++
++    if requirement:
++        # the options that don't get turned into an InstallRequirement
++        # should only be emitted once, even if the same option is in multiple
++        # requirements files, so we need to keep track of what has been emitted
++        # so that we don't emit it again if it's seen again
++        emitted_options = set()
++        # keep track of which files a requirement is in so that we can
++        # give an accurate warning if a requirement appears multiple times.
++        req_files = collections.defaultdict(list)
++        for req_file_path in requirement:
++            with open(req_file_path) as req_file:
++                for line in req_file:
++                    if (not line.strip() or
++                            line.strip().startswith('#') or
++                            (skip_match and skip_match(line)) or
++                            line.startswith((
++                                '-r', '--requirement',
++                                '-Z', '--always-unzip',
++                                '-f', '--find-links',
++                                '-i', '--index-url',
++                                '--pre',
++                                '--trusted-host',
++                                '--process-dependency-links',
++                                '--extra-index-url'))):
++                        line = line.rstrip()
++                        if line not in emitted_options:
++                            emitted_options.add(line)
++                            yield line
++                        continue
++
++                    if line.startswith('-e') or line.startswith('--editable'):
++                        if line.startswith('-e'):
++                            line = line[2:].strip()
++                        else:
++                            line = line[len('--editable'):].strip().lstrip('=')
++                        line_req = InstallRequirement.from_editable(
++                            line,
++                            isolated=isolated,
++                            wheel_cache=wheel_cache,
++                        )
++                    else:
++                        line_req = InstallRequirement.from_line(
++                            COMMENT_RE.sub('', line).strip(),
++                            isolated=isolated,
++                            wheel_cache=wheel_cache,
++                        )
++
++                    if not line_req.name:
++                        logger.info(
++                            "Skipping line in requirement file [%s] because "
++                            "it's not clear what it would install: %s",
++                            req_file_path, line.strip(),
++                        )
++                        logger.info(
++                            "  (add #egg=PackageName to the URL to avoid"
++                            " this warning)"
++                        )
++                    elif line_req.name not in installations:
++                        # either it's not installed, or it is installed
++                        # but has been processed already
++                        if not req_files[line_req.name]:
++                            logger.warning(
++                                "Requirement file [%s] contains %s, but that "
++                                "package is not installed",
++                                req_file_path,
++                                COMMENT_RE.sub('', line).strip(),
++                            )
++                        else:
++                            req_files[line_req.name].append(req_file_path)
++                    else:
++                        yield str(installations[line_req.name]).rstrip()
++                        del installations[line_req.name]
++                        req_files[line_req.name].append(req_file_path)
++
++        # Warn about requirements that were included multiple times (in a
++        # single requirements file or in different requirements files).
++        for name, files in six.iteritems(req_files):
++            if len(files) > 1:
++                logger.warning("Requirement %s included multiple times [%s]",
++                               name, ', '.join(sorted(set(files))))
++
++        yield(
++            '## The following requirements were added by '
++            'pip freeze:'
++        )
++    for installation in sorted(
++            installations.values(), key=lambda x: x.name.lower()):
++        if canonicalize_name(installation.name) not in skip:
++            yield str(installation).rstrip()
++
++
++class FrozenRequirement(object):
++    def __init__(self, name, req, editable, comments=()):
++        self.name = name
++        self.req = req
++        self.editable = editable
++        self.comments = comments
++
++    _rev_re = re.compile(r'-r(\d+)$')
++    _date_re = re.compile(r'-(20\d\d\d\d\d\d)$')
++
++    @classmethod
++    def from_dist(cls, dist, dependency_links):
++        location = os.path.normcase(os.path.abspath(dist.location))
++        comments = []
++        from pip._internal.vcs import vcs, get_src_requirement
++        if dist_is_editable(dist) and vcs.get_backend_name(location):
++            editable = True
++            try:
++                req = get_src_requirement(dist, location)
++            except InstallationError as exc:
++                logger.warning(
++                    "Error when trying to get requirement for VCS system %s, "
++                    "falling back to uneditable format", exc
++                )
++                req = None
++            if req is None:
++                logger.warning(
++                    'Could not determine repository location of %s', location
++                )
++                comments.append(
++                    '## !! Could not determine repository location'
++                )
++                req = dist.as_requirement()
++                editable = False
++        else:
++            editable = False
++            req = dist.as_requirement()
++            specs = req.specs
++            assert len(specs) == 1 and specs[0][0] in ["==", "==="], \
++                'Expected 1 spec with == or ===; specs = %r; dist = %r' % \
++                (specs, dist)
++            version = specs[0][1]
++            ver_match = cls._rev_re.search(version)
++            date_match = cls._date_re.search(version)
++            if ver_match or date_match:
++                svn_backend = vcs.get_backend('svn')
++                if svn_backend:
++                    svn_location = svn_backend().get_location(
++                        dist,
++                        dependency_links,
++                    )
++                if not svn_location:
++                    logger.warning(
++                        'Warning: cannot find svn location for %s', req,
++                    )
++                    comments.append(
++                        '## FIXME: could not find svn URL in dependency_links '
++                        'for this package:'
++                    )
++                else:
++                    warnings.warn(
++                        "SVN editable detection based on dependency links "
++                        "will be dropped in the future.",
++                        RemovedInPip11Warning,
++                    )
++                    comments.append(
++                        '# Installing as editable to satisfy requirement %s:' %
++                        req
++                    )
++                    if ver_match:
++                        rev = ver_match.group(1)
++                    else:
++                        rev = '{%s}' % date_match.group(1)
++                    editable = True
++                    req = '%s@%s#egg=%s' % (
++                        svn_location,
++                        rev,
++                        cls.egg_name(dist)
++                    )
++        return cls(dist.project_name, req, editable, comments)
++
++    @staticmethod
++    def egg_name(dist):
++        name = dist.egg_name()
++        match = re.search(r'-py\d\.\d$', name)
++        if match:
++            name = name[:match.start()]
++        return name
++
++    def __str__(self):
++        req = self.req
++        if self.editable:
++            req = '-e %s' % req
++        return '\n'.join(list(self.comments) + [str(req)]) + '\n'
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/__init__.py	(date 1573549700229)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/pip/_internal/operations/__init__.py	(date 1573549700229)
+@@ -0,0 +1,0 @@
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/entry_points.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/entry_points.txt	(date 1573549699865)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/entry_points.txt	(date 1573549699865)
+@@ -0,0 +1,5 @@
++[console_scripts]
++pip = pip._internal:main
++pip3 = pip._internal:main
++pip3.7 = pip._internal:main
++
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/top_level.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/top_level.txt	(date 1573549699885)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/top_level.txt	(date 1573549699885)
+@@ -0,0 +1,1 @@
++pip
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/dependency_links.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/dependency_links.txt	(date 1573549699858)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/dependency_links.txt	(date 1573549699858)
+@@ -0,0 +1,1 @@
++
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/not-zip-safe
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/not-zip-safe	(date 1573549699871)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/not-zip-safe	(date 1573549699871)
+@@ -0,0 +1,1 @@
++
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/PKG-INFO
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/PKG-INFO	(date 1573549699845)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/PKG-INFO	(date 1573549699845)
+@@ -0,0 +1,69 @@
++Metadata-Version: 2.1
++Name: pip
++Version: 10.0.1
++Summary: The PyPA recommended tool for installing Python packages.
++Home-page: https://pip.pypa.io/
++Author: The pip developers
++Author-email: python-virtualenv@groups.google.com
++License: MIT
++Description: pip
++        ===
++        
++        The `PyPA recommended`_ tool for installing Python packages.
++        
++        .. image:: https://img.shields.io/pypi/v/pip.svg
++           :target: https://pypi.org/project/pip/
++        
++        .. image:: https://img.shields.io/travis/pypa/pip/master.svg
++           :target: http://travis-ci.org/pypa/pip
++        
++        .. image:: https://img.shields.io/appveyor/ci/pypa/pip.svg
++           :target: https://ci.appveyor.com/project/pypa/pip/history
++        
++        .. image:: https://readthedocs.org/projects/pip/badge/?version=latest
++           :target: https://pip.pypa.io/en/latest
++        
++        * `Installation`_
++        * `Documentation`_
++        * `Changelog`_
++        * `GitHub Page`_
++        * `Issue Tracking`_
++        * `User mailing list`_
++        * `Dev mailing list`_
++        * User IRC: #pypa on Freenode.
++        * Dev IRC: #pypa-dev on Freenode.
++        
++        Code of Conduct
++        ---------------
++        
++        Everyone interacting in the pip project's codebases, issue trackers, chat
++        rooms and mailing lists is expected to follow the `PyPA Code of Conduct`_.
++        
++        .. _PyPA recommended: https://packaging.python.org/en/latest/current/
++        .. _Installation: https://pip.pypa.io/en/stable/installing.html
++        .. _Documentation: https://pip.pypa.io/en/stable/
++        .. _Changelog: https://pip.pypa.io/en/stable/news.html
++        .. _GitHub Page: https://github.com/pypa/pip
++        .. _Issue Tracking: https://github.com/pypa/pip/issues
++        .. _User mailing list: http://groups.google.com/group/python-virtualenv
++        .. _Dev mailing list: http://groups.google.com/group/pypa-dev
++        .. _PyPA Code of Conduct: https://www.pypa.io/en/latest/code-of-conduct/
++        
++Keywords: easy_install distutils setuptools egg virtualenv
++Platform: UNKNOWN
++Classifier: Development Status :: 5 - Production/Stable
++Classifier: Intended Audience :: Developers
++Classifier: License :: OSI Approved :: MIT License
++Classifier: Topic :: Software Development :: Build Tools
++Classifier: Programming Language :: Python
++Classifier: Programming Language :: Python :: 2
++Classifier: Programming Language :: Python :: 2.7
++Classifier: Programming Language :: Python :: 3
++Classifier: Programming Language :: Python :: 3.3
++Classifier: Programming Language :: Python :: 3.4
++Classifier: Programming Language :: Python :: 3.5
++Classifier: Programming Language :: Python :: 3.6
++Classifier: Programming Language :: Python :: Implementation :: CPython
++Classifier: Programming Language :: Python :: Implementation :: PyPy
++Requires-Python: >=2.7,!=3.0.*,!=3.1.*,!=3.2.*
++Provides-Extra: testing
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/SOURCES.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/SOURCES.txt	(date 1573549699852)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/SOURCES.txt	(date 1573549699852)
+@@ -0,0 +1,347 @@
++AUTHORS.txt
++LICENSE.txt
++MANIFEST.in
++NEWS.rst
++README.rst
++pyproject.toml
++setup.cfg
++setup.py
++docs/Makefile
++docs/__init__.py
++docs/conf.py
++docs/configuration.rst
++docs/cookbook.rst
++docs/development.rst
++docs/docutils.conf
++docs/index.rst
++docs/installing.rst
++docs/logic.rst
++docs/make.bat
++docs/news.rst
++docs/pipext.py
++docs/quickstart.rst
++docs/usage.rst
++docs/user_guide.rst
++docs/man/pip.rst
++docs/man/commands/check.rst
++docs/man/commands/config.rst
++docs/man/commands/download.rst
++docs/man/commands/freeze.rst
++docs/man/commands/hash.rst
++docs/man/commands/help.rst
++docs/man/commands/install.rst
++docs/man/commands/list.rst
++docs/man/commands/search.rst
++docs/man/commands/show.rst
++docs/man/commands/uninstall.rst
++docs/man/commands/wheel.rst
++docs/reference/index.rst
++docs/reference/pip.rst
++docs/reference/pip_check.rst
++docs/reference/pip_config.rst
++docs/reference/pip_download.rst
++docs/reference/pip_freeze.rst
++docs/reference/pip_hash.rst
++docs/reference/pip_install.rst
++docs/reference/pip_list.rst
++docs/reference/pip_search.rst
++docs/reference/pip_show.rst
++docs/reference/pip_uninstall.rst
++docs/reference/pip_wheel.rst
++src/pip/__init__.py
++src/pip/__main__.py
++src/pip.egg-info/PKG-INFO
++src/pip.egg-info/SOURCES.txt
++src/pip.egg-info/dependency_links.txt
++src/pip.egg-info/entry_points.txt
++src/pip.egg-info/not-zip-safe
++src/pip.egg-info/requires.txt
++src/pip.egg-info/top_level.txt
++src/pip/_internal/__init__.py
++src/pip/_internal/basecommand.py
++src/pip/_internal/baseparser.py
++src/pip/_internal/build_env.py
++src/pip/_internal/cache.py
++src/pip/_internal/cmdoptions.py
++src/pip/_internal/compat.py
++src/pip/_internal/configuration.py
++src/pip/_internal/download.py
++src/pip/_internal/exceptions.py
++src/pip/_internal/index.py
++src/pip/_internal/locations.py
++src/pip/_internal/pep425tags.py
++src/pip/_internal/resolve.py
++src/pip/_internal/status_codes.py
++src/pip/_internal/wheel.py
++src/pip/_internal/commands/__init__.py
++src/pip/_internal/commands/check.py
++src/pip/_internal/commands/completion.py
++src/pip/_internal/commands/configuration.py
++src/pip/_internal/commands/download.py
++src/pip/_internal/commands/freeze.py
++src/pip/_internal/commands/hash.py
++src/pip/_internal/commands/help.py
++src/pip/_internal/commands/install.py
++src/pip/_internal/commands/list.py
++src/pip/_internal/commands/search.py
++src/pip/_internal/commands/show.py
++src/pip/_internal/commands/uninstall.py
++src/pip/_internal/commands/wheel.py
++src/pip/_internal/models/__init__.py
++src/pip/_internal/models/index.py
++src/pip/_internal/operations/__init__.py
++src/pip/_internal/operations/check.py
++src/pip/_internal/operations/freeze.py
++src/pip/_internal/operations/prepare.py
++src/pip/_internal/req/__init__.py
++src/pip/_internal/req/req_file.py
++src/pip/_internal/req/req_install.py
++src/pip/_internal/req/req_set.py
++src/pip/_internal/req/req_uninstall.py
++src/pip/_internal/utils/__init__.py
++src/pip/_internal/utils/appdirs.py
++src/pip/_internal/utils/deprecation.py
++src/pip/_internal/utils/encoding.py
++src/pip/_internal/utils/filesystem.py
++src/pip/_internal/utils/glibc.py
++src/pip/_internal/utils/hashes.py
++src/pip/_internal/utils/logging.py
++src/pip/_internal/utils/misc.py
++src/pip/_internal/utils/outdated.py
++src/pip/_internal/utils/packaging.py
++src/pip/_internal/utils/setuptools_build.py
++src/pip/_internal/utils/temp_dir.py
++src/pip/_internal/utils/typing.py
++src/pip/_internal/utils/ui.py
++src/pip/_internal/vcs/__init__.py
++src/pip/_internal/vcs/bazaar.py
++src/pip/_internal/vcs/git.py
++src/pip/_internal/vcs/mercurial.py
++src/pip/_internal/vcs/subversion.py
++src/pip/_vendor/README.rst
++src/pip/_vendor/__init__.py
++src/pip/_vendor/appdirs.py
++src/pip/_vendor/distro.py
++src/pip/_vendor/ipaddress.py
++src/pip/_vendor/pyparsing.py
++src/pip/_vendor/retrying.py
++src/pip/_vendor/six.py
++src/pip/_vendor/vendor.txt
++src/pip/_vendor/cachecontrol/__init__.py
++src/pip/_vendor/cachecontrol/_cmd.py
++src/pip/_vendor/cachecontrol/adapter.py
++src/pip/_vendor/cachecontrol/cache.py
++src/pip/_vendor/cachecontrol/compat.py
++src/pip/_vendor/cachecontrol/controller.py
++src/pip/_vendor/cachecontrol/filewrapper.py
++src/pip/_vendor/cachecontrol/heuristics.py
++src/pip/_vendor/cachecontrol/serialize.py
++src/pip/_vendor/cachecontrol/wrapper.py
++src/pip/_vendor/cachecontrol/caches/__init__.py
++src/pip/_vendor/cachecontrol/caches/file_cache.py
++src/pip/_vendor/cachecontrol/caches/redis_cache.py
++src/pip/_vendor/certifi/__init__.py
++src/pip/_vendor/certifi/__main__.py
++src/pip/_vendor/certifi/cacert.pem
++src/pip/_vendor/certifi/core.py
++src/pip/_vendor/chardet/__init__.py
++src/pip/_vendor/chardet/big5freq.py
++src/pip/_vendor/chardet/big5prober.py
++src/pip/_vendor/chardet/chardistribution.py
++src/pip/_vendor/chardet/charsetgroupprober.py
++src/pip/_vendor/chardet/charsetprober.py
++src/pip/_vendor/chardet/codingstatemachine.py
++src/pip/_vendor/chardet/compat.py
++src/pip/_vendor/chardet/cp949prober.py
++src/pip/_vendor/chardet/enums.py
++src/pip/_vendor/chardet/escprober.py
++src/pip/_vendor/chardet/escsm.py
++src/pip/_vendor/chardet/eucjpprober.py
++src/pip/_vendor/chardet/euckrfreq.py
++src/pip/_vendor/chardet/euckrprober.py
++src/pip/_vendor/chardet/euctwfreq.py
++src/pip/_vendor/chardet/euctwprober.py
++src/pip/_vendor/chardet/gb2312freq.py
++src/pip/_vendor/chardet/gb2312prober.py
++src/pip/_vendor/chardet/hebrewprober.py
++src/pip/_vendor/chardet/jisfreq.py
++src/pip/_vendor/chardet/jpcntx.py
++src/pip/_vendor/chardet/langbulgarianmodel.py
++src/pip/_vendor/chardet/langcyrillicmodel.py
++src/pip/_vendor/chardet/langgreekmodel.py
++src/pip/_vendor/chardet/langhebrewmodel.py
++src/pip/_vendor/chardet/langhungarianmodel.py
++src/pip/_vendor/chardet/langthaimodel.py
++src/pip/_vendor/chardet/langturkishmodel.py
++src/pip/_vendor/chardet/latin1prober.py
++src/pip/_vendor/chardet/mbcharsetprober.py
++src/pip/_vendor/chardet/mbcsgroupprober.py
++src/pip/_vendor/chardet/mbcssm.py
++src/pip/_vendor/chardet/sbcharsetprober.py
++src/pip/_vendor/chardet/sbcsgroupprober.py
++src/pip/_vendor/chardet/sjisprober.py
++src/pip/_vendor/chardet/universaldetector.py
++src/pip/_vendor/chardet/utf8prober.py
++src/pip/_vendor/chardet/version.py
++src/pip/_vendor/chardet/cli/__init__.py
++src/pip/_vendor/chardet/cli/chardetect.py
++src/pip/_vendor/colorama/__init__.py
++src/pip/_vendor/colorama/ansi.py
++src/pip/_vendor/colorama/ansitowin32.py
++src/pip/_vendor/colorama/initialise.py
++src/pip/_vendor/colorama/win32.py
++src/pip/_vendor/colorama/winterm.py
++src/pip/_vendor/distlib/__init__.py
++src/pip/_vendor/distlib/compat.py
++src/pip/_vendor/distlib/database.py
++src/pip/_vendor/distlib/index.py
++src/pip/_vendor/distlib/locators.py
++src/pip/_vendor/distlib/manifest.py
++src/pip/_vendor/distlib/markers.py
++src/pip/_vendor/distlib/metadata.py
++src/pip/_vendor/distlib/resources.py
++src/pip/_vendor/distlib/scripts.py
++src/pip/_vendor/distlib/t32.exe
++src/pip/_vendor/distlib/t64.exe
++src/pip/_vendor/distlib/util.py
++src/pip/_vendor/distlib/version.py
++src/pip/_vendor/distlib/w32.exe
++src/pip/_vendor/distlib/w64.exe
++src/pip/_vendor/distlib/wheel.py
++src/pip/_vendor/distlib/_backport/__init__.py
++src/pip/_vendor/distlib/_backport/misc.py
++src/pip/_vendor/distlib/_backport/shutil.py
++src/pip/_vendor/distlib/_backport/sysconfig.cfg
++src/pip/_vendor/distlib/_backport/sysconfig.py
++src/pip/_vendor/distlib/_backport/tarfile.py
++src/pip/_vendor/html5lib/__init__.py
++src/pip/_vendor/html5lib/_ihatexml.py
++src/pip/_vendor/html5lib/_inputstream.py
++src/pip/_vendor/html5lib/_tokenizer.py
++src/pip/_vendor/html5lib/_utils.py
++src/pip/_vendor/html5lib/constants.py
++src/pip/_vendor/html5lib/html5parser.py
++src/pip/_vendor/html5lib/serializer.py
++src/pip/_vendor/html5lib/_trie/__init__.py
++src/pip/_vendor/html5lib/_trie/_base.py
++src/pip/_vendor/html5lib/_trie/datrie.py
++src/pip/_vendor/html5lib/_trie/py.py
++src/pip/_vendor/html5lib/filters/__init__.py
++src/pip/_vendor/html5lib/filters/alphabeticalattributes.py
++src/pip/_vendor/html5lib/filters/base.py
++src/pip/_vendor/html5lib/filters/inject_meta_charset.py
++src/pip/_vendor/html5lib/filters/lint.py
++src/pip/_vendor/html5lib/filters/optionaltags.py
++src/pip/_vendor/html5lib/filters/sanitizer.py
++src/pip/_vendor/html5lib/filters/whitespace.py
++src/pip/_vendor/html5lib/treeadapters/__init__.py
++src/pip/_vendor/html5lib/treeadapters/genshi.py
++src/pip/_vendor/html5lib/treeadapters/sax.py
++src/pip/_vendor/html5lib/treebuilders/__init__.py
++src/pip/_vendor/html5lib/treebuilders/base.py
++src/pip/_vendor/html5lib/treebuilders/dom.py
++src/pip/_vendor/html5lib/treebuilders/etree.py
++src/pip/_vendor/html5lib/treebuilders/etree_lxml.py
++src/pip/_vendor/html5lib/treewalkers/__init__.py
++src/pip/_vendor/html5lib/treewalkers/base.py
++src/pip/_vendor/html5lib/treewalkers/dom.py
++src/pip/_vendor/html5lib/treewalkers/etree.py
++src/pip/_vendor/html5lib/treewalkers/etree_lxml.py
++src/pip/_vendor/html5lib/treewalkers/genshi.py
++src/pip/_vendor/idna/__init__.py
++src/pip/_vendor/idna/codec.py
++src/pip/_vendor/idna/compat.py
++src/pip/_vendor/idna/core.py
++src/pip/_vendor/idna/idnadata.py
++src/pip/_vendor/idna/intranges.py
++src/pip/_vendor/idna/package_data.py
++src/pip/_vendor/idna/uts46data.py
++src/pip/_vendor/lockfile/__init__.py
++src/pip/_vendor/lockfile/linklockfile.py
++src/pip/_vendor/lockfile/mkdirlockfile.py
++src/pip/_vendor/lockfile/pidlockfile.py
++src/pip/_vendor/lockfile/sqlitelockfile.py
++src/pip/_vendor/lockfile/symlinklockfile.py
++src/pip/_vendor/msgpack/__init__.py
++src/pip/_vendor/msgpack/_version.py
++src/pip/_vendor/msgpack/exceptions.py
++src/pip/_vendor/msgpack/fallback.py
++src/pip/_vendor/packaging/__about__.py
++src/pip/_vendor/packaging/__init__.py
++src/pip/_vendor/packaging/_compat.py
++src/pip/_vendor/packaging/_structures.py
++src/pip/_vendor/packaging/markers.py
++src/pip/_vendor/packaging/requirements.py
++src/pip/_vendor/packaging/specifiers.py
++src/pip/_vendor/packaging/utils.py
++src/pip/_vendor/packaging/version.py
++src/pip/_vendor/pkg_resources/__init__.py
++src/pip/_vendor/pkg_resources/py31compat.py
++src/pip/_vendor/progress/__init__.py
++src/pip/_vendor/progress/bar.py
++src/pip/_vendor/progress/counter.py
++src/pip/_vendor/progress/helpers.py
++src/pip/_vendor/progress/spinner.py
++src/pip/_vendor/pytoml/__init__.py
++src/pip/_vendor/pytoml/core.py
++src/pip/_vendor/pytoml/parser.py
++src/pip/_vendor/pytoml/writer.py
++src/pip/_vendor/requests/__init__.py
++src/pip/_vendor/requests/__version__.py
++src/pip/_vendor/requests/_internal_utils.py
++src/pip/_vendor/requests/adapters.py
++src/pip/_vendor/requests/api.py
++src/pip/_vendor/requests/auth.py
++src/pip/_vendor/requests/certs.py
++src/pip/_vendor/requests/compat.py
++src/pip/_vendor/requests/cookies.py
++src/pip/_vendor/requests/exceptions.py
++src/pip/_vendor/requests/help.py
++src/pip/_vendor/requests/hooks.py
++src/pip/_vendor/requests/models.py
++src/pip/_vendor/requests/packages.py
++src/pip/_vendor/requests/sessions.py
++src/pip/_vendor/requests/status_codes.py
++src/pip/_vendor/requests/structures.py
++src/pip/_vendor/requests/utils.py
++src/pip/_vendor/urllib3/__init__.py
++src/pip/_vendor/urllib3/_collections.py
++src/pip/_vendor/urllib3/connection.py
++src/pip/_vendor/urllib3/connectionpool.py
++src/pip/_vendor/urllib3/exceptions.py
++src/pip/_vendor/urllib3/fields.py
++src/pip/_vendor/urllib3/filepost.py
++src/pip/_vendor/urllib3/poolmanager.py
++src/pip/_vendor/urllib3/request.py
++src/pip/_vendor/urllib3/response.py
++src/pip/_vendor/urllib3/contrib/__init__.py
++src/pip/_vendor/urllib3/contrib/appengine.py
++src/pip/_vendor/urllib3/contrib/ntlmpool.py
++src/pip/_vendor/urllib3/contrib/pyopenssl.py
++src/pip/_vendor/urllib3/contrib/securetransport.py
++src/pip/_vendor/urllib3/contrib/socks.py
++src/pip/_vendor/urllib3/contrib/_securetransport/__init__.py
++src/pip/_vendor/urllib3/contrib/_securetransport/bindings.py
++src/pip/_vendor/urllib3/contrib/_securetransport/low_level.py
++src/pip/_vendor/urllib3/packages/__init__.py
++src/pip/_vendor/urllib3/packages/ordered_dict.py
++src/pip/_vendor/urllib3/packages/six.py
++src/pip/_vendor/urllib3/packages/backports/__init__.py
++src/pip/_vendor/urllib3/packages/backports/makefile.py
++src/pip/_vendor/urllib3/packages/ssl_match_hostname/__init__.py
++src/pip/_vendor/urllib3/packages/ssl_match_hostname/_implementation.py
++src/pip/_vendor/urllib3/util/__init__.py
++src/pip/_vendor/urllib3/util/connection.py
++src/pip/_vendor/urllib3/util/request.py
++src/pip/_vendor/urllib3/util/response.py
++src/pip/_vendor/urllib3/util/retry.py
++src/pip/_vendor/urllib3/util/selectors.py
++src/pip/_vendor/urllib3/util/ssl_.py
++src/pip/_vendor/urllib3/util/timeout.py
++src/pip/_vendor/urllib3/util/url.py
++src/pip/_vendor/urllib3/util/wait.py
++src/pip/_vendor/webencodings/__init__.py
++src/pip/_vendor/webencodings/labels.py
++src/pip/_vendor/webencodings/mklabels.py
++src/pip/_vendor/webencodings/tests.py
++src/pip/_vendor/webencodings/x_user_defined.py
+\ No newline at end of file
+Index: venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/requires.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/requires.txt	(date 1573549699880)
++++ venv/Lib/site-packages/pip-10.0.1-py3.7.egg/EGG-INFO/requires.txt	(date 1573549699880)
+@@ -0,0 +1,8 @@
++
++[testing]
++pytest
++mock
++pretend
++scripttest>=1.3
++virtualenv>=1.10
++freezegun
+Index: venv/Scripts/pip3-script.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Scripts/pip3-script.py	(date 1573549702171)
++++ venv/Scripts/pip3-script.py	(date 1573549702171)
+@@ -0,0 +1,12 @@
++#!D:\softwaredata\python\guoya-1910a-zd\venv\Scripts\python.exe
++# EASY-INSTALL-ENTRY-SCRIPT: 'pip==10.0.1','console_scripts','pip3'
++__requires__ = 'pip==10.0.1'
++import re
++import sys
++from pkg_resources import load_entry_point
++
++if __name__ == '__main__':
++    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
++    sys.exit(
++        load_entry_point('pip==10.0.1', 'console_scripts', 'pip3')()
++    )
+Index: venv/Scripts/pip-script.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Scripts/pip-script.py	(date 1573549702156)
++++ venv/Scripts/pip-script.py	(date 1573549702156)
+@@ -0,0 +1,12 @@
++#!D:\softwaredata\python\guoya-1910a-zd\venv\Scripts\python.exe
++# EASY-INSTALL-ENTRY-SCRIPT: 'pip==10.0.1','console_scripts','pip'
++__requires__ = 'pip==10.0.1'
++import re
++import sys
++from pkg_resources import load_entry_point
++
++if __name__ == '__main__':
++    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
++    sys.exit(
++        load_entry_point('pip==10.0.1', 'console_scripts', 'pip')()
++    )
+Index: venv/Scripts/easy_install-script.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Scripts/easy_install-script.py	(date 1573549686899)
++++ venv/Scripts/easy_install-script.py	(date 1573549686899)
+@@ -0,0 +1,12 @@
++#!D:\softwaredata\python\guoya-1910a-zd\venv\Scripts\python.exe
++# EASY-INSTALL-ENTRY-SCRIPT: 'setuptools==39.1.0','console_scripts','easy_install'
++__requires__ = 'setuptools==39.1.0'
++import re
++import sys
++from pkg_resources import load_entry_point
++
++if __name__ == '__main__':
++    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
++    sys.exit(
++        load_entry_point('setuptools==39.1.0', 'console_scripts', 'easy_install')()
++    )
+Index: venv/Scripts/easy_install-3.7-script.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Scripts/easy_install-3.7-script.py	(date 1573549686913)
++++ venv/Scripts/easy_install-3.7-script.py	(date 1573549686913)
+@@ -0,0 +1,12 @@
++#!D:\softwaredata\python\guoya-1910a-zd\venv\Scripts\python.exe
++# EASY-INSTALL-ENTRY-SCRIPT: 'setuptools==39.1.0','console_scripts','easy_install-3.7'
++__requires__ = 'setuptools==39.1.0'
++import re
++import sys
++from pkg_resources import load_entry_point
++
++if __name__ == '__main__':
++    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
++    sys.exit(
++        load_entry_point('setuptools==39.1.0', 'console_scripts', 'easy_install-3.7')()
++    )
+Index: venv/Scripts/pip3.7-script.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Scripts/pip3.7-script.py	(date 1573549702185)
++++ venv/Scripts/pip3.7-script.py	(date 1573549702185)
+@@ -0,0 +1,12 @@
++#!D:\softwaredata\python\guoya-1910a-zd\venv\Scripts\python.exe
++# EASY-INSTALL-ENTRY-SCRIPT: 'pip==10.0.1','console_scripts','pip3.7'
++__requires__ = 'pip==10.0.1'
++import re
++import sys
++from pkg_resources import load_entry_point
++
++if __name__ == '__main__':
++    sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0])
++    sys.exit(
++        load_entry_point('pip==10.0.1', 'console_scripts', 'pip3.7')()
++    )
+Index: venv/Lib/site-packages/setuptools.pth
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/setuptools.pth	(date 1573549686888)
++++ venv/Lib/site-packages/setuptools.pth	(date 1573549686888)
+@@ -0,0 +1,1 @@
++./setuptools-39.1.0-py3.7.egg
+Index: venv/Lib/site-packages/easy-install.pth
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/easy-install.pth	(date 1573549702146)
++++ venv/Lib/site-packages/easy-install.pth	(date 1573549702146)
+@@ -0,0 +1,2 @@
++./setuptools-39.1.0-py3.7.egg
++./pip-10.0.1-py3.7.egg
+Index: venv/pip-selfcheck.json
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/pip-selfcheck.json	(date 1573549856129)
++++ venv/pip-selfcheck.json	(date 1573549856129)
+@@ -0,0 +1,1 @@
++{"last_check":"2019-11-12T09:10:54Z","pypi_version":"19.3.1"}
+\ No newline at end of file
+Index: venv/Lib/site-packages/selenium-3.141.0.dist-info/WHEEL
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium-3.141.0.dist-info/WHEEL	(date 1573549840226)
++++ venv/Lib/site-packages/selenium-3.141.0.dist-info/WHEEL	(date 1573549840226)
+@@ -0,0 +1,6 @@
++Wheel-Version: 1.0
++Generator: bdist_wheel (0.32.1)
++Root-Is-Purelib: true
++Tag: py2-none-any
++Tag: py3-none-any
++
+Index: venv/Lib/site-packages/selenium-3.141.0.dist-info/top_level.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium-3.141.0.dist-info/top_level.txt	(date 1573549840231)
++++ venv/Lib/site-packages/selenium-3.141.0.dist-info/top_level.txt	(date 1573549840231)
+@@ -0,0 +1,1 @@
++selenium
+Index: venv/Lib/site-packages/selenium-3.141.0.dist-info/RECORD
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium-3.141.0.dist-info/RECORD	(date 1573549854000)
++++ venv/Lib/site-packages/selenium-3.141.0.dist-info/RECORD	(date 1573549854000)
+@@ -0,0 +1,182 @@
++selenium/__init__.py,sha256=ivKrIxicNwGMCVkJUlUfMSzglBcjqpohr7qKb31xrl4,813
++selenium/common/__init__.py,sha256=E_V8-VsmWsxJhOnXZn-72u9k_jCgBI6PjV7kEd7P8jA,821
++selenium/common/exceptions.py,sha256=J07uP-vs00qcFYsylgi1q9BteIwqVIr1wKIRfGACiCI,9274
++selenium/webdriver/__init__.py,sha256=xOTI0V4LLgtTusXFiDErRCR_k7OlPbybRqbkyMYwAeo,1981
++selenium/webdriver/android/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/android/webdriver.py,sha256=1Vz2GfYAvyUICUrs90HzD0LELUNhV9u1Sx2qT4uFOV0,1735
++selenium/webdriver/blackberry/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/blackberry/webdriver.py,sha256=kkzKz_3qLB3nijgxlgbiKmqPusmvQdD3RQ6C7aClA-c,4870
++selenium/webdriver/chrome/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/chrome/options.py,sha256=MzNz00TtRPe5nsRtdf8j1kq1wYptp6nMuNdE5d9PL0Q,6540
++selenium/webdriver/chrome/remote_connection.py,sha256=lWuXN-kWVUYHIpNmfory2-r2pFJzg_x6_Gfakcnva4A,1447
++selenium/webdriver/chrome/service.py,sha256=9XiAB3Xl2TT84fIUKQOBFZAGy0u-_epLVIE4fbE8R94,1817
++selenium/webdriver/chrome/webdriver.py,sha256=VHBNRB_LzDiyTXJ5hhBJI9GRkb_TIAKuu9X5UbDR8mw,6024
++selenium/webdriver/common/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/common/action_chains.py,sha256=75peC1roLCzop3ZkSqRcywZchPgm6YsZibMidF6H1Wc,12332
++selenium/webdriver/common/alert.py,sha256=ahOGXUEb9XQWLqzn6X9n1kP665l9z3ARXjh28z3tFe8,3099
++selenium/webdriver/common/by.py,sha256=NrDsEfAwyuPqx74pw3febADe_fKi2mmZOZQvs3DI9qU,1111
++selenium/webdriver/common/desired_capabilities.py,sha256=s50bR1YbG1otMDlomPxULF1cc6R0FhIpveRWud0o3FE,3320
++selenium/webdriver/common/keys.py,sha256=5TAevg0zOzSzqted-4DJxT-vufbS3B9PiGQtgeVWYgU,2347
++selenium/webdriver/common/proxy.py,sha256=SPScO5HA_UqPqFcO3JCgRpGXqo8qaQsA5KNRJX6c9B8,10311
++selenium/webdriver/common/service.py,sha256=n2Gp7SJQ9ld57SZJJjvrM5Eqc802Gs06Q52jm8SqOuk,5855
++selenium/webdriver/common/touch_actions.py,sha256=ttZ8N3W_AE_r_F_npLsDUjwJkcQ_KfTZdfRQk6LdpCg,5980
++selenium/webdriver/common/utils.py,sha256=Zq1OrBzV2h00ZHaaJa323nhoEnb2YKZ0aFcvJg3tXU4,4231
++selenium/webdriver/common/actions/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/common/actions/action_builder.py,sha256=Nh7voMkO4dNtZGR3vrhyDZHuaZ8-OGa6Kj87cdyP-Yw,2825
++selenium/webdriver/common/actions/input_device.py,sha256=oww3ycrYxOl3fhXqgWSiGybiVbBVIXaY5ldfWeTBBMw,1279
++selenium/webdriver/common/actions/interaction.py,sha256=Sy6iVy_-QuTU3Ap3VQJRQoyEmfpgMU2JSAxHYKWlJPA,1434
++selenium/webdriver/common/actions/key_actions.py,sha256=EJJl4YNg7a_g6vsXKoC-rJaA0YaMsY0V8w0tZArDSys,1733
++selenium/webdriver/common/actions/key_input.py,sha256=4PBRGhtLTIZR3Oe28P0ve7RwLWbavV4-W2viunAwQ5k,1782
++selenium/webdriver/common/actions/mouse_button.py,sha256=wgbmQEy-4vqthysQ2w8U0XC_IWYJ7S6s-rvsssslnqg,70
++selenium/webdriver/common/actions/pointer_actions.py,sha256=URJPzC9kz5uq9dnsLD3wmyzf79FJOBACppMMvMj97w4,3335
++selenium/webdriver/common/actions/pointer_input.py,sha256=Hk6iHcDE0rfv012lVbmX1w_EQTC1lGI2hGoXBYaxPCs,2459
++selenium/webdriver/common/html5/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/common/html5/application_cache.py,sha256=Ymy_azm2B6ZSM9H7RCcMfNzFRuccqIa9mpmZxTzdiBM,1430
++selenium/webdriver/edge/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/edge/options.py,sha256=CxzIBjKh5263O_VbFFHDjCDSPV8yttrc3Y585rflOl0,1831
++selenium/webdriver/edge/service.py,sha256=efzKanyRcFE0DThRZ3FOsRH2dcDdaqUiZkD6M75x21k,2161
++selenium/webdriver/edge/webdriver.py,sha256=b6bkZ_xd6oYgBHfKGY7iX9yjp2wv3mLU5blSAJgvuCE,3111
++selenium/webdriver/firefox/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/firefox/extension_connection.py,sha256=8taS3GwbA2PslwXtQ8Xbh6a4U_SJJRIMgpAeCKLrnPI,2875
++selenium/webdriver/firefox/firefox_binary.py,sha256=InWj6DP1BMOqBFerftMvUN1P8JH0fPcy7kKE2ryNnTc,8752
++selenium/webdriver/firefox/firefox_profile.py,sha256=HSKu7Xp1nIbHDgObv95KCA37lR9J9dFbC2QA3b5snj8,15997
++selenium/webdriver/firefox/options.py,sha256=7L_F1qZUPPN9Shh0JsXCqtZdlOE-TIMUaaXnSGQHFe4,5868
++selenium/webdriver/firefox/remote_connection.py,sha256=oHpJ-oTwa9Vca88nDr43id4ghTyCuPNkRu8MjX8hfJI,1719
++selenium/webdriver/firefox/service.py,sha256=_PD-Brf-VmpuJsxKPghlACWe7KG7VvdofaV_na7fpSE,2261
++selenium/webdriver/firefox/webdriver.py,sha256=TiFg67uBZObaQTBMAGMFpqSRJDVxPaYqiZ6fCGZYTyg,10727
++selenium/webdriver/firefox/webdriver.xpi,sha256=hto_oOBBf_iWadkP7en2tj89kDkH7L4lg89-SiXOPnA,716611
++selenium/webdriver/firefox/webdriver_prefs.json,sha256=lGrdKYpeI0bj1T0cvorXwz5JlBMFEfbYt5JovlC3o0w,2826
++selenium/webdriver/firefox/webelement.py,sha256=yq5TGrvlhgHZ-98vxGaURXDNwLhDS9IStpDEgvnC1bs,2023
++selenium/webdriver/firefox/amd64/x_ignore_nofocus.so,sha256=_rACRCJ5PFRrxrHwxMVQbUncne826HyeKoOLdbsltb8,41262
++selenium/webdriver/firefox/x86/x_ignore_nofocus.so,sha256=sGarS2BVGdLrA7BwlEgcwCmXnDCW7fMmLGTaAoyq_tk,30887
++selenium/webdriver/ie/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/ie/options.py,sha256=DpOL-fWXpO0eR__5u8EdlXdcjIrUaKzfPDi2VJFB7U0,10565
++selenium/webdriver/ie/service.py,sha256=xmLctjwk99y3GkboTsU_vYAdLsObWOx_tNtdQKxxVG0,2235
++selenium/webdriver/ie/webdriver.py,sha256=A_Nc43UkQpAqD4RdTDVv4gQFhDzs3MVLW6izXIebPd8,4366
++selenium/webdriver/opera/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/opera/options.py,sha256=PL0tLcGwHN-tX9F2aoOD2mHFJ5PlHxzxPLZ2CA9gYLg,3572
++selenium/webdriver/opera/webdriver.py,sha256=ud0ixfHHkPAExQ414LR9eS4DSw5IMqW7s55h8Wh67Kw,3555
++selenium/webdriver/phantomjs/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/phantomjs/service.py,sha256=7ufko8h80dUTnI8ueP9ee01VKXNRs6fA-PwTzGRKVMQ,2587
++selenium/webdriver/phantomjs/webdriver.py,sha256=SqpeJjazxW_p9iQnEu20Zk6WD1m8SApzJM8_zGpZpSQ,3111
++selenium/webdriver/remote/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/remote/command.py,sha256=F2HYjoLGKGG_NDdDJ1-4b36-NPixU_N4JKNlPIRYkuQ,6499
++selenium/webdriver/remote/errorhandler.py,sha256=1lFjQBPRofEG4OCCTXW7fJJNgETjjvUcuLY4ZUQGl20,11691
++selenium/webdriver/remote/file_detector.py,sha256=YLKofsBD27Nk-PcQRGYJz7_h6vL2Fvlp0DxA47Y2yc4,1655
++selenium/webdriver/remote/getAttribute.js,sha256=EZipmZ3eJN0toNmHfMLo-N1wv9ru4LUBKyTlR0tQ6Iw,6354
++selenium/webdriver/remote/isDisplayed.js,sha256=waBAJOVBT8qMHe7bRSvneoudE7s89n_0Iw1Zg1N6MJY,43992
++selenium/webdriver/remote/mobile.py,sha256=FmRY6Q2xA0uC9mP53RC0-CpS4UWJioCZ3rgaXgtTKkE,2651
++selenium/webdriver/remote/remote_connection.py,sha256=5Vg-YydiKpejg7gaHjqfwWATLl5liA4xv_Ew7HZ20jU,20163
++selenium/webdriver/remote/switch_to.py,sha256=2aDQc0tbgfsBFQFN4mOev-yoekWSnXc7ED1TLL96HOA,4610
++selenium/webdriver/remote/utils.py,sha256=y1TMyyb6Jlq0S1kYoctYPCSUK51-hAAdEXzW5cpw4Yg,3068
++selenium/webdriver/remote/webdriver.py,sha256=nmNnXtUSi92qvBOwTqZUF-BKj9-B_IHGyaPrvk_XSio,41709
++selenium/webdriver/remote/webelement.py,sha256=BSY6pQYYQbwKHwcaCImVq9Ce9pgjhb__jTTANUads1k,23664
++selenium/webdriver/safari/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/safari/permissions.py,sha256=NkYeX6Oslo9PuofKAiWS8znaoV7piLX_7y9iaEyPCb0,942
++selenium/webdriver/safari/remote_connection.py,sha256=lMQ82qy8v-fVWCFYSKT_Dbxxi2l3DFlIRg5F_Q-XgQ0,1330
++selenium/webdriver/safari/service.py,sha256=SnS7bbGqtaJPdLt7VqDFbw5GWgRICMVsivSjRIv6km8,2414
++selenium/webdriver/safari/webdriver.py,sha256=vZDI5H2lodvebdosPwgPzxDvs1nSHYQ3KnlA1VPaeZo,4641
++selenium/webdriver/support/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/support/abstract_event_listener.py,sha256=vuXOIu91JBQXhcfTNUz46q0-dVK9Ct2YQN_-l1wTzUc,2033
++selenium/webdriver/support/color.py,sha256=OETTFeZ-alMCeMKGCS9wu35KVo4poeGL_o4KiHczLFc,11396
++selenium/webdriver/support/event_firing_webdriver.py,sha256=wOvYHB2yexd71O0wguMWnC11OcvhTR8SMyyTi8PMKhs,12342
++selenium/webdriver/support/events.py,sha256=amakwBWfJ37pInruIx1Jzev3-ocb6jCl9lc82TaD5Vw,920
++selenium/webdriver/support/expected_conditions.py,sha256=2s_rMJoQZ3zJHMiyyfTTU_MUEsh-levp_TJjd6xTFEs,13687
++selenium/webdriver/support/select.py,sha256=CLGqWOneyKZsMtb1SufEofJ-nAlp7GMjhIsXAL_bDGw,9257
++selenium/webdriver/support/ui.py,sha256=q6QHalMLPmpmPV8PfmN5K3LCS3JLpygOG6JmYUd3C5k,863
++selenium/webdriver/support/wait.py,sha256=XQs-3mdmZVmgkM3EW1qBw5_BDv_zxIOXEWO0hgGZjGc,4070
++selenium/webdriver/webkitgtk/__init__.py,sha256=3TKaBBK08eiCsGGFFcZlZwwjHHcmj2YO0xImghpJk38,787
++selenium/webdriver/webkitgtk/options.py,sha256=KnfE2dS27_1x-3KNTPl5Vn1jY5uVXNZG6dSuoGNMZMQ,3184
++selenium/webdriver/webkitgtk/service.py,sha256=zWwp9-5vIOTCZN7xHYXiepQ7e1-rNGDBo_e4PxWsPx4,1580
++selenium/webdriver/webkitgtk/webdriver.py,sha256=4dwcYWG5HW2AsXow9KcPMnwjBSvb5OMd7f2cE4eYd6w,2959
++selenium-3.141.0.dist-info/LICENSE,sha256=nIO72GZ6Z7Lad2jSmtue82mLJyCjgKl_Gt8iRURSwvU,11365
++selenium-3.141.0.dist-info/METADATA,sha256=TBrLl5AFOruU-rAPzCHxwwO0-fyouPdnFpAY0sIYtvw,6553
++selenium-3.141.0.dist-info/WHEEL,sha256=8T8fxefr_r-A79qbOJ9d_AaEgkpCGmEPHc-gpCq5BRg,110
++selenium-3.141.0.dist-info/top_level.txt,sha256=h_bHiVqSBt4ebSallLryIo7BkFqIehRmrWlnccPYc5M,9
++selenium-3.141.0.dist-info/RECORD,,
++selenium-3.141.0.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4
++selenium/common/__pycache__/exceptions.cpython-37.pyc,,
++selenium/common/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/android/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/android/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/blackberry/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/blackberry/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/chrome/__pycache__/options.cpython-37.pyc,,
++selenium/webdriver/chrome/__pycache__/remote_connection.cpython-37.pyc,,
++selenium/webdriver/chrome/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/chrome/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/chrome/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/action_builder.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/input_device.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/interaction.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/key_actions.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/key_input.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/mouse_button.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/pointer_actions.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/pointer_input.cpython-37.pyc,,
++selenium/webdriver/common/actions/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/common/html5/__pycache__/application_cache.cpython-37.pyc,,
++selenium/webdriver/common/html5/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/action_chains.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/alert.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/by.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/desired_capabilities.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/keys.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/proxy.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/touch_actions.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/utils.cpython-37.pyc,,
++selenium/webdriver/common/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/edge/__pycache__/options.cpython-37.pyc,,
++selenium/webdriver/edge/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/edge/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/edge/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/extension_connection.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/firefox_binary.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/firefox_profile.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/options.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/remote_connection.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/webelement.cpython-37.pyc,,
++selenium/webdriver/firefox/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/ie/__pycache__/options.cpython-37.pyc,,
++selenium/webdriver/ie/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/ie/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/ie/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/opera/__pycache__/options.cpython-37.pyc,,
++selenium/webdriver/opera/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/opera/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/phantomjs/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/phantomjs/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/phantomjs/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/command.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/errorhandler.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/file_detector.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/mobile.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/remote_connection.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/switch_to.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/utils.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/webelement.cpython-37.pyc,,
++selenium/webdriver/remote/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/safari/__pycache__/permissions.cpython-37.pyc,,
++selenium/webdriver/safari/__pycache__/remote_connection.cpython-37.pyc,,
++selenium/webdriver/safari/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/safari/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/safari/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/abstract_event_listener.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/color.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/events.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/event_firing_webdriver.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/expected_conditions.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/select.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/ui.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/wait.cpython-37.pyc,,
++selenium/webdriver/support/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/webkitgtk/__pycache__/options.cpython-37.pyc,,
++selenium/webdriver/webkitgtk/__pycache__/service.cpython-37.pyc,,
++selenium/webdriver/webkitgtk/__pycache__/webdriver.cpython-37.pyc,,
++selenium/webdriver/webkitgtk/__pycache__/__init__.cpython-37.pyc,,
++selenium/webdriver/__pycache__/__init__.cpython-37.pyc,,
++selenium/__pycache__/__init__.cpython-37.pyc,,
+Index: .idea/vcs.xml
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- .idea/vcs.xml	(date 1573623974182)
++++ .idea/vcs.xml	(date 1573623974182)
+@@ -0,0 +1,6 @@
++<?xml version="1.0" encoding="UTF-8"?>
++<project version="4">
++  <component name="VcsDirectoryMappings">
++    <mapping directory="$PROJECT_DIR$" vcs="Git" />
++  </component>
++</project>
+\ No newline at end of file
+Index: venv/Lib/site-packages/selenium-3.141.0.dist-info/INSTALLER
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium-3.141.0.dist-info/INSTALLER	(date 1573549853985)
++++ venv/Lib/site-packages/selenium-3.141.0.dist-info/INSTALLER	(date 1573549853985)
+@@ -0,0 +1,1 @@
++pip
+Index: venv/Lib/site-packages/selenium-3.141.0.dist-info/METADATA
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium-3.141.0.dist-info/METADATA	(date 1573549840221)
++++ venv/Lib/site-packages/selenium-3.141.0.dist-info/METADATA	(date 1573549840221)
+@@ -0,0 +1,171 @@
++Metadata-Version: 2.1
++Name: selenium
++Version: 3.141.0
++Summary: Python bindings for Selenium
++Home-page: https://github.com/SeleniumHQ/selenium/
++Author: UNKNOWN
++Author-email: UNKNOWN
++License: Apache 2.0
++Platform: UNKNOWN
++Classifier: Development Status :: 5 - Production/Stable
++Classifier: Intended Audience :: Developers
++Classifier: License :: OSI Approved :: Apache Software License
++Classifier: Operating System :: POSIX
++Classifier: Operating System :: Microsoft :: Windows
++Classifier: Operating System :: MacOS :: MacOS X
++Classifier: Topic :: Software Development :: Testing
++Classifier: Topic :: Software Development :: Libraries
++Classifier: Programming Language :: Python
++Classifier: Programming Language :: Python :: 2.7
++Classifier: Programming Language :: Python :: 3.4
++Classifier: Programming Language :: Python :: 3.5
++Classifier: Programming Language :: Python :: 3.6
++Requires-Dist: urllib3
++
++======================
++Selenium Client Driver
++======================
++
++Introduction
++============
++
++Python language bindings for Selenium WebDriver.
++
++The `selenium` package is used to automate web browser interaction from Python.
++
+++-----------+--------------------------------------------------------------------------------------+
++| **Home**: | http://www.seleniumhq.org                                                            |
+++-----------+--------------------------------------------------------------------------------------+
++| **Docs**: | `selenium package API <https://seleniumhq.github.io/selenium/docs/api/py/api.html>`_ |
+++-----------+--------------------------------------------------------------------------------------+
++| **Dev**:  | https://github.com/SeleniumHQ/Selenium                                               |
+++-----------+--------------------------------------------------------------------------------------+
++| **PyPI**: | https://pypi.org/project/selenium/                                                   |
+++-----------+--------------------------------------------------------------------------------------+
++| **IRC**:  | **#selenium** channel on freenode                                                    |
+++-----------+--------------------------------------------------------------------------------------+
++
++Several browsers/drivers are supported (Firefox, Chrome, Internet Explorer), as well as the Remote protocol.
++
++Supported Python Versions
++=========================
++
++* Python 2.7, 3.4+
++
++Installing
++==========
++
++If you have `pip <https://pip.pypa.io/>`_ on your system, you can simply install or upgrade the Python bindings::
++
++    pip install -U selenium
++
++Alternately, you can download the source distribution from `PyPI <https://pypi.org/project/selenium/#files>`_ (e.g. selenium-3.141.0.tar.gz), unarchive it, and run::
++
++    python setup.py install
++
++Note: You may want to consider using `virtualenv <http://www.virtualenv.org/>`_ to create isolated Python environments.
++
++Drivers
++=======
++
++Selenium requires a driver to interface with the chosen browser. Firefox,
++for example, requires `geckodriver <https://github.com/mozilla/geckodriver/releases>`_, which needs to be installed before the below examples can be run. Make sure it's in your `PATH`, e. g., place it in `/usr/bin` or `/usr/local/bin`.
++
++Failure to observe this step will give you an error `selenium.common.exceptions.WebDriverException: Message: 'geckodriver' executable needs to be in PATH.`
++
++Other supported browsers will have their own drivers available. Links to some of the more popular browser drivers follow.
++
+++--------------+-----------------------------------------------------------------------+
++| **Chrome**:  | https://sites.google.com/a/chromium.org/chromedriver/downloads        |
+++--------------+-----------------------------------------------------------------------+
++| **Edge**:    | https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/ |
+++--------------+-----------------------------------------------------------------------+
++| **Firefox**: | https://github.com/mozilla/geckodriver/releases                       |
+++--------------+-----------------------------------------------------------------------+
++| **Safari**:  | https://webkit.org/blog/6900/webdriver-support-in-safari-10/          |
+++--------------+-----------------------------------------------------------------------+
++
++Example 0:
++==========
++
++* open a new Firefox browser
++* load the page at the given URL
++
++.. code-block:: python
++
++    from selenium import webdriver
++
++    browser = webdriver.Firefox()
++    browser.get('http://seleniumhq.org/')
++
++Example 1:
++==========
++
++* open a new Firefox browser
++* load the Yahoo homepage
++* search for "seleniumhq"
++* close the browser
++
++.. code-block:: python
++
++    from selenium import webdriver
++    from selenium.webdriver.common.keys import Keys
++
++    browser = webdriver.Firefox()
++
++    browser.get('http://www.yahoo.com')
++    assert 'Yahoo' in browser.title
++
++    elem = browser.find_element_by_name('p')  # Find the search box
++    elem.send_keys('seleniumhq' + Keys.RETURN)
++
++    browser.quit()
++
++Example 2:
++==========
++
++Selenium WebDriver is often used as a basis for testing web applications.  Here is a simple example using Python's standard `unittest <http://docs.python.org/3/library/unittest.html>`_ library:
++
++.. code-block:: python
++
++    import unittest
++    from selenium import webdriver
++
++    class GoogleTestCase(unittest.TestCase):
++
++        def setUp(self):
++            self.browser = webdriver.Firefox()
++            self.addCleanup(self.browser.quit)
++
++        def testPageTitle(self):
++            self.browser.get('http://www.google.com')
++            self.assertIn('Google', self.browser.title)
++
++    if __name__ == '__main__':
++        unittest.main(verbosity=2)
++
++Selenium Server (optional)
++==========================
++
++For normal WebDriver scripts (non-Remote), the Java server is not needed.
++
++However, to use Selenium Webdriver Remote or the legacy Selenium API (Selenium-RC), you need to also run the Selenium server.  The server requires a Java Runtime Environment (JRE).
++
++Download the server separately, from: http://selenium-release.storage.googleapis.com/3.141/selenium-server-standalone-3.141.0.jar
++
++Run the server from the command line::
++
++    java -jar selenium-server-standalone-3.141.0.jar
++
++Then run your Python client scripts.
++
++Use The Source Luke!
++====================
++
++View source code online:
++
+++-----------+-------------------------------------------------------+
++| official: | https://github.com/SeleniumHQ/selenium/tree/master/py |
+++-----------+-------------------------------------------------------+
++
++
+Index: venv/Lib/site-packages/selenium-3.141.0.dist-info/LICENSE
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium-3.141.0.dist-info/LICENSE	(date 1573549840215)
++++ venv/Lib/site-packages/selenium-3.141.0.dist-info/LICENSE	(date 1573549840215)
+@@ -0,0 +1,202 @@
++
++                                 Apache License
++                           Version 2.0, January 2004
++                        http://www.apache.org/licenses/
++
++   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
++
++   1. Definitions.
++
++      "License" shall mean the terms and conditions for use, reproduction,
++      and distribution as defined by Sections 1 through 9 of this document.
++
++      "Licensor" shall mean the copyright owner or entity authorized by
++      the copyright owner that is granting the License.
++
++      "Legal Entity" shall mean the union of the acting entity and all
++      other entities that control, are controlled by, or are under common
++      control with that entity. For the purposes of this definition,
++      "control" means (i) the power, direct or indirect, to cause the
++      direction or management of such entity, whether by contract or
++      otherwise, or (ii) ownership of fifty percent (50%) or more of the
++      outstanding shares, or (iii) beneficial ownership of such entity.
++
++      "You" (or "Your") shall mean an individual or Legal Entity
++      exercising permissions granted by this License.
++
++      "Source" form shall mean the preferred form for making modifications,
++      including but not limited to software source code, documentation
++      source, and configuration files.
++
++      "Object" form shall mean any form resulting from mechanical
++      transformation or translation of a Source form, including but
++      not limited to compiled object code, generated documentation,
++      and conversions to other media types.
++
++      "Work" shall mean the work of authorship, whether in Source or
++      Object form, made available under the License, as indicated by a
++      copyright notice that is included in or attached to the work
++      (an example is provided in the Appendix below).
++
++      "Derivative Works" shall mean any work, whether in Source or Object
++      form, that is based on (or derived from) the Work and for which the
++      editorial revisions, annotations, elaborations, or other modifications
++      represent, as a whole, an original work of authorship. For the purposes
++      of this License, Derivative Works shall not include works that remain
++      separable from, or merely link (or bind by name) to the interfaces of,
++      the Work and Derivative Works thereof.
++
++      "Contribution" shall mean any work of authorship, including
++      the original version of the Work and any modifications or additions
++      to that Work or Derivative Works thereof, that is intentionally
++      submitted to Licensor for inclusion in the Work by the copyright owner
++      or by an individual or Legal Entity authorized to submit on behalf of
++      the copyright owner. For the purposes of this definition, "submitted"
++      means any form of electronic, verbal, or written communication sent
++      to the Licensor or its representatives, including but not limited to
++      communication on electronic mailing lists, source code control systems,
++      and issue tracking systems that are managed by, or on behalf of, the
++      Licensor for the purpose of discussing and improving the Work, but
++      excluding communication that is conspicuously marked or otherwise
++      designated in writing by the copyright owner as "Not a Contribution."
++
++      "Contributor" shall mean Licensor and any individual or Legal Entity
++      on behalf of whom a Contribution has been received by Licensor and
++      subsequently incorporated within the Work.
++
++   2. Grant of Copyright License. Subject to the terms and conditions of
++      this License, each Contributor hereby grants to You a perpetual,
++      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
++      copyright license to reproduce, prepare Derivative Works of,
++      publicly display, publicly perform, sublicense, and distribute the
++      Work and such Derivative Works in Source or Object form.
++
++   3. Grant of Patent License. Subject to the terms and conditions of
++      this License, each Contributor hereby grants to You a perpetual,
++      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
++      (except as stated in this section) patent license to make, have made,
++      use, offer to sell, sell, import, and otherwise transfer the Work,
++      where such license applies only to those patent claims licensable
++      by such Contributor that are necessarily infringed by their
++      Contribution(s) alone or by combination of their Contribution(s)
++      with the Work to which such Contribution(s) was submitted. If You
++      institute patent litigation against any entity (including a
++      cross-claim or counterclaim in a lawsuit) alleging that the Work
++      or a Contribution incorporated within the Work constitutes direct
++      or contributory patent infringement, then any patent licenses
++      granted to You under this License for that Work shall terminate
++      as of the date such litigation is filed.
++
++   4. Redistribution. You may reproduce and distribute copies of the
++      Work or Derivative Works thereof in any medium, with or without
++      modifications, and in Source or Object form, provided that You
++      meet the following conditions:
++
++      (a) You must give any other recipients of the Work or
++          Derivative Works a copy of this License; and
++
++      (b) You must cause any modified files to carry prominent notices
++          stating that You changed the files; and
++
++      (c) You must retain, in the Source form of any Derivative Works
++          that You distribute, all copyright, patent, trademark, and
++          attribution notices from the Source form of the Work,
++          excluding those notices that do not pertain to any part of
++          the Derivative Works; and
++
++      (d) If the Work includes a "NOTICE" text file as part of its
++          distribution, then any Derivative Works that You distribute must
++          include a readable copy of the attribution notices contained
++          within such NOTICE file, excluding those notices that do not
++          pertain to any part of the Derivative Works, in at least one
++          of the following places: within a NOTICE text file distributed
++          as part of the Derivative Works; within the Source form or
++          documentation, if provided along with the Derivative Works; or,
++          within a display generated by the Derivative Works, if and
++          wherever such third-party notices normally appear. The contents
++          of the NOTICE file are for informational purposes only and
++          do not modify the License. You may add Your own attribution
++          notices within Derivative Works that You distribute, alongside
++          or as an addendum to the NOTICE text from the Work, provided
++          that such additional attribution notices cannot be construed
++          as modifying the License.
++
++      You may add Your own copyright statement to Your modifications and
++      may provide additional or different license terms and conditions
++      for use, reproduction, or distribution of Your modifications, or
++      for any such Derivative Works as a whole, provided Your use,
++      reproduction, and distribution of the Work otherwise complies with
++      the conditions stated in this License.
++
++   5. Submission of Contributions. Unless You explicitly state otherwise,
++      any Contribution intentionally submitted for inclusion in the Work
++      by You to the Licensor shall be under the terms and conditions of
++      this License, without any additional terms or conditions.
++      Notwithstanding the above, nothing herein shall supersede or modify
++      the terms of any separate license agreement you may have executed
++      with Licensor regarding such Contributions.
++
++   6. Trademarks. This License does not grant permission to use the trade
++      names, trademarks, service marks, or product names of the Licensor,
++      except as required for reasonable and customary use in describing the
++      origin of the Work and reproducing the content of the NOTICE file.
++
++   7. Disclaimer of Warranty. Unless required by applicable law or
++      agreed to in writing, Licensor provides the Work (and each
++      Contributor provides its Contributions) on an "AS IS" BASIS,
++      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
++      implied, including, without limitation, any warranties or conditions
++      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
++      PARTICULAR PURPOSE. You are solely responsible for determining the
++      appropriateness of using or redistributing the Work and assume any
++      risks associated with Your exercise of permissions under this License.
++
++   8. Limitation of Liability. In no event and under no legal theory,
++      whether in tort (including negligence), contract, or otherwise,
++      unless required by applicable law (such as deliberate and grossly
++      negligent acts) or agreed to in writing, shall any Contributor be
++      liable to You for damages, including any direct, indirect, special,
++      incidental, or consequential damages of any character arising as a
++      result of this License or out of the use or inability to use the
++      Work (including but not limited to damages for loss of goodwill,
++      work stoppage, computer failure or malfunction, or any and all
++      other commercial damages or losses), even if such Contributor
++      has been advised of the possibility of such damages.
++
++   9. Accepting Warranty or Additional Liability. While redistributing
++      the Work or Derivative Works thereof, You may choose to offer,
++      and charge a fee for, acceptance of support, warranty, indemnity,
++      or other liability obligations and/or rights consistent with this
++      License. However, in accepting such obligations, You may act only
++      on Your own behalf and on Your sole responsibility, not on behalf
++      of any other Contributor, and only if You agree to indemnify,
++      defend, and hold each Contributor harmless for any liability
++      incurred by, or claims asserted against, such Contributor by reason
++      of your accepting any such warranty or additional liability.
++
++   END OF TERMS AND CONDITIONS
++
++   APPENDIX: How to apply the Apache License to your work.
++
++      To apply the Apache License to your work, attach the following
++      boilerplate notice, with the fields enclosed by brackets "[]"
++      replaced with your own identifying information. (Don't include
++      the brackets!)  The text should be enclosed in the appropriate
++      comment syntax for the file format. We also recommend that a
++      file or class name and description of purpose be included on the
++      same "printed page" as the copyright notice for easier
++      identification within third-party archives.
++
++   Copyright 2018 Software Freedom Conservancy (SFC)
++
++   Licensed under the Apache License, Version 2.0 (the "License");
++   you may not use this file except in compliance with the License.
++   You may obtain a copy of the License at
++
++       http://www.apache.org/licenses/LICENSE-2.0
++
++   Unless required by applicable law or agreed to in writing, software
++   distributed under the License is distributed on an "AS IS" BASIS,
++   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
++   See the License for the specific language governing permissions and
++   limitations under the License.
+Index: venv/Lib/site-packages/urllib3/_collections.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/_collections.py	(date 1573549849381)
++++ venv/Lib/site-packages/urllib3/_collections.py	(date 1573549849381)
+@@ -0,0 +1,336 @@
++from __future__ import absolute_import
++
++try:
++    from collections.abc import Mapping, MutableMapping
++except ImportError:
++    from collections import Mapping, MutableMapping
++try:
++    from threading import RLock
++except ImportError:  # Platform-specific: No threads available
++
++    class RLock:
++        def __enter__(self):
++            pass
++
++        def __exit__(self, exc_type, exc_value, traceback):
++            pass
++
++
++from collections import OrderedDict
++from .exceptions import InvalidHeader
++from .packages.six import iterkeys, itervalues, PY3
++
++
++__all__ = ["RecentlyUsedContainer", "HTTPHeaderDict"]
++
++
++_Null = object()
++
++
++class RecentlyUsedContainer(MutableMapping):
++    """
++    Provides a thread-safe dict-like container which maintains up to
++    ``maxsize`` keys while throwing away the least-recently-used keys beyond
++    ``maxsize``.
++
++    :param maxsize:
++        Maximum number of recent elements to retain.
++
++    :param dispose_func:
++        Every time an item is evicted from the container,
++        ``dispose_func(value)`` is called.  Callback which will get called
++    """
++
++    ContainerCls = OrderedDict
++
++    def __init__(self, maxsize=10, dispose_func=None):
++        self._maxsize = maxsize
++        self.dispose_func = dispose_func
++
++        self._container = self.ContainerCls()
++        self.lock = RLock()
++
++    def __getitem__(self, key):
++        # Re-insert the item, moving it to the end of the eviction line.
++        with self.lock:
++            item = self._container.pop(key)
++            self._container[key] = item
++            return item
++
++    def __setitem__(self, key, value):
++        evicted_value = _Null
++        with self.lock:
++            # Possibly evict the existing value of 'key'
++            evicted_value = self._container.get(key, _Null)
++            self._container[key] = value
++
++            # If we didn't evict an existing value, we might have to evict the
++            # least recently used item from the beginning of the container.
++            if len(self._container) > self._maxsize:
++                _key, evicted_value = self._container.popitem(last=False)
++
++        if self.dispose_func and evicted_value is not _Null:
++            self.dispose_func(evicted_value)
++
++    def __delitem__(self, key):
++        with self.lock:
++            value = self._container.pop(key)
++
++        if self.dispose_func:
++            self.dispose_func(value)
++
++    def __len__(self):
++        with self.lock:
++            return len(self._container)
++
++    def __iter__(self):
++        raise NotImplementedError(
++            "Iteration over this class is unlikely to be threadsafe."
++        )
++
++    def clear(self):
++        with self.lock:
++            # Copy pointers to all values, then wipe the mapping
++            values = list(itervalues(self._container))
++            self._container.clear()
++
++        if self.dispose_func:
++            for value in values:
++                self.dispose_func(value)
++
++    def keys(self):
++        with self.lock:
++            return list(iterkeys(self._container))
++
++
++class HTTPHeaderDict(MutableMapping):
++    """
++    :param headers:
++        An iterable of field-value pairs. Must not contain multiple field names
++        when compared case-insensitively.
++
++    :param kwargs:
++        Additional field-value pairs to pass in to ``dict.update``.
++
++    A ``dict`` like container for storing HTTP Headers.
++
++    Field names are stored and compared case-insensitively in compliance with
++    RFC 7230. Iteration provides the first case-sensitive key seen for each
++    case-insensitive pair.
++
++    Using ``__setitem__`` syntax overwrites fields that compare equal
++    case-insensitively in order to maintain ``dict``'s api. For fields that
++    compare equal, instead create a new ``HTTPHeaderDict`` and use ``.add``
++    in a loop.
++
++    If multiple fields that are equal case-insensitively are passed to the
++    constructor or ``.update``, the behavior is undefined and some will be
++    lost.
++
++    >>> headers = HTTPHeaderDict()
++    >>> headers.add('Set-Cookie', 'foo=bar')
++    >>> headers.add('set-cookie', 'baz=quxx')
++    >>> headers['content-length'] = '7'
++    >>> headers['SET-cookie']
++    'foo=bar, baz=quxx'
++    >>> headers['Content-Length']
++    '7'
++    """
++
++    def __init__(self, headers=None, **kwargs):
++        super(HTTPHeaderDict, self).__init__()
++        self._container = OrderedDict()
++        if headers is not None:
++            if isinstance(headers, HTTPHeaderDict):
++                self._copy_from(headers)
++            else:
++                self.extend(headers)
++        if kwargs:
++            self.extend(kwargs)
++
++    def __setitem__(self, key, val):
++        self._container[key.lower()] = [key, val]
++        return self._container[key.lower()]
++
++    def __getitem__(self, key):
++        val = self._container[key.lower()]
++        return ", ".join(val[1:])
++
++    def __delitem__(self, key):
++        del self._container[key.lower()]
++
++    def __contains__(self, key):
++        return key.lower() in self._container
++
++    def __eq__(self, other):
++        if not isinstance(other, Mapping) and not hasattr(other, "keys"):
++            return False
++        if not isinstance(other, type(self)):
++            other = type(self)(other)
++        return dict((k.lower(), v) for k, v in self.itermerged()) == dict(
++            (k.lower(), v) for k, v in other.itermerged()
++        )
++
++    def __ne__(self, other):
++        return not self.__eq__(other)
++
++    if not PY3:  # Python 2
++        iterkeys = MutableMapping.iterkeys
++        itervalues = MutableMapping.itervalues
++
++    __marker = object()
++
++    def __len__(self):
++        return len(self._container)
++
++    def __iter__(self):
++        # Only provide the originally cased names
++        for vals in self._container.values():
++            yield vals[0]
++
++    def pop(self, key, default=__marker):
++        """D.pop(k[,d]) -> v, remove specified key and return the corresponding value.
++          If key is not found, d is returned if given, otherwise KeyError is raised.
++        """
++        # Using the MutableMapping function directly fails due to the private marker.
++        # Using ordinary dict.pop would expose the internal structures.
++        # So let's reinvent the wheel.
++        try:
++            value = self[key]
++        except KeyError:
++            if default is self.__marker:
++                raise
++            return default
++        else:
++            del self[key]
++            return value
++
++    def discard(self, key):
++        try:
++            del self[key]
++        except KeyError:
++            pass
++
++    def add(self, key, val):
++        """Adds a (name, value) pair, doesn't overwrite the value if it already
++        exists.
++
++        >>> headers = HTTPHeaderDict(foo='bar')
++        >>> headers.add('Foo', 'baz')
++        >>> headers['foo']
++        'bar, baz'
++        """
++        key_lower = key.lower()
++        new_vals = [key, val]
++        # Keep the common case aka no item present as fast as possible
++        vals = self._container.setdefault(key_lower, new_vals)
++        if new_vals is not vals:
++            vals.append(val)
++
++    def extend(self, *args, **kwargs):
++        """Generic import function for any type of header-like object.
++        Adapted version of MutableMapping.update in order to insert items
++        with self.add instead of self.__setitem__
++        """
++        if len(args) > 1:
++            raise TypeError(
++                "extend() takes at most 1 positional "
++                "arguments ({0} given)".format(len(args))
++            )
++        other = args[0] if len(args) >= 1 else ()
++
++        if isinstance(other, HTTPHeaderDict):
++            for key, val in other.iteritems():
++                self.add(key, val)
++        elif isinstance(other, Mapping):
++            for key in other:
++                self.add(key, other[key])
++        elif hasattr(other, "keys"):
++            for key in other.keys():
++                self.add(key, other[key])
++        else:
++            for key, value in other:
++                self.add(key, value)
++
++        for key, value in kwargs.items():
++            self.add(key, value)
++
++    def getlist(self, key, default=__marker):
++        """Returns a list of all the values for the named field. Returns an
++        empty list if the key doesn't exist."""
++        try:
++            vals = self._container[key.lower()]
++        except KeyError:
++            if default is self.__marker:
++                return []
++            return default
++        else:
++            return vals[1:]
++
++    # Backwards compatibility for httplib
++    getheaders = getlist
++    getallmatchingheaders = getlist
++    iget = getlist
++
++    # Backwards compatibility for http.cookiejar
++    get_all = getlist
++
++    def __repr__(self):
++        return "%s(%s)" % (type(self).__name__, dict(self.itermerged()))
++
++    def _copy_from(self, other):
++        for key in other:
++            val = other.getlist(key)
++            if isinstance(val, list):
++                # Don't need to convert tuples
++                val = list(val)
++            self._container[key.lower()] = [key] + val
++
++    def copy(self):
++        clone = type(self)()
++        clone._copy_from(self)
++        return clone
++
++    def iteritems(self):
++        """Iterate over all header lines, including duplicate ones."""
++        for key in self:
++            vals = self._container[key.lower()]
++            for val in vals[1:]:
++                yield vals[0], val
++
++    def itermerged(self):
++        """Iterate over all headers, merging duplicate ones together."""
++        for key in self:
++            val = self._container[key.lower()]
++            yield val[0], ", ".join(val[1:])
++
++    def items(self):
++        return list(self.iteritems())
++
++    @classmethod
++    def from_httplib(cls, message):  # Python 2
++        """Read headers from a Python 2 httplib message object."""
++        # python2.7 does not expose a proper API for exporting multiheaders
++        # efficiently. This function re-reads raw lines from the message
++        # object and extracts the multiheaders properly.
++        obs_fold_continued_leaders = (" ", "\t")
++        headers = []
++
++        for line in message.headers:
++            if line.startswith(obs_fold_continued_leaders):
++                if not headers:
++                    # We received a header line that starts with OWS as described
++                    # in RFC-7230 S3.2.4. This indicates a multiline header, but
++                    # there exists no previous header to which we can attach it.
++                    raise InvalidHeader(
++                        "Header continuation with no previous header: %s" % line
++                    )
++                else:
++                    key, value = headers[-1]
++                    headers[-1] = (key, value + " " + line.strip())
++                    continue
++
++            key, value = line.split(":", 1)
++            headers.append((key, value.strip()))
++
++        return cls(headers)
+Index: venv/Lib/site-packages/urllib3/exceptions.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/exceptions.py	(date 1573549849396)
++++ venv/Lib/site-packages/urllib3/exceptions.py	(date 1573549849396)
+@@ -0,0 +1,255 @@
++from __future__ import absolute_import
++from .packages.six.moves.http_client import IncompleteRead as httplib_IncompleteRead
++
++# Base Exceptions
++
++
++class HTTPError(Exception):
++    "Base exception used by this module."
++    pass
++
++
++class HTTPWarning(Warning):
++    "Base warning used by this module."
++    pass
++
++
++class PoolError(HTTPError):
++    "Base exception for errors caused within a pool."
++
++    def __init__(self, pool, message):
++        self.pool = pool
++        HTTPError.__init__(self, "%s: %s" % (pool, message))
++
++    def __reduce__(self):
++        # For pickling purposes.
++        return self.__class__, (None, None)
++
++
++class RequestError(PoolError):
++    "Base exception for PoolErrors that have associated URLs."
++
++    def __init__(self, pool, url, message):
++        self.url = url
++        PoolError.__init__(self, pool, message)
++
++    def __reduce__(self):
++        # For pickling purposes.
++        return self.__class__, (None, self.url, None)
++
++
++class SSLError(HTTPError):
++    "Raised when SSL certificate fails in an HTTPS connection."
++    pass
++
++
++class ProxyError(HTTPError):
++    "Raised when the connection to a proxy fails."
++    pass
++
++
++class DecodeError(HTTPError):
++    "Raised when automatic decoding based on Content-Type fails."
++    pass
++
++
++class ProtocolError(HTTPError):
++    "Raised when something unexpected happens mid-request/response."
++    pass
++
++
++#: Renamed to ProtocolError but aliased for backwards compatibility.
++ConnectionError = ProtocolError
++
++
++# Leaf Exceptions
++
++
++class MaxRetryError(RequestError):
++    """Raised when the maximum number of retries is exceeded.
++
++    :param pool: The connection pool
++    :type pool: :class:`~urllib3.connectionpool.HTTPConnectionPool`
++    :param string url: The requested Url
++    :param exceptions.Exception reason: The underlying error
++
++    """
++
++    def __init__(self, pool, url, reason=None):
++        self.reason = reason
++
++        message = "Max retries exceeded with url: %s (Caused by %r)" % (url, reason)
++
++        RequestError.__init__(self, pool, url, message)
++
++
++class HostChangedError(RequestError):
++    "Raised when an existing pool gets a request for a foreign host."
++
++    def __init__(self, pool, url, retries=3):
++        message = "Tried to open a foreign host with url: %s" % url
++        RequestError.__init__(self, pool, url, message)
++        self.retries = retries
++
++
++class TimeoutStateError(HTTPError):
++    """ Raised when passing an invalid state to a timeout """
++
++    pass
++
++
++class TimeoutError(HTTPError):
++    """ Raised when a socket timeout error occurs.
++
++    Catching this error will catch both :exc:`ReadTimeoutErrors
++    <ReadTimeoutError>` and :exc:`ConnectTimeoutErrors <ConnectTimeoutError>`.
++    """
++
++    pass
++
++
++class ReadTimeoutError(TimeoutError, RequestError):
++    "Raised when a socket timeout occurs while receiving data from a server"
++    pass
++
++
++# This timeout error does not have a URL attached and needs to inherit from the
++# base HTTPError
++class ConnectTimeoutError(TimeoutError):
++    "Raised when a socket timeout occurs while connecting to a server"
++    pass
++
++
++class NewConnectionError(ConnectTimeoutError, PoolError):
++    "Raised when we fail to establish a new connection. Usually ECONNREFUSED."
++    pass
++
++
++class EmptyPoolError(PoolError):
++    "Raised when a pool runs out of connections and no more are allowed."
++    pass
++
++
++class ClosedPoolError(PoolError):
++    "Raised when a request enters a pool after the pool has been closed."
++    pass
++
++
++class LocationValueError(ValueError, HTTPError):
++    "Raised when there is something wrong with a given URL input."
++    pass
++
++
++class LocationParseError(LocationValueError):
++    "Raised when get_host or similar fails to parse the URL input."
++
++    def __init__(self, location):
++        message = "Failed to parse: %s" % location
++        HTTPError.__init__(self, message)
++
++        self.location = location
++
++
++class ResponseError(HTTPError):
++    "Used as a container for an error reason supplied in a MaxRetryError."
++    GENERIC_ERROR = "too many error responses"
++    SPECIFIC_ERROR = "too many {status_code} error responses"
++
++
++class SecurityWarning(HTTPWarning):
++    "Warned when performing security reducing actions"
++    pass
++
++
++class SubjectAltNameWarning(SecurityWarning):
++    "Warned when connecting to a host with a certificate missing a SAN."
++    pass
++
++
++class InsecureRequestWarning(SecurityWarning):
++    "Warned when making an unverified HTTPS request."
++    pass
++
++
++class SystemTimeWarning(SecurityWarning):
++    "Warned when system time is suspected to be wrong"
++    pass
++
++
++class InsecurePlatformWarning(SecurityWarning):
++    "Warned when certain SSL configuration is not available on a platform."
++    pass
++
++
++class SNIMissingWarning(HTTPWarning):
++    "Warned when making a HTTPS request without SNI available."
++    pass
++
++
++class DependencyWarning(HTTPWarning):
++    """
++    Warned when an attempt is made to import a module with missing optional
++    dependencies.
++    """
++
++    pass
++
++
++class ResponseNotChunked(ProtocolError, ValueError):
++    "Response needs to be chunked in order to read it as chunks."
++    pass
++
++
++class BodyNotHttplibCompatible(HTTPError):
++    """
++    Body should be httplib.HTTPResponse like (have an fp attribute which
++    returns raw chunks) for read_chunked().
++    """
++
++    pass
++
++
++class IncompleteRead(HTTPError, httplib_IncompleteRead):
++    """
++    Response length doesn't match expected Content-Length
++
++    Subclass of http_client.IncompleteRead to allow int value
++    for `partial` to avoid creating large objects on streamed
++    reads.
++    """
++
++    def __init__(self, partial, expected):
++        super(IncompleteRead, self).__init__(partial, expected)
++
++    def __repr__(self):
++        return "IncompleteRead(%i bytes read, %i more expected)" % (
++            self.partial,
++            self.expected,
++        )
++
++
++class InvalidHeader(HTTPError):
++    "The header provided was somehow invalid."
++    pass
++
++
++class ProxySchemeUnknown(AssertionError, ValueError):
++    "ProxyManager does not support the supplied scheme"
++    # TODO(t-8ch): Stop inheriting from AssertionError in v2.0.
++
++    def __init__(self, scheme):
++        message = "Not supported proxy scheme %s" % scheme
++        super(ProxySchemeUnknown, self).__init__(message)
++
++
++class HeaderParsingError(HTTPError):
++    "Raised by assert_header_parsing, but we convert it to a log.warning statement."
++
++    def __init__(self, defects, unparsed_data):
++        message = "%s, unparsed data: %r" % (defects or "Unknown", unparsed_data)
++        super(HeaderParsingError, self).__init__(message)
++
++
++class UnrewindableBodyError(HTTPError):
++    "urllib3 encountered an error when trying to rewind a body"
++    pass
+Index: venv/Lib/site-packages/urllib3/fields.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/fields.py	(date 1573549849401)
++++ venv/Lib/site-packages/urllib3/fields.py	(date 1573549849401)
+@@ -0,0 +1,273 @@
++from __future__ import absolute_import
++import email.utils
++import mimetypes
++import re
++
++from .packages import six
++
++
++def guess_content_type(filename, default="application/octet-stream"):
++    """
++    Guess the "Content-Type" of a file.
++
++    :param filename:
++        The filename to guess the "Content-Type" of using :mod:`mimetypes`.
++    :param default:
++        If no "Content-Type" can be guessed, default to `default`.
++    """
++    if filename:
++        return mimetypes.guess_type(filename)[0] or default
++    return default
++
++
++def format_header_param_rfc2231(name, value):
++    """
++    Helper function to format and quote a single header parameter using the
++    strategy defined in RFC 2231.
++
++    Particularly useful for header parameters which might contain
++    non-ASCII values, like file names. This follows RFC 2388 Section 4.4.
++
++    :param name:
++        The name of the parameter, a string expected to be ASCII only.
++    :param value:
++        The value of the parameter, provided as ``bytes`` or `str``.
++    :ret:
++        An RFC-2231-formatted unicode string.
++    """
++    if isinstance(value, six.binary_type):
++        value = value.decode("utf-8")
++
++    if not any(ch in value for ch in '"\\\r\n'):
++        result = u'%s="%s"' % (name, value)
++        try:
++            result.encode("ascii")
++        except (UnicodeEncodeError, UnicodeDecodeError):
++            pass
++        else:
++            return result
++
++    if six.PY2:  # Python 2:
++        value = value.encode("utf-8")
++
++    # encode_rfc2231 accepts an encoded string and returns an ascii-encoded
++    # string in Python 2 but accepts and returns unicode strings in Python 3
++    value = email.utils.encode_rfc2231(value, "utf-8")
++    value = "%s*=%s" % (name, value)
++
++    if six.PY2:  # Python 2:
++        value = value.decode("utf-8")
++
++    return value
++
++
++_HTML5_REPLACEMENTS = {
++    u"\u0022": u"%22",
++    # Replace "\" with "\\".
++    u"\u005C": u"\u005C\u005C",
++    u"\u005C": u"\u005C\u005C",
++}
++
++# All control characters from 0x00 to 0x1F *except* 0x1B.
++_HTML5_REPLACEMENTS.update(
++    {
++        six.unichr(cc): u"%{:02X}".format(cc)
++        for cc in range(0x00, 0x1F + 1)
++        if cc not in (0x1B,)
++    }
++)
++
++
++def _replace_multiple(value, needles_and_replacements):
++    def replacer(match):
++        return needles_and_replacements[match.group(0)]
++
++    pattern = re.compile(
++        r"|".join([re.escape(needle) for needle in needles_and_replacements.keys()])
++    )
++
++    result = pattern.sub(replacer, value)
++
++    return result
++
++
++def format_header_param_html5(name, value):
++    """
++    Helper function to format and quote a single header parameter using the
++    HTML5 strategy.
++
++    Particularly useful for header parameters which might contain
++    non-ASCII values, like file names. This follows the `HTML5 Working Draft
++    Section 4.10.22.7`_ and matches the behavior of curl and modern browsers.
++
++    .. _HTML5 Working Draft Section 4.10.22.7:
++        https://w3c.github.io/html/sec-forms.html#multipart-form-data
++
++    :param name:
++        The name of the parameter, a string expected to be ASCII only.
++    :param value:
++        The value of the parameter, provided as ``bytes`` or `str``.
++    :ret:
++        A unicode string, stripped of troublesome characters.
++    """
++    if isinstance(value, six.binary_type):
++        value = value.decode("utf-8")
++
++    value = _replace_multiple(value, _HTML5_REPLACEMENTS)
++
++    return u'%s="%s"' % (name, value)
++
++
++# For backwards-compatibility.
++format_header_param = format_header_param_html5
++
++
++class RequestField(object):
++    """
++    A data container for request body parameters.
++
++    :param name:
++        The name of this request field. Must be unicode.
++    :param data:
++        The data/value body.
++    :param filename:
++        An optional filename of the request field. Must be unicode.
++    :param headers:
++        An optional dict-like object of headers to initially use for the field.
++    :param header_formatter:
++        An optional callable that is used to encode and format the headers. By
++        default, this is :func:`format_header_param_html5`.
++    """
++
++    def __init__(
++        self,
++        name,
++        data,
++        filename=None,
++        headers=None,
++        header_formatter=format_header_param_html5,
++    ):
++        self._name = name
++        self._filename = filename
++        self.data = data
++        self.headers = {}
++        if headers:
++            self.headers = dict(headers)
++        self.header_formatter = header_formatter
++
++    @classmethod
++    def from_tuples(cls, fieldname, value, header_formatter=format_header_param_html5):
++        """
++        A :class:`~urllib3.fields.RequestField` factory from old-style tuple parameters.
++
++        Supports constructing :class:`~urllib3.fields.RequestField` from
++        parameter of key/value strings AND key/filetuple. A filetuple is a
++        (filename, data, MIME type) tuple where the MIME type is optional.
++        For example::
++
++            'foo': 'bar',
++            'fakefile': ('foofile.txt', 'contents of foofile'),
++            'realfile': ('barfile.txt', open('realfile').read()),
++            'typedfile': ('bazfile.bin', open('bazfile').read(), 'image/jpeg'),
++            'nonamefile': 'contents of nonamefile field',
++
++        Field names and filenames must be unicode.
++        """
++        if isinstance(value, tuple):
++            if len(value) == 3:
++                filename, data, content_type = value
++            else:
++                filename, data = value
++                content_type = guess_content_type(filename)
++        else:
++            filename = None
++            content_type = None
++            data = value
++
++        request_param = cls(
++            fieldname, data, filename=filename, header_formatter=header_formatter
++        )
++        request_param.make_multipart(content_type=content_type)
++
++        return request_param
++
++    def _render_part(self, name, value):
++        """
++        Overridable helper function to format a single header parameter. By
++        default, this calls ``self.header_formatter``.
++
++        :param name:
++            The name of the parameter, a string expected to be ASCII only.
++        :param value:
++            The value of the parameter, provided as a unicode string.
++        """
++
++        return self.header_formatter(name, value)
++
++    def _render_parts(self, header_parts):
++        """
++        Helper function to format and quote a single header.
++
++        Useful for single headers that are composed of multiple items. E.g.,
++        'Content-Disposition' fields.
++
++        :param header_parts:
++            A sequence of (k, v) tuples or a :class:`dict` of (k, v) to format
++            as `k1="v1"; k2="v2"; ...`.
++        """
++        parts = []
++        iterable = header_parts
++        if isinstance(header_parts, dict):
++            iterable = header_parts.items()
++
++        for name, value in iterable:
++            if value is not None:
++                parts.append(self._render_part(name, value))
++
++        return u"; ".join(parts)
++
++    def render_headers(self):
++        """
++        Renders the headers for this request field.
++        """
++        lines = []
++
++        sort_keys = ["Content-Disposition", "Content-Type", "Content-Location"]
++        for sort_key in sort_keys:
++            if self.headers.get(sort_key, False):
++                lines.append(u"%s: %s" % (sort_key, self.headers[sort_key]))
++
++        for header_name, header_value in self.headers.items():
++            if header_name not in sort_keys:
++                if header_value:
++                    lines.append(u"%s: %s" % (header_name, header_value))
++
++        lines.append(u"\r\n")
++        return u"\r\n".join(lines)
++
++    def make_multipart(
++        self, content_disposition=None, content_type=None, content_location=None
++    ):
++        """
++        Makes this request field into a multipart request field.
++
++        This method overrides "Content-Disposition", "Content-Type" and
++        "Content-Location" headers to the request parameter.
++
++        :param content_type:
++            The 'Content-Type' of the request body.
++        :param content_location:
++            The 'Content-Location' of the request body.
++
++        """
++        self.headers["Content-Disposition"] = content_disposition or u"form-data"
++        self.headers["Content-Disposition"] += u"; ".join(
++            [
++                u"",
++                self._render_parts(
++                    ((u"name", self._name), (u"filename", self._filename))
++                ),
++            ]
++        )
++        self.headers["Content-Type"] = content_type
++        self.headers["Content-Location"] = content_location
+Index: venv/Lib/site-packages/urllib3/response.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/response.py	(date 1573549849422)
++++ venv/Lib/site-packages/urllib3/response.py	(date 1573549849422)
+@@ -0,0 +1,809 @@
++from __future__ import absolute_import
++from contextlib import contextmanager
++import zlib
++import io
++import logging
++from socket import timeout as SocketTimeout
++from socket import error as SocketError
++
++try:
++    import brotli
++except ImportError:
++    brotli = None
++
++from ._collections import HTTPHeaderDict
++from .exceptions import (
++    BodyNotHttplibCompatible,
++    ProtocolError,
++    DecodeError,
++    ReadTimeoutError,
++    ResponseNotChunked,
++    IncompleteRead,
++    InvalidHeader,
++)
++from .packages.six import string_types as basestring, PY3
++from .packages.six.moves import http_client as httplib
++from .connection import HTTPException, BaseSSLError
++from .util.response import is_fp_closed, is_response_to_head
++
++log = logging.getLogger(__name__)
++
++
++class DeflateDecoder(object):
++    def __init__(self):
++        self._first_try = True
++        self._data = b""
++        self._obj = zlib.decompressobj()
++
++    def __getattr__(self, name):
++        return getattr(self._obj, name)
++
++    def decompress(self, data):
++        if not data:
++            return data
++
++        if not self._first_try:
++            return self._obj.decompress(data)
++
++        self._data += data
++        try:
++            decompressed = self._obj.decompress(data)
++            if decompressed:
++                self._first_try = False
++                self._data = None
++            return decompressed
++        except zlib.error:
++            self._first_try = False
++            self._obj = zlib.decompressobj(-zlib.MAX_WBITS)
++            try:
++                return self.decompress(self._data)
++            finally:
++                self._data = None
++
++
++class GzipDecoderState(object):
++
++    FIRST_MEMBER = 0
++    OTHER_MEMBERS = 1
++    SWALLOW_DATA = 2
++
++
++class GzipDecoder(object):
++    def __init__(self):
++        self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
++        self._state = GzipDecoderState.FIRST_MEMBER
++
++    def __getattr__(self, name):
++        return getattr(self._obj, name)
++
++    def decompress(self, data):
++        ret = bytearray()
++        if self._state == GzipDecoderState.SWALLOW_DATA or not data:
++            return bytes(ret)
++        while True:
++            try:
++                ret += self._obj.decompress(data)
++            except zlib.error:
++                previous_state = self._state
++                # Ignore data after the first error
++                self._state = GzipDecoderState.SWALLOW_DATA
++                if previous_state == GzipDecoderState.OTHER_MEMBERS:
++                    # Allow trailing garbage acceptable in other gzip clients
++                    return bytes(ret)
++                raise
++            data = self._obj.unused_data
++            if not data:
++                return bytes(ret)
++            self._state = GzipDecoderState.OTHER_MEMBERS
++            self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
++
++
++if brotli is not None:
++
++    class BrotliDecoder(object):
++        # Supports both 'brotlipy' and 'Brotli' packages
++        # since they share an import name. The top branches
++        # are for 'brotlipy' and bottom branches for 'Brotli'
++        def __init__(self):
++            self._obj = brotli.Decompressor()
++
++        def decompress(self, data):
++            if hasattr(self._obj, "decompress"):
++                return self._obj.decompress(data)
++            return self._obj.process(data)
++
++        def flush(self):
++            if hasattr(self._obj, "flush"):
++                return self._obj.flush()
++            return b""
++
++
++class MultiDecoder(object):
++    """
++    From RFC7231:
++        If one or more encodings have been applied to a representation, the
++        sender that applied the encodings MUST generate a Content-Encoding
++        header field that lists the content codings in the order in which
++        they were applied.
++    """
++
++    def __init__(self, modes):
++        self._decoders = [_get_decoder(m.strip()) for m in modes.split(",")]
++
++    def flush(self):
++        return self._decoders[0].flush()
++
++    def decompress(self, data):
++        for d in reversed(self._decoders):
++            data = d.decompress(data)
++        return data
++
++
++def _get_decoder(mode):
++    if "," in mode:
++        return MultiDecoder(mode)
++
++    if mode == "gzip":
++        return GzipDecoder()
++
++    if brotli is not None and mode == "br":
++        return BrotliDecoder()
++
++    return DeflateDecoder()
++
++
++class HTTPResponse(io.IOBase):
++    """
++    HTTP Response container.
++
++    Backwards-compatible to httplib's HTTPResponse but the response ``body`` is
++    loaded and decoded on-demand when the ``data`` property is accessed.  This
++    class is also compatible with the Python standard library's :mod:`io`
++    module, and can hence be treated as a readable object in the context of that
++    framework.
++
++    Extra parameters for behaviour not present in httplib.HTTPResponse:
++
++    :param preload_content:
++        If True, the response's body will be preloaded during construction.
++
++    :param decode_content:
++        If True, will attempt to decode the body based on the
++        'content-encoding' header.
++
++    :param original_response:
++        When this HTTPResponse wrapper is generated from an httplib.HTTPResponse
++        object, it's convenient to include the original for debug purposes. It's
++        otherwise unused.
++
++    :param retries:
++        The retries contains the last :class:`~urllib3.util.retry.Retry` that
++        was used during the request.
++
++    :param enforce_content_length:
++        Enforce content length checking. Body returned by server must match
++        value of Content-Length header, if present. Otherwise, raise error.
++    """
++
++    CONTENT_DECODERS = ["gzip", "deflate"]
++    if brotli is not None:
++        CONTENT_DECODERS += ["br"]
++    REDIRECT_STATUSES = [301, 302, 303, 307, 308]
++
++    def __init__(
++        self,
++        body="",
++        headers=None,
++        status=0,
++        version=0,
++        reason=None,
++        strict=0,
++        preload_content=True,
++        decode_content=True,
++        original_response=None,
++        pool=None,
++        connection=None,
++        msg=None,
++        retries=None,
++        enforce_content_length=False,
++        request_method=None,
++        request_url=None,
++        auto_close=True,
++    ):
++
++        if isinstance(headers, HTTPHeaderDict):
++            self.headers = headers
++        else:
++            self.headers = HTTPHeaderDict(headers)
++        self.status = status
++        self.version = version
++        self.reason = reason
++        self.strict = strict
++        self.decode_content = decode_content
++        self.retries = retries
++        self.enforce_content_length = enforce_content_length
++        self.auto_close = auto_close
++
++        self._decoder = None
++        self._body = None
++        self._fp = None
++        self._original_response = original_response
++        self._fp_bytes_read = 0
++        self.msg = msg
++        self._request_url = request_url
++
++        if body and isinstance(body, (basestring, bytes)):
++            self._body = body
++
++        self._pool = pool
++        self._connection = connection
++
++        if hasattr(body, "read"):
++            self._fp = body
++
++        # Are we using the chunked-style of transfer encoding?
++        self.chunked = False
++        self.chunk_left = None
++        tr_enc = self.headers.get("transfer-encoding", "").lower()
++        # Don't incur the penalty of creating a list and then discarding it
++        encodings = (enc.strip() for enc in tr_enc.split(","))
++        if "chunked" in encodings:
++            self.chunked = True
++
++        # Determine length of response
++        self.length_remaining = self._init_length(request_method)
++
++        # If requested, preload the body.
++        if preload_content and not self._body:
++            self._body = self.read(decode_content=decode_content)
++
++    def get_redirect_location(self):
++        """
++        Should we redirect and where to?
++
++        :returns: Truthy redirect location string if we got a redirect status
++            code and valid location. ``None`` if redirect status and no
++            location. ``False`` if not a redirect status code.
++        """
++        if self.status in self.REDIRECT_STATUSES:
++            return self.headers.get("location")
++
++        return False
++
++    def release_conn(self):
++        if not self._pool or not self._connection:
++            return
++
++        self._pool._put_conn(self._connection)
++        self._connection = None
++
++    @property
++    def data(self):
++        # For backwords-compat with earlier urllib3 0.4 and earlier.
++        if self._body:
++            return self._body
++
++        if self._fp:
++            return self.read(cache_content=True)
++
++    @property
++    def connection(self):
++        return self._connection
++
++    def isclosed(self):
++        return is_fp_closed(self._fp)
++
++    def tell(self):
++        """
++        Obtain the number of bytes pulled over the wire so far. May differ from
++        the amount of content returned by :meth:``HTTPResponse.read`` if bytes
++        are encoded on the wire (e.g, compressed).
++        """
++        return self._fp_bytes_read
++
++    def _init_length(self, request_method):
++        """
++        Set initial length value for Response content if available.
++        """
++        length = self.headers.get("content-length")
++
++        if length is not None:
++            if self.chunked:
++                # This Response will fail with an IncompleteRead if it can't be
++                # received as chunked. This method falls back to attempt reading
++                # the response before raising an exception.
++                log.warning(
++                    "Received response with both Content-Length and "
++                    "Transfer-Encoding set. This is expressly forbidden "
++                    "by RFC 7230 sec 3.3.2. Ignoring Content-Length and "
++                    "attempting to process response as Transfer-Encoding: "
++                    "chunked."
++                )
++                return None
++
++            try:
++                # RFC 7230 section 3.3.2 specifies multiple content lengths can
++                # be sent in a single Content-Length header
++                # (e.g. Content-Length: 42, 42). This line ensures the values
++                # are all valid ints and that as long as the `set` length is 1,
++                # all values are the same. Otherwise, the header is invalid.
++                lengths = set([int(val) for val in length.split(",")])
++                if len(lengths) > 1:
++                    raise InvalidHeader(
++                        "Content-Length contained multiple "
++                        "unmatching values (%s)" % length
++                    )
++                length = lengths.pop()
++            except ValueError:
++                length = None
++            else:
++                if length < 0:
++                    length = None
++
++        # Convert status to int for comparison
++        # In some cases, httplib returns a status of "_UNKNOWN"
++        try:
++            status = int(self.status)
++        except ValueError:
++            status = 0
++
++        # Check for responses that shouldn't include a body
++        if status in (204, 304) or 100 <= status < 200 or request_method == "HEAD":
++            length = 0
++
++        return length
++
++    def _init_decoder(self):
++        """
++        Set-up the _decoder attribute if necessary.
++        """
++        # Note: content-encoding value should be case-insensitive, per RFC 7230
++        # Section 3.2
++        content_encoding = self.headers.get("content-encoding", "").lower()
++        if self._decoder is None:
++            if content_encoding in self.CONTENT_DECODERS:
++                self._decoder = _get_decoder(content_encoding)
++            elif "," in content_encoding:
++                encodings = [
++                    e.strip()
++                    for e in content_encoding.split(",")
++                    if e.strip() in self.CONTENT_DECODERS
++                ]
++                if len(encodings):
++                    self._decoder = _get_decoder(content_encoding)
++
++    DECODER_ERROR_CLASSES = (IOError, zlib.error)
++    if brotli is not None:
++        DECODER_ERROR_CLASSES += (brotli.error,)
++
++    def _decode(self, data, decode_content, flush_decoder):
++        """
++        Decode the data passed in and potentially flush the decoder.
++        """
++        if not decode_content:
++            return data
++
++        try:
++            if self._decoder:
++                data = self._decoder.decompress(data)
++        except self.DECODER_ERROR_CLASSES as e:
++            content_encoding = self.headers.get("content-encoding", "").lower()
++            raise DecodeError(
++                "Received response with content-encoding: %s, but "
++                "failed to decode it." % content_encoding,
++                e,
++            )
++        if flush_decoder:
++            data += self._flush_decoder()
++
++        return data
++
++    def _flush_decoder(self):
++        """
++        Flushes the decoder. Should only be called if the decoder is actually
++        being used.
++        """
++        if self._decoder:
++            buf = self._decoder.decompress(b"")
++            return buf + self._decoder.flush()
++
++        return b""
++
++    @contextmanager
++    def _error_catcher(self):
++        """
++        Catch low-level python exceptions, instead re-raising urllib3
++        variants, so that low-level exceptions are not leaked in the
++        high-level api.
++
++        On exit, release the connection back to the pool.
++        """
++        clean_exit = False
++
++        try:
++            try:
++                yield
++
++            except SocketTimeout:
++                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
++                # there is yet no clean way to get at it from this context.
++                raise ReadTimeoutError(self._pool, None, "Read timed out.")
++
++            except BaseSSLError as e:
++                # FIXME: Is there a better way to differentiate between SSLErrors?
++                if "read operation timed out" not in str(e):  # Defensive:
++                    # This shouldn't happen but just in case we're missing an edge
++                    # case, let's avoid swallowing SSL errors.
++                    raise
++
++                raise ReadTimeoutError(self._pool, None, "Read timed out.")
++
++            except (HTTPException, SocketError) as e:
++                # This includes IncompleteRead.
++                raise ProtocolError("Connection broken: %r" % e, e)
++
++            # If no exception is thrown, we should avoid cleaning up
++            # unnecessarily.
++            clean_exit = True
++        finally:
++            # If we didn't terminate cleanly, we need to throw away our
++            # connection.
++            if not clean_exit:
++                # The response may not be closed but we're not going to use it
++                # anymore so close it now to ensure that the connection is
++                # released back to the pool.
++                if self._original_response:
++                    self._original_response.close()
++
++                # Closing the response may not actually be sufficient to close
++                # everything, so if we have a hold of the connection close that
++                # too.
++                if self._connection:
++                    self._connection.close()
++
++            # If we hold the original response but it's closed now, we should
++            # return the connection back to the pool.
++            if self._original_response and self._original_response.isclosed():
++                self.release_conn()
++
++    def read(self, amt=None, decode_content=None, cache_content=False):
++        """
++        Similar to :meth:`httplib.HTTPResponse.read`, but with two additional
++        parameters: ``decode_content`` and ``cache_content``.
++
++        :param amt:
++            How much of the content to read. If specified, caching is skipped
++            because it doesn't make sense to cache partial content as the full
++            response.
++
++        :param decode_content:
++            If True, will attempt to decode the body based on the
++            'content-encoding' header.
++
++        :param cache_content:
++            If True, will save the returned data such that the same result is
++            returned despite of the state of the underlying file object. This
++            is useful if you want the ``.data`` property to continue working
++            after having ``.read()`` the file object. (Overridden if ``amt`` is
++            set.)
++        """
++        self._init_decoder()
++        if decode_content is None:
++            decode_content = self.decode_content
++
++        if self._fp is None:
++            return
++
++        flush_decoder = False
++        fp_closed = getattr(self._fp, "closed", False)
++
++        with self._error_catcher():
++            if amt is None:
++                # cStringIO doesn't like amt=None
++                data = self._fp.read() if not fp_closed else b""
++                flush_decoder = True
++            else:
++                cache_content = False
++                data = self._fp.read(amt) if not fp_closed else b""
++                if (
++                    amt != 0 and not data
++                ):  # Platform-specific: Buggy versions of Python.
++                    # Close the connection when no data is returned
++                    #
++                    # This is redundant to what httplib/http.client _should_
++                    # already do.  However, versions of python released before
++                    # December 15, 2012 (http://bugs.python.org/issue16298) do
++                    # not properly close the connection in all cases. There is
++                    # no harm in redundantly calling close.
++                    self._fp.close()
++                    flush_decoder = True
++                    if self.enforce_content_length and self.length_remaining not in (
++                        0,
++                        None,
++                    ):
++                        # This is an edge case that httplib failed to cover due
++                        # to concerns of backward compatibility. We're
++                        # addressing it here to make sure IncompleteRead is
++                        # raised during streaming, so all calls with incorrect
++                        # Content-Length are caught.
++                        raise IncompleteRead(self._fp_bytes_read, self.length_remaining)
++
++        if data:
++            self._fp_bytes_read += len(data)
++            if self.length_remaining is not None:
++                self.length_remaining -= len(data)
++
++            data = self._decode(data, decode_content, flush_decoder)
++
++            if cache_content:
++                self._body = data
++
++        return data
++
++    def stream(self, amt=2 ** 16, decode_content=None):
++        """
++        A generator wrapper for the read() method. A call will block until
++        ``amt`` bytes have been read from the connection or until the
++        connection is closed.
++
++        :param amt:
++            How much of the content to read. The generator will return up to
++            much data per iteration, but may return less. This is particularly
++            likely when using compressed data. However, the empty string will
++            never be returned.
++
++        :param decode_content:
++            If True, will attempt to decode the body based on the
++            'content-encoding' header.
++        """
++        if self.chunked and self.supports_chunked_reads():
++            for line in self.read_chunked(amt, decode_content=decode_content):
++                yield line
++        else:
++            while not is_fp_closed(self._fp):
++                data = self.read(amt=amt, decode_content=decode_content)
++
++                if data:
++                    yield data
++
++    @classmethod
++    def from_httplib(ResponseCls, r, **response_kw):
++        """
++        Given an :class:`httplib.HTTPResponse` instance ``r``, return a
++        corresponding :class:`urllib3.response.HTTPResponse` object.
++
++        Remaining parameters are passed to the HTTPResponse constructor, along
++        with ``original_response=r``.
++        """
++        headers = r.msg
++
++        if not isinstance(headers, HTTPHeaderDict):
++            if PY3:
++                headers = HTTPHeaderDict(headers.items())
++            else:
++                # Python 2.7
++                headers = HTTPHeaderDict.from_httplib(headers)
++
++        # HTTPResponse objects in Python 3 don't have a .strict attribute
++        strict = getattr(r, "strict", 0)
++        resp = ResponseCls(
++            body=r,
++            headers=headers,
++            status=r.status,
++            version=r.version,
++            reason=r.reason,
++            strict=strict,
++            original_response=r,
++            **response_kw
++        )
++        return resp
++
++    # Backwards-compatibility methods for httplib.HTTPResponse
++    def getheaders(self):
++        return self.headers
++
++    def getheader(self, name, default=None):
++        return self.headers.get(name, default)
++
++    # Backwards compatibility for http.cookiejar
++    def info(self):
++        return self.headers
++
++    # Overrides from io.IOBase
++    def close(self):
++        if not self.closed:
++            self._fp.close()
++
++        if self._connection:
++            self._connection.close()
++
++        if not self.auto_close:
++            io.IOBase.close(self)
++
++    @property
++    def closed(self):
++        if not self.auto_close:
++            return io.IOBase.closed.__get__(self)
++        elif self._fp is None:
++            return True
++        elif hasattr(self._fp, "isclosed"):
++            return self._fp.isclosed()
++        elif hasattr(self._fp, "closed"):
++            return self._fp.closed
++        else:
++            return True
++
++    def fileno(self):
++        if self._fp is None:
++            raise IOError("HTTPResponse has no file to get a fileno from")
++        elif hasattr(self._fp, "fileno"):
++            return self._fp.fileno()
++        else:
++            raise IOError(
++                "The file-like object this HTTPResponse is wrapped "
++                "around has no file descriptor"
++            )
++
++    def flush(self):
++        if (
++            self._fp is not None
++            and hasattr(self._fp, "flush")
++            and not getattr(self._fp, "closed", False)
++        ):
++            return self._fp.flush()
++
++    def readable(self):
++        # This method is required for `io` module compatibility.
++        return True
++
++    def readinto(self, b):
++        # This method is required for `io` module compatibility.
++        temp = self.read(len(b))
++        if len(temp) == 0:
++            return 0
++        else:
++            b[: len(temp)] = temp
++            return len(temp)
++
++    def supports_chunked_reads(self):
++        """
++        Checks if the underlying file-like object looks like a
++        httplib.HTTPResponse object. We do this by testing for the fp
++        attribute. If it is present we assume it returns raw chunks as
++        processed by read_chunked().
++        """
++        return hasattr(self._fp, "fp")
++
++    def _update_chunk_length(self):
++        # First, we'll figure out length of a chunk and then
++        # we'll try to read it from socket.
++        if self.chunk_left is not None:
++            return
++        line = self._fp.fp.readline()
++        line = line.split(b";", 1)[0]
++        try:
++            self.chunk_left = int(line, 16)
++        except ValueError:
++            # Invalid chunked protocol response, abort.
++            self.close()
++            raise httplib.IncompleteRead(line)
++
++    def _handle_chunk(self, amt):
++        returned_chunk = None
++        if amt is None:
++            chunk = self._fp._safe_read(self.chunk_left)
++            returned_chunk = chunk
++            self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
++            self.chunk_left = None
++        elif amt < self.chunk_left:
++            value = self._fp._safe_read(amt)
++            self.chunk_left = self.chunk_left - amt
++            returned_chunk = value
++        elif amt == self.chunk_left:
++            value = self._fp._safe_read(amt)
++            self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
++            self.chunk_left = None
++            returned_chunk = value
++        else:  # amt > self.chunk_left
++            returned_chunk = self._fp._safe_read(self.chunk_left)
++            self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
++            self.chunk_left = None
++        return returned_chunk
++
++    def read_chunked(self, amt=None, decode_content=None):
++        """
++        Similar to :meth:`HTTPResponse.read`, but with an additional
++        parameter: ``decode_content``.
++
++        :param amt:
++            How much of the content to read. If specified, caching is skipped
++            because it doesn't make sense to cache partial content as the full
++            response.
++
++        :param decode_content:
++            If True, will attempt to decode the body based on the
++            'content-encoding' header.
++        """
++        self._init_decoder()
++        # FIXME: Rewrite this method and make it a class with a better structured logic.
++        if not self.chunked:
++            raise ResponseNotChunked(
++                "Response is not chunked. "
++                "Header 'transfer-encoding: chunked' is missing."
++            )
++        if not self.supports_chunked_reads():
++            raise BodyNotHttplibCompatible(
++                "Body should be httplib.HTTPResponse like. "
++                "It should have have an fp attribute which returns raw chunks."
++            )
++
++        with self._error_catcher():
++            # Don't bother reading the body of a HEAD request.
++            if self._original_response and is_response_to_head(self._original_response):
++                self._original_response.close()
++                return
++
++            # If a response is already read and closed
++            # then return immediately.
++            if self._fp.fp is None:
++                return
++
++            while True:
++                self._update_chunk_length()
++                if self.chunk_left == 0:
++                    break
++                chunk = self._handle_chunk(amt)
++                decoded = self._decode(
++                    chunk, decode_content=decode_content, flush_decoder=False
++                )
++                if decoded:
++                    yield decoded
++
++            if decode_content:
++                # On CPython and PyPy, we should never need to flush the
++                # decoder. However, on Jython we *might* need to, so
++                # lets defensively do it anyway.
++                decoded = self._flush_decoder()
++                if decoded:  # Platform-specific: Jython.
++                    yield decoded
++
++            # Chunk content ends with \r\n: discard it.
++            while True:
++                line = self._fp.fp.readline()
++                if not line:
++                    # Some sites may not end with '\r\n'.
++                    break
++                if line == b"\r\n":
++                    break
++
++            # We read everything; close the "file".
++            if self._original_response:
++                self._original_response.close()
++
++    def geturl(self):
++        """
++        Returns the URL that was the source of this response.
++        If the request that generated this response redirected, this method
++        will return the final redirect location.
++        """
++        if self.retries is not None and len(self.retries.history):
++            return self.retries.history[-1].redirect_location
++        else:
++            return self._request_url
++
++    def __iter__(self):
++        buffer = [b""]
++        for chunk in self.stream(decode_content=True):
++            if b"\n" in chunk:
++                chunk = chunk.split(b"\n")
++                yield b"".join(buffer) + chunk[0] + b"\n"
++                for x in chunk[1:-1]:
++                    yield x + b"\n"
++                if chunk[-1]:
++                    buffer = [chunk[-1]]
++                else:
++                    buffer = []
++            else:
++                buffer.append(chunk)
++        if buffer:
++            yield b"".join(buffer)
+Index: venv/Lib/site-packages/urllib3/connection.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/connection.py	(date 1573549849385)
++++ venv/Lib/site-packages/urllib3/connection.py	(date 1573549849385)
+@@ -0,0 +1,448 @@
++from __future__ import absolute_import
++import datetime
++import logging
++import os
++import socket
++from socket import error as SocketError, timeout as SocketTimeout
++import warnings
++from .packages import six
++from .packages.six.moves.http_client import HTTPConnection as _HTTPConnection
++from .packages.six.moves.http_client import HTTPException  # noqa: F401
++
++try:  # Compiled with SSL?
++    import ssl
++
++    BaseSSLError = ssl.SSLError
++except (ImportError, AttributeError):  # Platform-specific: No SSL.
++    ssl = None
++
++    class BaseSSLError(BaseException):
++        pass
++
++
++try:
++    # Python 3: not a no-op, we're adding this to the namespace so it can be imported.
++    ConnectionError = ConnectionError
++except NameError:
++    # Python 2
++    class ConnectionError(Exception):
++        pass
++
++
++from .exceptions import (
++    NewConnectionError,
++    ConnectTimeoutError,
++    SubjectAltNameWarning,
++    SystemTimeWarning,
++)
++from .packages.ssl_match_hostname import match_hostname, CertificateError
++
++from .util.ssl_ import (
++    resolve_cert_reqs,
++    resolve_ssl_version,
++    assert_fingerprint,
++    create_urllib3_context,
++    ssl_wrap_socket,
++)
++
++
++from .util import connection
++
++from ._collections import HTTPHeaderDict
++
++log = logging.getLogger(__name__)
++
++port_by_scheme = {"http": 80, "https": 443}
++
++# When it comes time to update this value as a part of regular maintenance
++# (ie test_recent_date is failing) update it to ~6 months before the current date.
++RECENT_DATE = datetime.date(2019, 1, 1)
++
++
++class DummyConnection(object):
++    """Used to detect a failed ConnectionCls import."""
++
++    pass
++
++
++class HTTPConnection(_HTTPConnection, object):
++    """
++    Based on httplib.HTTPConnection but provides an extra constructor
++    backwards-compatibility layer between older and newer Pythons.
++
++    Additional keyword parameters are used to configure attributes of the connection.
++    Accepted parameters include:
++
++      - ``strict``: See the documentation on :class:`urllib3.connectionpool.HTTPConnectionPool`
++      - ``source_address``: Set the source address for the current connection.
++      - ``socket_options``: Set specific options on the underlying socket. If not specified, then
++        defaults are loaded from ``HTTPConnection.default_socket_options`` which includes disabling
++        Nagle's algorithm (sets TCP_NODELAY to 1) unless the connection is behind a proxy.
++
++        For example, if you wish to enable TCP Keep Alive in addition to the defaults,
++        you might pass::
++
++            HTTPConnection.default_socket_options + [
++                (socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1),
++            ]
++
++        Or you may want to disable the defaults by passing an empty list (e.g., ``[]``).
++    """
++
++    default_port = port_by_scheme["http"]
++
++    #: Disable Nagle's algorithm by default.
++    #: ``[(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]``
++    default_socket_options = [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)]
++
++    #: Whether this connection verifies the host's certificate.
++    is_verified = False
++
++    def __init__(self, *args, **kw):
++        if not six.PY2:
++            kw.pop("strict", None)
++
++        # Pre-set source_address.
++        self.source_address = kw.get("source_address")
++
++        #: The socket options provided by the user. If no options are
++        #: provided, we use the default options.
++        self.socket_options = kw.pop("socket_options", self.default_socket_options)
++
++        _HTTPConnection.__init__(self, *args, **kw)
++
++    @property
++    def host(self):
++        """
++        Getter method to remove any trailing dots that indicate the hostname is an FQDN.
++
++        In general, SSL certificates don't include the trailing dot indicating a
++        fully-qualified domain name, and thus, they don't validate properly when
++        checked against a domain name that includes the dot. In addition, some
++        servers may not expect to receive the trailing dot when provided.
++
++        However, the hostname with trailing dot is critical to DNS resolution; doing a
++        lookup with the trailing dot will properly only resolve the appropriate FQDN,
++        whereas a lookup without a trailing dot will search the system's search domain
++        list. Thus, it's important to keep the original host around for use only in
++        those cases where it's appropriate (i.e., when doing DNS lookup to establish the
++        actual TCP connection across which we're going to send HTTP requests).
++        """
++        return self._dns_host.rstrip(".")
++
++    @host.setter
++    def host(self, value):
++        """
++        Setter for the `host` property.
++
++        We assume that only urllib3 uses the _dns_host attribute; httplib itself
++        only uses `host`, and it seems reasonable that other libraries follow suit.
++        """
++        self._dns_host = value
++
++    def _new_conn(self):
++        """ Establish a socket connection and set nodelay settings on it.
++
++        :return: New socket connection.
++        """
++        extra_kw = {}
++        if self.source_address:
++            extra_kw["source_address"] = self.source_address
++
++        if self.socket_options:
++            extra_kw["socket_options"] = self.socket_options
++
++        try:
++            conn = connection.create_connection(
++                (self._dns_host, self.port), self.timeout, **extra_kw
++            )
++
++        except SocketTimeout:
++            raise ConnectTimeoutError(
++                self,
++                "Connection to %s timed out. (connect timeout=%s)"
++                % (self.host, self.timeout),
++            )
++
++        except SocketError as e:
++            raise NewConnectionError(
++                self, "Failed to establish a new connection: %s" % e
++            )
++
++        return conn
++
++    def _prepare_conn(self, conn):
++        self.sock = conn
++        # Google App Engine's httplib does not define _tunnel_host
++        if getattr(self, "_tunnel_host", None):
++            # TODO: Fix tunnel so it doesn't depend on self.sock state.
++            self._tunnel()
++            # Mark this connection as not reusable
++            self.auto_open = 0
++
++    def connect(self):
++        conn = self._new_conn()
++        self._prepare_conn(conn)
++
++    def request_chunked(self, method, url, body=None, headers=None):
++        """
++        Alternative to the common request method, which sends the
++        body with chunked encoding and not as one block
++        """
++        headers = HTTPHeaderDict(headers if headers is not None else {})
++        skip_accept_encoding = "accept-encoding" in headers
++        skip_host = "host" in headers
++        self.putrequest(
++            method, url, skip_accept_encoding=skip_accept_encoding, skip_host=skip_host
++        )
++        for header, value in headers.items():
++            self.putheader(header, value)
++        if "transfer-encoding" not in headers:
++            self.putheader("Transfer-Encoding", "chunked")
++        self.endheaders()
++
++        if body is not None:
++            stringish_types = six.string_types + (bytes,)
++            if isinstance(body, stringish_types):
++                body = (body,)
++            for chunk in body:
++                if not chunk:
++                    continue
++                if not isinstance(chunk, bytes):
++                    chunk = chunk.encode("utf8")
++                len_str = hex(len(chunk))[2:]
++                self.send(len_str.encode("utf-8"))
++                self.send(b"\r\n")
++                self.send(chunk)
++                self.send(b"\r\n")
++
++        # After the if clause, to always have a closed body
++        self.send(b"0\r\n\r\n")
++
++
++class HTTPSConnection(HTTPConnection):
++    default_port = port_by_scheme["https"]
++
++    ssl_version = None
++
++    def __init__(
++        self,
++        host,
++        port=None,
++        key_file=None,
++        cert_file=None,
++        key_password=None,
++        strict=None,
++        timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
++        ssl_context=None,
++        server_hostname=None,
++        **kw
++    ):
++
++        HTTPConnection.__init__(self, host, port, strict=strict, timeout=timeout, **kw)
++
++        self.key_file = key_file
++        self.cert_file = cert_file
++        self.key_password = key_password
++        self.ssl_context = ssl_context
++        self.server_hostname = server_hostname
++
++        # Required property for Google AppEngine 1.9.0 which otherwise causes
++        # HTTPS requests to go out as HTTP. (See Issue #356)
++        self._protocol = "https"
++
++    def connect(self):
++        conn = self._new_conn()
++        self._prepare_conn(conn)
++
++        # Wrap socket using verification with the root certs in
++        # trusted_root_certs
++        default_ssl_context = False
++        if self.ssl_context is None:
++            default_ssl_context = True
++            self.ssl_context = create_urllib3_context(
++                ssl_version=resolve_ssl_version(self.ssl_version),
++                cert_reqs=resolve_cert_reqs(self.cert_reqs),
++            )
++
++        # Try to load OS default certs if none are given.
++        # Works well on Windows (requires Python3.4+)
++        context = self.ssl_context
++        if (
++            not self.ca_certs
++            and not self.ca_cert_dir
++            and default_ssl_context
++            and hasattr(context, "load_default_certs")
++        ):
++            context.load_default_certs()
++
++        self.sock = ssl_wrap_socket(
++            sock=conn,
++            keyfile=self.key_file,
++            certfile=self.cert_file,
++            key_password=self.key_password,
++            ssl_context=self.ssl_context,
++            server_hostname=self.server_hostname,
++        )
++
++
++class VerifiedHTTPSConnection(HTTPSConnection):
++    """
++    Based on httplib.HTTPSConnection but wraps the socket with
++    SSL certification.
++    """
++
++    cert_reqs = None
++    ca_certs = None
++    ca_cert_dir = None
++    ssl_version = None
++    assert_fingerprint = None
++
++    def set_cert(
++        self,
++        key_file=None,
++        cert_file=None,
++        cert_reqs=None,
++        key_password=None,
++        ca_certs=None,
++        assert_hostname=None,
++        assert_fingerprint=None,
++        ca_cert_dir=None,
++    ):
++        """
++        This method should only be called once, before the connection is used.
++        """
++        # If cert_reqs is not provided we'll assume CERT_REQUIRED unless we also
++        # have an SSLContext object in which case we'll use its verify_mode.
++        if cert_reqs is None:
++            if self.ssl_context is not None:
++                cert_reqs = self.ssl_context.verify_mode
++            else:
++                cert_reqs = resolve_cert_reqs(None)
++
++        self.key_file = key_file
++        self.cert_file = cert_file
++        self.cert_reqs = cert_reqs
++        self.key_password = key_password
++        self.assert_hostname = assert_hostname
++        self.assert_fingerprint = assert_fingerprint
++        self.ca_certs = ca_certs and os.path.expanduser(ca_certs)
++        self.ca_cert_dir = ca_cert_dir and os.path.expanduser(ca_cert_dir)
++
++    def connect(self):
++        # Add certificate verification
++        conn = self._new_conn()
++        hostname = self.host
++
++        # Google App Engine's httplib does not define _tunnel_host
++        if getattr(self, "_tunnel_host", None):
++            self.sock = conn
++            # Calls self._set_hostport(), so self.host is
++            # self._tunnel_host below.
++            self._tunnel()
++            # Mark this connection as not reusable
++            self.auto_open = 0
++
++            # Override the host with the one we're requesting data from.
++            hostname = self._tunnel_host
++
++        server_hostname = hostname
++        if self.server_hostname is not None:
++            server_hostname = self.server_hostname
++
++        is_time_off = datetime.date.today() < RECENT_DATE
++        if is_time_off:
++            warnings.warn(
++                (
++                    "System time is way off (before {0}). This will probably "
++                    "lead to SSL verification errors"
++                ).format(RECENT_DATE),
++                SystemTimeWarning,
++            )
++
++        # Wrap socket using verification with the root certs in
++        # trusted_root_certs
++        default_ssl_context = False
++        if self.ssl_context is None:
++            default_ssl_context = True
++            self.ssl_context = create_urllib3_context(
++                ssl_version=resolve_ssl_version(self.ssl_version),
++                cert_reqs=resolve_cert_reqs(self.cert_reqs),
++            )
++
++        context = self.ssl_context
++        context.verify_mode = resolve_cert_reqs(self.cert_reqs)
++
++        # Try to load OS default certs if none are given.
++        # Works well on Windows (requires Python3.4+)
++        if (
++            not self.ca_certs
++            and not self.ca_cert_dir
++            and default_ssl_context
++            and hasattr(context, "load_default_certs")
++        ):
++            context.load_default_certs()
++
++        self.sock = ssl_wrap_socket(
++            sock=conn,
++            keyfile=self.key_file,
++            certfile=self.cert_file,
++            key_password=self.key_password,
++            ca_certs=self.ca_certs,
++            ca_cert_dir=self.ca_cert_dir,
++            server_hostname=server_hostname,
++            ssl_context=context,
++        )
++
++        if self.assert_fingerprint:
++            assert_fingerprint(
++                self.sock.getpeercert(binary_form=True), self.assert_fingerprint
++            )
++        elif (
++            context.verify_mode != ssl.CERT_NONE
++            and not getattr(context, "check_hostname", False)
++            and self.assert_hostname is not False
++        ):
++            # While urllib3 attempts to always turn off hostname matching from
++            # the TLS library, this cannot always be done. So we check whether
++            # the TLS Library still thinks it's matching hostnames.
++            cert = self.sock.getpeercert()
++            if not cert.get("subjectAltName", ()):
++                warnings.warn(
++                    (
++                        "Certificate for {0} has no `subjectAltName`, falling back to check for a "
++                        "`commonName` for now. This feature is being removed by major browsers and "
++                        "deprecated by RFC 2818. (See https://github.com/urllib3/urllib3/issues/497 "
++                        "for details.)".format(hostname)
++                    ),
++                    SubjectAltNameWarning,
++                )
++            _match_hostname(cert, self.assert_hostname or server_hostname)
++
++        self.is_verified = (
++            context.verify_mode == ssl.CERT_REQUIRED
++            or self.assert_fingerprint is not None
++        )
++
++
++def _match_hostname(cert, asserted_hostname):
++    try:
++        match_hostname(cert, asserted_hostname)
++    except CertificateError as e:
++        log.warning(
++            "Certificate did not match expected hostname: %s. Certificate: %s",
++            asserted_hostname,
++            cert,
++        )
++        # Add cert to exception and reraise so client code can inspect
++        # the cert when catching the exception, if they want to
++        e._peer_cert = cert
++        raise
++
++
++if ssl:
++    # Make a copy for testing.
++    UnverifiedHTTPSConnection = HTTPSConnection
++    HTTPSConnection = VerifiedHTTPSConnection
++else:
++    HTTPSConnection = DummyConnection
+Index: venv/Lib/site-packages/urllib3/poolmanager.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/poolmanager.py	(date 1573549849412)
++++ venv/Lib/site-packages/urllib3/poolmanager.py	(date 1573549849412)
+@@ -0,0 +1,470 @@
++from __future__ import absolute_import
++import collections
++import functools
++import logging
++
++from ._collections import RecentlyUsedContainer
++from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool
++from .connectionpool import port_by_scheme
++from .exceptions import LocationValueError, MaxRetryError, ProxySchemeUnknown
++from .packages import six
++from .packages.six.moves.urllib.parse import urljoin
++from .request import RequestMethods
++from .util.url import parse_url
++from .util.retry import Retry
++
++
++__all__ = ["PoolManager", "ProxyManager", "proxy_from_url"]
++
++
++log = logging.getLogger(__name__)
++
++SSL_KEYWORDS = (
++    "key_file",
++    "cert_file",
++    "cert_reqs",
++    "ca_certs",
++    "ssl_version",
++    "ca_cert_dir",
++    "ssl_context",
++    "key_password",
++)
++
++# All known keyword arguments that could be provided to the pool manager, its
++# pools, or the underlying connections. This is used to construct a pool key.
++_key_fields = (
++    "key_scheme",  # str
++    "key_host",  # str
++    "key_port",  # int
++    "key_timeout",  # int or float or Timeout
++    "key_retries",  # int or Retry
++    "key_strict",  # bool
++    "key_block",  # bool
++    "key_source_address",  # str
++    "key_key_file",  # str
++    "key_key_password",  # str
++    "key_cert_file",  # str
++    "key_cert_reqs",  # str
++    "key_ca_certs",  # str
++    "key_ssl_version",  # str
++    "key_ca_cert_dir",  # str
++    "key_ssl_context",  # instance of ssl.SSLContext or urllib3.util.ssl_.SSLContext
++    "key_maxsize",  # int
++    "key_headers",  # dict
++    "key__proxy",  # parsed proxy url
++    "key__proxy_headers",  # dict
++    "key_socket_options",  # list of (level (int), optname (int), value (int or str)) tuples
++    "key__socks_options",  # dict
++    "key_assert_hostname",  # bool or string
++    "key_assert_fingerprint",  # str
++    "key_server_hostname",  # str
++)
++
++#: The namedtuple class used to construct keys for the connection pool.
++#: All custom key schemes should include the fields in this key at a minimum.
++PoolKey = collections.namedtuple("PoolKey", _key_fields)
++
++
++def _default_key_normalizer(key_class, request_context):
++    """
++    Create a pool key out of a request context dictionary.
++
++    According to RFC 3986, both the scheme and host are case-insensitive.
++    Therefore, this function normalizes both before constructing the pool
++    key for an HTTPS request. If you wish to change this behaviour, provide
++    alternate callables to ``key_fn_by_scheme``.
++
++    :param key_class:
++        The class to use when constructing the key. This should be a namedtuple
++        with the ``scheme`` and ``host`` keys at a minimum.
++    :type  key_class: namedtuple
++    :param request_context:
++        A dictionary-like object that contain the context for a request.
++    :type  request_context: dict
++
++    :return: A namedtuple that can be used as a connection pool key.
++    :rtype:  PoolKey
++    """
++    # Since we mutate the dictionary, make a copy first
++    context = request_context.copy()
++    context["scheme"] = context["scheme"].lower()
++    context["host"] = context["host"].lower()
++
++    # These are both dictionaries and need to be transformed into frozensets
++    for key in ("headers", "_proxy_headers", "_socks_options"):
++        if key in context and context[key] is not None:
++            context[key] = frozenset(context[key].items())
++
++    # The socket_options key may be a list and needs to be transformed into a
++    # tuple.
++    socket_opts = context.get("socket_options")
++    if socket_opts is not None:
++        context["socket_options"] = tuple(socket_opts)
++
++    # Map the kwargs to the names in the namedtuple - this is necessary since
++    # namedtuples can't have fields starting with '_'.
++    for key in list(context.keys()):
++        context["key_" + key] = context.pop(key)
++
++    # Default to ``None`` for keys missing from the context
++    for field in key_class._fields:
++        if field not in context:
++            context[field] = None
++
++    return key_class(**context)
++
++
++#: A dictionary that maps a scheme to a callable that creates a pool key.
++#: This can be used to alter the way pool keys are constructed, if desired.
++#: Each PoolManager makes a copy of this dictionary so they can be configured
++#: globally here, or individually on the instance.
++key_fn_by_scheme = {
++    "http": functools.partial(_default_key_normalizer, PoolKey),
++    "https": functools.partial(_default_key_normalizer, PoolKey),
++}
++
++pool_classes_by_scheme = {"http": HTTPConnectionPool, "https": HTTPSConnectionPool}
++
++
++class PoolManager(RequestMethods):
++    """
++    Allows for arbitrary requests while transparently keeping track of
++    necessary connection pools for you.
++
++    :param num_pools:
++        Number of connection pools to cache before discarding the least
++        recently used pool.
++
++    :param headers:
++        Headers to include with all requests, unless other headers are given
++        explicitly.
++
++    :param \\**connection_pool_kw:
++        Additional parameters are used to create fresh
++        :class:`urllib3.connectionpool.ConnectionPool` instances.
++
++    Example::
++
++        >>> manager = PoolManager(num_pools=2)
++        >>> r = manager.request('GET', 'http://google.com/')
++        >>> r = manager.request('GET', 'http://google.com/mail')
++        >>> r = manager.request('GET', 'http://yahoo.com/')
++        >>> len(manager.pools)
++        2
++
++    """
++
++    proxy = None
++
++    def __init__(self, num_pools=10, headers=None, **connection_pool_kw):
++        RequestMethods.__init__(self, headers)
++        self.connection_pool_kw = connection_pool_kw
++        self.pools = RecentlyUsedContainer(num_pools, dispose_func=lambda p: p.close())
++
++        # Locally set the pool classes and keys so other PoolManagers can
++        # override them.
++        self.pool_classes_by_scheme = pool_classes_by_scheme
++        self.key_fn_by_scheme = key_fn_by_scheme.copy()
++
++    def __enter__(self):
++        return self
++
++    def __exit__(self, exc_type, exc_val, exc_tb):
++        self.clear()
++        # Return False to re-raise any potential exceptions
++        return False
++
++    def _new_pool(self, scheme, host, port, request_context=None):
++        """
++        Create a new :class:`ConnectionPool` based on host, port, scheme, and
++        any additional pool keyword arguments.
++
++        If ``request_context`` is provided, it is provided as keyword arguments
++        to the pool class used. This method is used to actually create the
++        connection pools handed out by :meth:`connection_from_url` and
++        companion methods. It is intended to be overridden for customization.
++        """
++        pool_cls = self.pool_classes_by_scheme[scheme]
++        if request_context is None:
++            request_context = self.connection_pool_kw.copy()
++
++        # Although the context has everything necessary to create the pool,
++        # this function has historically only used the scheme, host, and port
++        # in the positional args. When an API change is acceptable these can
++        # be removed.
++        for key in ("scheme", "host", "port"):
++            request_context.pop(key, None)
++
++        if scheme == "http":
++            for kw in SSL_KEYWORDS:
++                request_context.pop(kw, None)
++
++        return pool_cls(host, port, **request_context)
++
++    def clear(self):
++        """
++        Empty our store of pools and direct them all to close.
++
++        This will not affect in-flight connections, but they will not be
++        re-used after completion.
++        """
++        self.pools.clear()
++
++    def connection_from_host(self, host, port=None, scheme="http", pool_kwargs=None):
++        """
++        Get a :class:`ConnectionPool` based on the host, port, and scheme.
++
++        If ``port`` isn't given, it will be derived from the ``scheme`` using
++        ``urllib3.connectionpool.port_by_scheme``. If ``pool_kwargs`` is
++        provided, it is merged with the instance's ``connection_pool_kw``
++        variable and used to create the new connection pool, if one is
++        needed.
++        """
++
++        if not host:
++            raise LocationValueError("No host specified.")
++
++        request_context = self._merge_pool_kwargs(pool_kwargs)
++        request_context["scheme"] = scheme or "http"
++        if not port:
++            port = port_by_scheme.get(request_context["scheme"].lower(), 80)
++        request_context["port"] = port
++        request_context["host"] = host
++
++        return self.connection_from_context(request_context)
++
++    def connection_from_context(self, request_context):
++        """
++        Get a :class:`ConnectionPool` based on the request context.
++
++        ``request_context`` must at least contain the ``scheme`` key and its
++        value must be a key in ``key_fn_by_scheme`` instance variable.
++        """
++        scheme = request_context["scheme"].lower()
++        pool_key_constructor = self.key_fn_by_scheme[scheme]
++        pool_key = pool_key_constructor(request_context)
++
++        return self.connection_from_pool_key(pool_key, request_context=request_context)
++
++    def connection_from_pool_key(self, pool_key, request_context=None):
++        """
++        Get a :class:`ConnectionPool` based on the provided pool key.
++
++        ``pool_key`` should be a namedtuple that only contains immutable
++        objects. At a minimum it must have the ``scheme``, ``host``, and
++        ``port`` fields.
++        """
++        with self.pools.lock:
++            # If the scheme, host, or port doesn't match existing open
++            # connections, open a new ConnectionPool.
++            pool = self.pools.get(pool_key)
++            if pool:
++                return pool
++
++            # Make a fresh ConnectionPool of the desired type
++            scheme = request_context["scheme"]
++            host = request_context["host"]
++            port = request_context["port"]
++            pool = self._new_pool(scheme, host, port, request_context=request_context)
++            self.pools[pool_key] = pool
++
++        return pool
++
++    def connection_from_url(self, url, pool_kwargs=None):
++        """
++        Similar to :func:`urllib3.connectionpool.connection_from_url`.
++
++        If ``pool_kwargs`` is not provided and a new pool needs to be
++        constructed, ``self.connection_pool_kw`` is used to initialize
++        the :class:`urllib3.connectionpool.ConnectionPool`. If ``pool_kwargs``
++        is provided, it is used instead. Note that if a new pool does not
++        need to be created for the request, the provided ``pool_kwargs`` are
++        not used.
++        """
++        u = parse_url(url)
++        return self.connection_from_host(
++            u.host, port=u.port, scheme=u.scheme, pool_kwargs=pool_kwargs
++        )
++
++    def _merge_pool_kwargs(self, override):
++        """
++        Merge a dictionary of override values for self.connection_pool_kw.
++
++        This does not modify self.connection_pool_kw and returns a new dict.
++        Any keys in the override dictionary with a value of ``None`` are
++        removed from the merged dictionary.
++        """
++        base_pool_kwargs = self.connection_pool_kw.copy()
++        if override:
++            for key, value in override.items():
++                if value is None:
++                    try:
++                        del base_pool_kwargs[key]
++                    except KeyError:
++                        pass
++                else:
++                    base_pool_kwargs[key] = value
++        return base_pool_kwargs
++
++    def urlopen(self, method, url, redirect=True, **kw):
++        """
++        Same as :meth:`urllib3.connectionpool.HTTPConnectionPool.urlopen`
++        with custom cross-host redirect logic and only sends the request-uri
++        portion of the ``url``.
++
++        The given ``url`` parameter must be absolute, such that an appropriate
++        :class:`urllib3.connectionpool.ConnectionPool` can be chosen for it.
++        """
++        u = parse_url(url)
++        conn = self.connection_from_host(u.host, port=u.port, scheme=u.scheme)
++
++        kw["assert_same_host"] = False
++        kw["redirect"] = False
++
++        if "headers" not in kw:
++            kw["headers"] = self.headers.copy()
++
++        if self.proxy is not None and u.scheme == "http":
++            response = conn.urlopen(method, url, **kw)
++        else:
++            response = conn.urlopen(method, u.request_uri, **kw)
++
++        redirect_location = redirect and response.get_redirect_location()
++        if not redirect_location:
++            return response
++
++        # Support relative URLs for redirecting.
++        redirect_location = urljoin(url, redirect_location)
++
++        # RFC 7231, Section 6.4.4
++        if response.status == 303:
++            method = "GET"
++
++        retries = kw.get("retries")
++        if not isinstance(retries, Retry):
++            retries = Retry.from_int(retries, redirect=redirect)
++
++        # Strip headers marked as unsafe to forward to the redirected location.
++        # Check remove_headers_on_redirect to avoid a potential network call within
++        # conn.is_same_host() which may use socket.gethostbyname() in the future.
++        if retries.remove_headers_on_redirect and not conn.is_same_host(
++            redirect_location
++        ):
++            headers = list(six.iterkeys(kw["headers"]))
++            for header in headers:
++                if header.lower() in retries.remove_headers_on_redirect:
++                    kw["headers"].pop(header, None)
++
++        try:
++            retries = retries.increment(method, url, response=response, _pool=conn)
++        except MaxRetryError:
++            if retries.raise_on_redirect:
++                raise
++            return response
++
++        kw["retries"] = retries
++        kw["redirect"] = redirect
++
++        log.info("Redirecting %s -> %s", url, redirect_location)
++        return self.urlopen(method, redirect_location, **kw)
++
++
++class ProxyManager(PoolManager):
++    """
++    Behaves just like :class:`PoolManager`, but sends all requests through
++    the defined proxy, using the CONNECT method for HTTPS URLs.
++
++    :param proxy_url:
++        The URL of the proxy to be used.
++
++    :param proxy_headers:
++        A dictionary containing headers that will be sent to the proxy. In case
++        of HTTP they are being sent with each request, while in the
++        HTTPS/CONNECT case they are sent only once. Could be used for proxy
++        authentication.
++
++    Example:
++        >>> proxy = urllib3.ProxyManager('http://localhost:3128/')
++        >>> r1 = proxy.request('GET', 'http://google.com/')
++        >>> r2 = proxy.request('GET', 'http://httpbin.org/')
++        >>> len(proxy.pools)
++        1
++        >>> r3 = proxy.request('GET', 'https://httpbin.org/')
++        >>> r4 = proxy.request('GET', 'https://twitter.com/')
++        >>> len(proxy.pools)
++        3
++
++    """
++
++    def __init__(
++        self,
++        proxy_url,
++        num_pools=10,
++        headers=None,
++        proxy_headers=None,
++        **connection_pool_kw
++    ):
++
++        if isinstance(proxy_url, HTTPConnectionPool):
++            proxy_url = "%s://%s:%i" % (
++                proxy_url.scheme,
++                proxy_url.host,
++                proxy_url.port,
++            )
++        proxy = parse_url(proxy_url)
++        if not proxy.port:
++            port = port_by_scheme.get(proxy.scheme, 80)
++            proxy = proxy._replace(port=port)
++
++        if proxy.scheme not in ("http", "https"):
++            raise ProxySchemeUnknown(proxy.scheme)
++
++        self.proxy = proxy
++        self.proxy_headers = proxy_headers or {}
++
++        connection_pool_kw["_proxy"] = self.proxy
++        connection_pool_kw["_proxy_headers"] = self.proxy_headers
++
++        super(ProxyManager, self).__init__(num_pools, headers, **connection_pool_kw)
++
++    def connection_from_host(self, host, port=None, scheme="http", pool_kwargs=None):
++        if scheme == "https":
++            return super(ProxyManager, self).connection_from_host(
++                host, port, scheme, pool_kwargs=pool_kwargs
++            )
++
++        return super(ProxyManager, self).connection_from_host(
++            self.proxy.host, self.proxy.port, self.proxy.scheme, pool_kwargs=pool_kwargs
++        )
++
++    def _set_proxy_headers(self, url, headers=None):
++        """
++        Sets headers needed by proxies: specifically, the Accept and Host
++        headers. Only sets headers not provided by the user.
++        """
++        headers_ = {"Accept": "*/*"}
++
++        netloc = parse_url(url).netloc
++        if netloc:
++            headers_["Host"] = netloc
++
++        if headers:
++            headers_.update(headers)
++        return headers_
++
++    def urlopen(self, method, url, redirect=True, **kw):
++        "Same as HTTP(S)ConnectionPool.urlopen, ``url`` must be absolute."
++        u = parse_url(url)
++
++        if u.scheme == "http":
++            # For proxied HTTPS requests, httplib sets the necessary headers
++            # on the CONNECT to the proxy. For HTTP, we'll definitely
++            # need to set 'Host' at the very least.
++            headers = kw.get("headers", self.headers)
++            kw["headers"] = self._set_proxy_headers(url, headers)
++
++        return super(ProxyManager, self).urlopen(method, url, redirect=redirect, **kw)
++
++
++def proxy_from_url(url, **kw):
++    return ProxyManager(proxy_url=url, **kw)
+Index: venv/Lib/site-packages/urllib3/connectionpool.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/connectionpool.py	(date 1573549849391)
++++ venv/Lib/site-packages/urllib3/connectionpool.py	(date 1573549849391)
+@@ -0,0 +1,1051 @@
++from __future__ import absolute_import
++import errno
++import logging
++import sys
++import warnings
++
++from socket import error as SocketError, timeout as SocketTimeout
++import socket
++
++
++from .exceptions import (
++    ClosedPoolError,
++    ProtocolError,
++    EmptyPoolError,
++    HeaderParsingError,
++    HostChangedError,
++    LocationValueError,
++    MaxRetryError,
++    ProxyError,
++    ReadTimeoutError,
++    SSLError,
++    TimeoutError,
++    InsecureRequestWarning,
++    NewConnectionError,
++)
++from .packages.ssl_match_hostname import CertificateError
++from .packages import six
++from .packages.six.moves import queue
++from .connection import (
++    port_by_scheme,
++    DummyConnection,
++    HTTPConnection,
++    HTTPSConnection,
++    VerifiedHTTPSConnection,
++    HTTPException,
++    BaseSSLError,
++)
++from .request import RequestMethods
++from .response import HTTPResponse
++
++from .util.connection import is_connection_dropped
++from .util.request import set_file_position
++from .util.response import assert_header_parsing
++from .util.retry import Retry
++from .util.timeout import Timeout
++from .util.url import (
++    get_host,
++    parse_url,
++    Url,
++    _normalize_host as normalize_host,
++    _encode_target,
++)
++from .util.queue import LifoQueue
++
++
++xrange = six.moves.xrange
++
++log = logging.getLogger(__name__)
++
++_Default = object()
++
++
++# Pool objects
++class ConnectionPool(object):
++    """
++    Base class for all connection pools, such as
++    :class:`.HTTPConnectionPool` and :class:`.HTTPSConnectionPool`.
++    """
++
++    scheme = None
++    QueueCls = LifoQueue
++
++    def __init__(self, host, port=None):
++        if not host:
++            raise LocationValueError("No host specified.")
++
++        self.host = _normalize_host(host, scheme=self.scheme)
++        self._proxy_host = host.lower()
++        self.port = port
++
++    def __str__(self):
++        return "%s(host=%r, port=%r)" % (type(self).__name__, self.host, self.port)
++
++    def __enter__(self):
++        return self
++
++    def __exit__(self, exc_type, exc_val, exc_tb):
++        self.close()
++        # Return False to re-raise any potential exceptions
++        return False
++
++    def close(self):
++        """
++        Close all pooled connections and disable the pool.
++        """
++        pass
++
++
++# This is taken from http://hg.python.org/cpython/file/7aaba721ebc0/Lib/socket.py#l252
++_blocking_errnos = {errno.EAGAIN, errno.EWOULDBLOCK}
++
++
++class HTTPConnectionPool(ConnectionPool, RequestMethods):
++    """
++    Thread-safe connection pool for one host.
++
++    :param host:
++        Host used for this HTTP Connection (e.g. "localhost"), passed into
++        :class:`httplib.HTTPConnection`.
++
++    :param port:
++        Port used for this HTTP Connection (None is equivalent to 80), passed
++        into :class:`httplib.HTTPConnection`.
++
++    :param strict:
++        Causes BadStatusLine to be raised if the status line can't be parsed
++        as a valid HTTP/1.0 or 1.1 status line, passed into
++        :class:`httplib.HTTPConnection`.
++
++        .. note::
++           Only works in Python 2. This parameter is ignored in Python 3.
++
++    :param timeout:
++        Socket timeout in seconds for each individual connection. This can
++        be a float or integer, which sets the timeout for the HTTP request,
++        or an instance of :class:`urllib3.util.Timeout` which gives you more
++        fine-grained control over request timeouts. After the constructor has
++        been parsed, this is always a `urllib3.util.Timeout` object.
++
++    :param maxsize:
++        Number of connections to save that can be reused. More than 1 is useful
++        in multithreaded situations. If ``block`` is set to False, more
++        connections will be created but they will not be saved once they've
++        been used.
++
++    :param block:
++        If set to True, no more than ``maxsize`` connections will be used at
++        a time. When no free connections are available, the call will block
++        until a connection has been released. This is a useful side effect for
++        particular multithreaded situations where one does not want to use more
++        than maxsize connections per host to prevent flooding.
++
++    :param headers:
++        Headers to include with all requests, unless other headers are given
++        explicitly.
++
++    :param retries:
++        Retry configuration to use by default with requests in this pool.
++
++    :param _proxy:
++        Parsed proxy URL, should not be used directly, instead, see
++        :class:`urllib3.connectionpool.ProxyManager`"
++
++    :param _proxy_headers:
++        A dictionary with proxy headers, should not be used directly,
++        instead, see :class:`urllib3.connectionpool.ProxyManager`"
++
++    :param \\**conn_kw:
++        Additional parameters are used to create fresh :class:`urllib3.connection.HTTPConnection`,
++        :class:`urllib3.connection.HTTPSConnection` instances.
++    """
++
++    scheme = "http"
++    ConnectionCls = HTTPConnection
++    ResponseCls = HTTPResponse
++
++    def __init__(
++        self,
++        host,
++        port=None,
++        strict=False,
++        timeout=Timeout.DEFAULT_TIMEOUT,
++        maxsize=1,
++        block=False,
++        headers=None,
++        retries=None,
++        _proxy=None,
++        _proxy_headers=None,
++        **conn_kw
++    ):
++        ConnectionPool.__init__(self, host, port)
++        RequestMethods.__init__(self, headers)
++
++        self.strict = strict
++
++        if not isinstance(timeout, Timeout):
++            timeout = Timeout.from_float(timeout)
++
++        if retries is None:
++            retries = Retry.DEFAULT
++
++        self.timeout = timeout
++        self.retries = retries
++
++        self.pool = self.QueueCls(maxsize)
++        self.block = block
++
++        self.proxy = _proxy
++        self.proxy_headers = _proxy_headers or {}
++
++        # Fill the queue up so that doing get() on it will block properly
++        for _ in xrange(maxsize):
++            self.pool.put(None)
++
++        # These are mostly for testing and debugging purposes.
++        self.num_connections = 0
++        self.num_requests = 0
++        self.conn_kw = conn_kw
++
++        if self.proxy:
++            # Enable Nagle's algorithm for proxies, to avoid packet fragmentation.
++            # We cannot know if the user has added default socket options, so we cannot replace the
++            # list.
++            self.conn_kw.setdefault("socket_options", [])
++
++    def _new_conn(self):
++        """
++        Return a fresh :class:`HTTPConnection`.
++        """
++        self.num_connections += 1
++        log.debug(
++            "Starting new HTTP connection (%d): %s:%s",
++            self.num_connections,
++            self.host,
++            self.port or "80",
++        )
++
++        conn = self.ConnectionCls(
++            host=self.host,
++            port=self.port,
++            timeout=self.timeout.connect_timeout,
++            strict=self.strict,
++            **self.conn_kw
++        )
++        return conn
++
++    def _get_conn(self, timeout=None):
++        """
++        Get a connection. Will return a pooled connection if one is available.
++
++        If no connections are available and :prop:`.block` is ``False``, then a
++        fresh connection is returned.
++
++        :param timeout:
++            Seconds to wait before giving up and raising
++            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and
++            :prop:`.block` is ``True``.
++        """
++        conn = None
++        try:
++            conn = self.pool.get(block=self.block, timeout=timeout)
++
++        except AttributeError:  # self.pool is None
++            raise ClosedPoolError(self, "Pool is closed.")
++
++        except queue.Empty:
++            if self.block:
++                raise EmptyPoolError(
++                    self,
++                    "Pool reached maximum size and no more connections are allowed.",
++                )
++            pass  # Oh well, we'll create a new connection then
++
++        # If this is a persistent connection, check if it got disconnected
++        if conn and is_connection_dropped(conn):
++            log.debug("Resetting dropped connection: %s", self.host)
++            conn.close()
++            if getattr(conn, "auto_open", 1) == 0:
++                # This is a proxied connection that has been mutated by
++                # httplib._tunnel() and cannot be reused (since it would
++                # attempt to bypass the proxy)
++                conn = None
++
++        return conn or self._new_conn()
++
++    def _put_conn(self, conn):
++        """
++        Put a connection back into the pool.
++
++        :param conn:
++            Connection object for the current host and port as returned by
++            :meth:`._new_conn` or :meth:`._get_conn`.
++
++        If the pool is already full, the connection is closed and discarded
++        because we exceeded maxsize. If connections are discarded frequently,
++        then maxsize should be increased.
++
++        If the pool is closed, then the connection will be closed and discarded.
++        """
++        try:
++            self.pool.put(conn, block=False)
++            return  # Everything is dandy, done.
++        except AttributeError:
++            # self.pool is None.
++            pass
++        except queue.Full:
++            # This should never happen if self.block == True
++            log.warning("Connection pool is full, discarding connection: %s", self.host)
++
++        # Connection never got put back into the pool, close it.
++        if conn:
++            conn.close()
++
++    def _validate_conn(self, conn):
++        """
++        Called right before a request is made, after the socket is created.
++        """
++        pass
++
++    def _prepare_proxy(self, conn):
++        # Nothing to do for HTTP connections.
++        pass
++
++    def _get_timeout(self, timeout):
++        """ Helper that always returns a :class:`urllib3.util.Timeout` """
++        if timeout is _Default:
++            return self.timeout.clone()
++
++        if isinstance(timeout, Timeout):
++            return timeout.clone()
++        else:
++            # User passed us an int/float. This is for backwards compatibility,
++            # can be removed later
++            return Timeout.from_float(timeout)
++
++    def _raise_timeout(self, err, url, timeout_value):
++        """Is the error actually a timeout? Will raise a ReadTimeout or pass"""
++
++        if isinstance(err, SocketTimeout):
++            raise ReadTimeoutError(
++                self, url, "Read timed out. (read timeout=%s)" % timeout_value
++            )
++
++        # See the above comment about EAGAIN in Python 3. In Python 2 we have
++        # to specifically catch it and throw the timeout error
++        if hasattr(err, "errno") and err.errno in _blocking_errnos:
++            raise ReadTimeoutError(
++                self, url, "Read timed out. (read timeout=%s)" % timeout_value
++            )
++
++        # Catch possible read timeouts thrown as SSL errors. If not the
++        # case, rethrow the original. We need to do this because of:
++        # http://bugs.python.org/issue10272
++        if "timed out" in str(err) or "did not complete (read)" in str(
++            err
++        ):  # Python < 2.7.4
++            raise ReadTimeoutError(
++                self, url, "Read timed out. (read timeout=%s)" % timeout_value
++            )
++
++    def _make_request(
++        self, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw
++    ):
++        """
++        Perform a request on a given urllib connection object taken from our
++        pool.
++
++        :param conn:
++            a connection from one of our connection pools
++
++        :param timeout:
++            Socket timeout in seconds for the request. This can be a
++            float or integer, which will set the same timeout value for
++            the socket connect and the socket read, or an instance of
++            :class:`urllib3.util.Timeout`, which gives you more fine-grained
++            control over your timeouts.
++        """
++        self.num_requests += 1
++
++        timeout_obj = self._get_timeout(timeout)
++        timeout_obj.start_connect()
++        conn.timeout = timeout_obj.connect_timeout
++
++        # Trigger any extra validation we need to do.
++        try:
++            self._validate_conn(conn)
++        except (SocketTimeout, BaseSSLError) as e:
++            # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.
++            self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
++            raise
++
++        # conn.request() calls httplib.*.request, not the method in
++        # urllib3.request. It also calls makefile (recv) on the socket.
++        if chunked:
++            conn.request_chunked(method, url, **httplib_request_kw)
++        else:
++            conn.request(method, url, **httplib_request_kw)
++
++        # Reset the timeout for the recv() on the socket
++        read_timeout = timeout_obj.read_timeout
++
++        # App Engine doesn't have a sock attr
++        if getattr(conn, "sock", None):
++            # In Python 3 socket.py will catch EAGAIN and return None when you
++            # try and read into the file pointer created by http.client, which
++            # instead raises a BadStatusLine exception. Instead of catching
++            # the exception and assuming all BadStatusLine exceptions are read
++            # timeouts, check for a zero timeout before making the request.
++            if read_timeout == 0:
++                raise ReadTimeoutError(
++                    self, url, "Read timed out. (read timeout=%s)" % read_timeout
++                )
++            if read_timeout is Timeout.DEFAULT_TIMEOUT:
++                conn.sock.settimeout(socket.getdefaulttimeout())
++            else:  # None or a value
++                conn.sock.settimeout(read_timeout)
++
++        # Receive the response from the server
++        try:
++            try:
++                # Python 2.7, use buffering of HTTP responses
++                httplib_response = conn.getresponse(buffering=True)
++            except TypeError:
++                # Python 3
++                try:
++                    httplib_response = conn.getresponse()
++                except BaseException as e:
++                    # Remove the TypeError from the exception chain in
++                    # Python 3 (including for exceptions like SystemExit).
++                    # Otherwise it looks like a bug in the code.
++                    six.raise_from(e, None)
++        except (SocketTimeout, BaseSSLError, SocketError) as e:
++            self._raise_timeout(err=e, url=url, timeout_value=read_timeout)
++            raise
++
++        # AppEngine doesn't have a version attr.
++        http_version = getattr(conn, "_http_vsn_str", "HTTP/?")
++        log.debug(
++            '%s://%s:%s "%s %s %s" %s %s',
++            self.scheme,
++            self.host,
++            self.port,
++            method,
++            url,
++            http_version,
++            httplib_response.status,
++            httplib_response.length,
++        )
++
++        try:
++            assert_header_parsing(httplib_response.msg)
++        except (HeaderParsingError, TypeError) as hpe:  # Platform-specific: Python 3
++            log.warning(
++                "Failed to parse headers (url=%s): %s",
++                self._absolute_url(url),
++                hpe,
++                exc_info=True,
++            )
++
++        return httplib_response
++
++    def _absolute_url(self, path):
++        return Url(scheme=self.scheme, host=self.host, port=self.port, path=path).url
++
++    def close(self):
++        """
++        Close all pooled connections and disable the pool.
++        """
++        if self.pool is None:
++            return
++        # Disable access to the pool
++        old_pool, self.pool = self.pool, None
++
++        try:
++            while True:
++                conn = old_pool.get(block=False)
++                if conn:
++                    conn.close()
++
++        except queue.Empty:
++            pass  # Done.
++
++    def is_same_host(self, url):
++        """
++        Check if the given ``url`` is a member of the same host as this
++        connection pool.
++        """
++        if url.startswith("/"):
++            return True
++
++        # TODO: Add optional support for socket.gethostbyname checking.
++        scheme, host, port = get_host(url)
++        if host is not None:
++            host = _normalize_host(host, scheme=scheme)
++
++        # Use explicit default port for comparison when none is given
++        if self.port and not port:
++            port = port_by_scheme.get(scheme)
++        elif not self.port and port == port_by_scheme.get(scheme):
++            port = None
++
++        return (scheme, host, port) == (self.scheme, self.host, self.port)
++
++    def urlopen(
++        self,
++        method,
++        url,
++        body=None,
++        headers=None,
++        retries=None,
++        redirect=True,
++        assert_same_host=True,
++        timeout=_Default,
++        pool_timeout=None,
++        release_conn=None,
++        chunked=False,
++        body_pos=None,
++        **response_kw
++    ):
++        """
++        Get a connection from the pool and perform an HTTP request. This is the
++        lowest level call for making a request, so you'll need to specify all
++        the raw details.
++
++        .. note::
++
++           More commonly, it's appropriate to use a convenience method provided
++           by :class:`.RequestMethods`, such as :meth:`request`.
++
++        .. note::
++
++           `release_conn` will only behave as expected if
++           `preload_content=False` because we want to make
++           `preload_content=False` the default behaviour someday soon without
++           breaking backwards compatibility.
++
++        :param method:
++            HTTP request method (such as GET, POST, PUT, etc.)
++
++        :param body:
++            Data to send in the request body (useful for creating
++            POST requests, see HTTPConnectionPool.post_url for
++            more convenience).
++
++        :param headers:
++            Dictionary of custom headers to send, such as User-Agent,
++            If-None-Match, etc. If None, pool headers are used. If provided,
++            these headers completely replace any pool-specific headers.
++
++        :param retries:
++            Configure the number of retries to allow before raising a
++            :class:`~urllib3.exceptions.MaxRetryError` exception.
++
++            Pass ``None`` to retry until you receive a response. Pass a
++            :class:`~urllib3.util.retry.Retry` object for fine-grained control
++            over different types of retries.
++            Pass an integer number to retry connection errors that many times,
++            but no other types of errors. Pass zero to never retry.
++
++            If ``False``, then retries are disabled and any exception is raised
++            immediately. Also, instead of raising a MaxRetryError on redirects,
++            the redirect response will be returned.
++
++        :type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.
++
++        :param redirect:
++            If True, automatically handle redirects (status codes 301, 302,
++            303, 307, 308). Each redirect counts as a retry. Disabling retries
++            will disable redirect, too.
++
++        :param assert_same_host:
++            If ``True``, will make sure that the host of the pool requests is
++            consistent else will raise HostChangedError. When False, you can
++            use the pool on an HTTP proxy and request foreign hosts.
++
++        :param timeout:
++            If specified, overrides the default timeout for this one
++            request. It may be a float (in seconds) or an instance of
++            :class:`urllib3.util.Timeout`.
++
++        :param pool_timeout:
++            If set and the pool is set to block=True, then this method will
++            block for ``pool_timeout`` seconds and raise EmptyPoolError if no
++            connection is available within the time period.
++
++        :param release_conn:
++            If False, then the urlopen call will not release the connection
++            back into the pool once a response is received (but will release if
++            you read the entire contents of the response such as when
++            `preload_content=True`). This is useful if you're not preloading
++            the response's content immediately. You will need to call
++            ``r.release_conn()`` on the response ``r`` to return the connection
++            back into the pool. If None, it takes the value of
++            ``response_kw.get('preload_content', True)``.
++
++        :param chunked:
++            If True, urllib3 will send the body using chunked transfer
++            encoding. Otherwise, urllib3 will send the body using the standard
++            content-length form. Defaults to False.
++
++        :param int body_pos:
++            Position to seek to in file-like body in the event of a retry or
++            redirect. Typically this won't need to be set because urllib3 will
++            auto-populate the value when needed.
++
++        :param \\**response_kw:
++            Additional parameters are passed to
++            :meth:`urllib3.response.HTTPResponse.from_httplib`
++        """
++        if headers is None:
++            headers = self.headers
++
++        if not isinstance(retries, Retry):
++            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)
++
++        if release_conn is None:
++            release_conn = response_kw.get("preload_content", True)
++
++        # Check host
++        if assert_same_host and not self.is_same_host(url):
++            raise HostChangedError(self, url, retries)
++
++        # Ensure that the URL we're connecting to is properly encoded
++        if url.startswith("/"):
++            url = six.ensure_str(_encode_target(url))
++        else:
++            url = six.ensure_str(parse_url(url).url)
++
++        conn = None
++
++        # Track whether `conn` needs to be released before
++        # returning/raising/recursing. Update this variable if necessary, and
++        # leave `release_conn` constant throughout the function. That way, if
++        # the function recurses, the original value of `release_conn` will be
++        # passed down into the recursive call, and its value will be respected.
++        #
++        # See issue #651 [1] for details.
++        #
++        # [1] <https://github.com/urllib3/urllib3/issues/651>
++        release_this_conn = release_conn
++
++        # Merge the proxy headers. Only do this in HTTP. We have to copy the
++        # headers dict so we can safely change it without those changes being
++        # reflected in anyone else's copy.
++        if self.scheme == "http":
++            headers = headers.copy()
++            headers.update(self.proxy_headers)
++
++        # Must keep the exception bound to a separate variable or else Python 3
++        # complains about UnboundLocalError.
++        err = None
++
++        # Keep track of whether we cleanly exited the except block. This
++        # ensures we do proper cleanup in finally.
++        clean_exit = False
++
++        # Rewind body position, if needed. Record current position
++        # for future rewinds in the event of a redirect/retry.
++        body_pos = set_file_position(body, body_pos)
++
++        try:
++            # Request a connection from the queue.
++            timeout_obj = self._get_timeout(timeout)
++            conn = self._get_conn(timeout=pool_timeout)
++
++            conn.timeout = timeout_obj.connect_timeout
++
++            is_new_proxy_conn = self.proxy is not None and not getattr(
++                conn, "sock", None
++            )
++            if is_new_proxy_conn:
++                self._prepare_proxy(conn)
++
++            # Make the request on the httplib connection object.
++            httplib_response = self._make_request(
++                conn,
++                method,
++                url,
++                timeout=timeout_obj,
++                body=body,
++                headers=headers,
++                chunked=chunked,
++            )
++
++            # If we're going to release the connection in ``finally:``, then
++            # the response doesn't need to know about the connection. Otherwise
++            # it will also try to release it and we'll have a double-release
++            # mess.
++            response_conn = conn if not release_conn else None
++
++            # Pass method to Response for length checking
++            response_kw["request_method"] = method
++
++            # Import httplib's response into our own wrapper object
++            response = self.ResponseCls.from_httplib(
++                httplib_response,
++                pool=self,
++                connection=response_conn,
++                retries=retries,
++                **response_kw
++            )
++
++            # Everything went great!
++            clean_exit = True
++
++        except queue.Empty:
++            # Timed out by queue.
++            raise EmptyPoolError(self, "No pool connections are available.")
++
++        except (
++            TimeoutError,
++            HTTPException,
++            SocketError,
++            ProtocolError,
++            BaseSSLError,
++            SSLError,
++            CertificateError,
++        ) as e:
++            # Discard the connection for these exceptions. It will be
++            # replaced during the next _get_conn() call.
++            clean_exit = False
++            if isinstance(e, (BaseSSLError, CertificateError)):
++                e = SSLError(e)
++            elif isinstance(e, (SocketError, NewConnectionError)) and self.proxy:
++                e = ProxyError("Cannot connect to proxy.", e)
++            elif isinstance(e, (SocketError, HTTPException)):
++                e = ProtocolError("Connection aborted.", e)
++
++            retries = retries.increment(
++                method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
++            )
++            retries.sleep()
++
++            # Keep track of the error for the retry warning.
++            err = e
++
++        finally:
++            if not clean_exit:
++                # We hit some kind of exception, handled or otherwise. We need
++                # to throw the connection away unless explicitly told not to.
++                # Close the connection, set the variable to None, and make sure
++                # we put the None back in the pool to avoid leaking it.
++                conn = conn and conn.close()
++                release_this_conn = True
++
++            if release_this_conn:
++                # Put the connection back to be reused. If the connection is
++                # expired then it will be None, which will get replaced with a
++                # fresh connection during _get_conn.
++                self._put_conn(conn)
++
++        if not conn:
++            # Try again
++            log.warning(
++                "Retrying (%r) after connection broken by '%r': %s", retries, err, url
++            )
++            return self.urlopen(
++                method,
++                url,
++                body,
++                headers,
++                retries,
++                redirect,
++                assert_same_host,
++                timeout=timeout,
++                pool_timeout=pool_timeout,
++                release_conn=release_conn,
++                chunked=chunked,
++                body_pos=body_pos,
++                **response_kw
++            )
++
++        def drain_and_release_conn(response):
++            try:
++                # discard any remaining response body, the connection will be
++                # released back to the pool once the entire response is read
++                response.read()
++            except (
++                TimeoutError,
++                HTTPException,
++                SocketError,
++                ProtocolError,
++                BaseSSLError,
++                SSLError,
++            ):
++                pass
++
++        # Handle redirect?
++        redirect_location = redirect and response.get_redirect_location()
++        if redirect_location:
++            if response.status == 303:
++                method = "GET"
++
++            try:
++                retries = retries.increment(method, url, response=response, _pool=self)
++            except MaxRetryError:
++                if retries.raise_on_redirect:
++                    # Drain and release the connection for this response, since
++                    # we're not returning it to be released manually.
++                    drain_and_release_conn(response)
++                    raise
++                return response
++
++            # drain and return the connection to the pool before recursing
++            drain_and_release_conn(response)
++
++            retries.sleep_for_retry(response)
++            log.debug("Redirecting %s -> %s", url, redirect_location)
++            return self.urlopen(
++                method,
++                redirect_location,
++                body,
++                headers,
++                retries=retries,
++                redirect=redirect,
++                assert_same_host=assert_same_host,
++                timeout=timeout,
++                pool_timeout=pool_timeout,
++                release_conn=release_conn,
++                chunked=chunked,
++                body_pos=body_pos,
++                **response_kw
++            )
++
++        # Check if we should retry the HTTP response.
++        has_retry_after = bool(response.getheader("Retry-After"))
++        if retries.is_retry(method, response.status, has_retry_after):
++            try:
++                retries = retries.increment(method, url, response=response, _pool=self)
++            except MaxRetryError:
++                if retries.raise_on_status:
++                    # Drain and release the connection for this response, since
++                    # we're not returning it to be released manually.
++                    drain_and_release_conn(response)
++                    raise
++                return response
++
++            # drain and return the connection to the pool before recursing
++            drain_and_release_conn(response)
++
++            retries.sleep(response)
++            log.debug("Retry: %s", url)
++            return self.urlopen(
++                method,
++                url,
++                body,
++                headers,
++                retries=retries,
++                redirect=redirect,
++                assert_same_host=assert_same_host,
++                timeout=timeout,
++                pool_timeout=pool_timeout,
++                release_conn=release_conn,
++                chunked=chunked,
++                body_pos=body_pos,
++                **response_kw
++            )
++
++        return response
++
++
++class HTTPSConnectionPool(HTTPConnectionPool):
++    """
++    Same as :class:`.HTTPConnectionPool`, but HTTPS.
++
++    When Python is compiled with the :mod:`ssl` module, then
++    :class:`.VerifiedHTTPSConnection` is used, which *can* verify certificates,
++    instead of :class:`.HTTPSConnection`.
++
++    :class:`.VerifiedHTTPSConnection` uses one of ``assert_fingerprint``,
++    ``assert_hostname`` and ``host`` in this order to verify connections.
++    If ``assert_hostname`` is False, no verification is done.
++
++    The ``key_file``, ``cert_file``, ``cert_reqs``, ``ca_certs``,
++    ``ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
++    is available and are fed into :meth:`urllib3.util.ssl_wrap_socket` to upgrade
++    the connection socket into an SSL socket.
++    """
++
++    scheme = "https"
++    ConnectionCls = HTTPSConnection
++
++    def __init__(
++        self,
++        host,
++        port=None,
++        strict=False,
++        timeout=Timeout.DEFAULT_TIMEOUT,
++        maxsize=1,
++        block=False,
++        headers=None,
++        retries=None,
++        _proxy=None,
++        _proxy_headers=None,
++        key_file=None,
++        cert_file=None,
++        cert_reqs=None,
++        key_password=None,
++        ca_certs=None,
++        ssl_version=None,
++        assert_hostname=None,
++        assert_fingerprint=None,
++        ca_cert_dir=None,
++        **conn_kw
++    ):
++
++        HTTPConnectionPool.__init__(
++            self,
++            host,
++            port,
++            strict,
++            timeout,
++            maxsize,
++            block,
++            headers,
++            retries,
++            _proxy,
++            _proxy_headers,
++            **conn_kw
++        )
++
++        self.key_file = key_file
++        self.cert_file = cert_file
++        self.cert_reqs = cert_reqs
++        self.key_password = key_password
++        self.ca_certs = ca_certs
++        self.ca_cert_dir = ca_cert_dir
++        self.ssl_version = ssl_version
++        self.assert_hostname = assert_hostname
++        self.assert_fingerprint = assert_fingerprint
++
++    def _prepare_conn(self, conn):
++        """
++        Prepare the ``connection`` for :meth:`urllib3.util.ssl_wrap_socket`
++        and establish the tunnel if proxy is used.
++        """
++
++        if isinstance(conn, VerifiedHTTPSConnection):
++            conn.set_cert(
++                key_file=self.key_file,
++                key_password=self.key_password,
++                cert_file=self.cert_file,
++                cert_reqs=self.cert_reqs,
++                ca_certs=self.ca_certs,
++                ca_cert_dir=self.ca_cert_dir,
++                assert_hostname=self.assert_hostname,
++                assert_fingerprint=self.assert_fingerprint,
++            )
++            conn.ssl_version = self.ssl_version
++        return conn
++
++    def _prepare_proxy(self, conn):
++        """
++        Establish tunnel connection early, because otherwise httplib
++        would improperly set Host: header to proxy's IP:port.
++        """
++        conn.set_tunnel(self._proxy_host, self.port, self.proxy_headers)
++        conn.connect()
++
++    def _new_conn(self):
++        """
++        Return a fresh :class:`httplib.HTTPSConnection`.
++        """
++        self.num_connections += 1
++        log.debug(
++            "Starting new HTTPS connection (%d): %s:%s",
++            self.num_connections,
++            self.host,
++            self.port or "443",
++        )
++
++        if not self.ConnectionCls or self.ConnectionCls is DummyConnection:
++            raise SSLError(
++                "Can't connect to HTTPS URL because the SSL module is not available."
++            )
++
++        actual_host = self.host
++        actual_port = self.port
++        if self.proxy is not None:
++            actual_host = self.proxy.host
++            actual_port = self.proxy.port
++
++        conn = self.ConnectionCls(
++            host=actual_host,
++            port=actual_port,
++            timeout=self.timeout.connect_timeout,
++            strict=self.strict,
++            cert_file=self.cert_file,
++            key_file=self.key_file,
++            key_password=self.key_password,
++            **self.conn_kw
++        )
++
++        return self._prepare_conn(conn)
++
++    def _validate_conn(self, conn):
++        """
++        Called right before a request is made, after the socket is created.
++        """
++        super(HTTPSConnectionPool, self)._validate_conn(conn)
++
++        # Force connect early to allow us to validate the connection.
++        if not getattr(conn, "sock", None):  # AppEngine might not have  `.sock`
++            conn.connect()
++
++        if not conn.is_verified:
++            warnings.warn(
++                (
++                    "Unverified HTTPS request is being made. "
++                    "Adding certificate verification is strongly advised. See: "
++                    "https://urllib3.readthedocs.io/en/latest/advanced-usage.html"
++                    "#ssl-warnings"
++                ),
++                InsecureRequestWarning,
++            )
++
++
++def connection_from_url(url, **kw):
++    """
++    Given a url, return an :class:`.ConnectionPool` instance of its host.
++
++    This is a shortcut for not having to parse out the scheme, host, and port
++    of the url before creating an :class:`.ConnectionPool` instance.
++
++    :param url:
++        Absolute URL string that must include the scheme. Port is optional.
++
++    :param \\**kw:
++        Passes additional parameters to the constructor of the appropriate
++        :class:`.ConnectionPool`. Useful for specifying things like
++        timeout, maxsize, headers, etc.
++
++    Example::
++
++        >>> conn = connection_from_url('http://google.com/')
++        >>> r = conn.request('GET', '/')
++    """
++    scheme, host, port = get_host(url)
++    port = port or port_by_scheme.get(scheme, 80)
++    if scheme == "https":
++        return HTTPSConnectionPool(host, port=port, **kw)
++    else:
++        return HTTPConnectionPool(host, port=port, **kw)
++
++
++def _normalize_host(host, scheme):
++    """
++    Normalize hosts for comparisons and use with sockets.
++    """
++
++    host = normalize_host(host, scheme)
++
++    # httplib doesn't like it when we include brackets in IPv6 addresses
++    # Specifically, if we include brackets but also pass the port then
++    # httplib crazily doubles up the square brackets on the Host header.
++    # Instead, we need to make sure we never pass ``None`` as the port.
++    # However, for backward compatibility reasons we can't actually
++    # *assert* that.  See http://bugs.python.org/issue28539
++    if host.startswith("[") and host.endswith("]"):
++        host = host[1:-1]
++    return host
+Index: venv/Lib/site-packages/urllib3/filepost.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/filepost.py	(date 1573549849406)
++++ venv/Lib/site-packages/urllib3/filepost.py	(date 1573549849406)
+@@ -0,0 +1,98 @@
++from __future__ import absolute_import
++import binascii
++import codecs
++import os
++
++from io import BytesIO
++
++from .packages import six
++from .packages.six import b
++from .fields import RequestField
++
++writer = codecs.lookup("utf-8")[3]
++
++
++def choose_boundary():
++    """
++    Our embarrassingly-simple replacement for mimetools.choose_boundary.
++    """
++    boundary = binascii.hexlify(os.urandom(16))
++    if not six.PY2:
++        boundary = boundary.decode("ascii")
++    return boundary
++
++
++def iter_field_objects(fields):
++    """
++    Iterate over fields.
++
++    Supports list of (k, v) tuples and dicts, and lists of
++    :class:`~urllib3.fields.RequestField`.
++
++    """
++    if isinstance(fields, dict):
++        i = six.iteritems(fields)
++    else:
++        i = iter(fields)
++
++    for field in i:
++        if isinstance(field, RequestField):
++            yield field
++        else:
++            yield RequestField.from_tuples(*field)
++
++
++def iter_fields(fields):
++    """
++    .. deprecated:: 1.6
++
++    Iterate over fields.
++
++    The addition of :class:`~urllib3.fields.RequestField` makes this function
++    obsolete. Instead, use :func:`iter_field_objects`, which returns
++    :class:`~urllib3.fields.RequestField` objects.
++
++    Supports list of (k, v) tuples and dicts.
++    """
++    if isinstance(fields, dict):
++        return ((k, v) for k, v in six.iteritems(fields))
++
++    return ((k, v) for k, v in fields)
++
++
++def encode_multipart_formdata(fields, boundary=None):
++    """
++    Encode a dictionary of ``fields`` using the multipart/form-data MIME format.
++
++    :param fields:
++        Dictionary of fields or list of (key, :class:`~urllib3.fields.RequestField`).
++
++    :param boundary:
++        If not specified, then a random boundary will be generated using
++        :func:`urllib3.filepost.choose_boundary`.
++    """
++    body = BytesIO()
++    if boundary is None:
++        boundary = choose_boundary()
++
++    for field in iter_field_objects(fields):
++        body.write(b("--%s\r\n" % (boundary)))
++
++        writer(body).write(field.render_headers())
++        data = field.data
++
++        if isinstance(data, int):
++            data = str(data)  # Backwards compatibility
++
++        if isinstance(data, six.text_type):
++            writer(body).write(data)
++        else:
++            body.write(data)
++
++        body.write(b"\r\n")
++
++    body.write(b("--%s--\r\n" % (boundary)))
++
++    content_type = str("multipart/form-data; boundary=%s" % boundary)
++
++    return body.getvalue(), content_type
+Index: venv/Lib/site-packages/urllib3/request.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/request.py	(date 1573549849417)
++++ venv/Lib/site-packages/urllib3/request.py	(date 1573549849417)
+@@ -0,0 +1,171 @@
++from __future__ import absolute_import
++
++from .filepost import encode_multipart_formdata
++from .packages.six.moves.urllib.parse import urlencode
++
++
++__all__ = ["RequestMethods"]
++
++
++class RequestMethods(object):
++    """
++    Convenience mixin for classes who implement a :meth:`urlopen` method, such
++    as :class:`~urllib3.connectionpool.HTTPConnectionPool` and
++    :class:`~urllib3.poolmanager.PoolManager`.
++
++    Provides behavior for making common types of HTTP request methods and
++    decides which type of request field encoding to use.
++
++    Specifically,
++
++    :meth:`.request_encode_url` is for sending requests whose fields are
++    encoded in the URL (such as GET, HEAD, DELETE).
++
++    :meth:`.request_encode_body` is for sending requests whose fields are
++    encoded in the *body* of the request using multipart or www-form-urlencoded
++    (such as for POST, PUT, PATCH).
++
++    :meth:`.request` is for making any kind of request, it will look up the
++    appropriate encoding format and use one of the above two methods to make
++    the request.
++
++    Initializer parameters:
++
++    :param headers:
++        Headers to include with all requests, unless other headers are given
++        explicitly.
++    """
++
++    _encode_url_methods = {"DELETE", "GET", "HEAD", "OPTIONS"}
++
++    def __init__(self, headers=None):
++        self.headers = headers or {}
++
++    def urlopen(
++        self,
++        method,
++        url,
++        body=None,
++        headers=None,
++        encode_multipart=True,
++        multipart_boundary=None,
++        **kw
++    ):  # Abstract
++        raise NotImplementedError(
++            "Classes extending RequestMethods must implement "
++            "their own ``urlopen`` method."
++        )
++
++    def request(self, method, url, fields=None, headers=None, **urlopen_kw):
++        """
++        Make a request using :meth:`urlopen` with the appropriate encoding of
++        ``fields`` based on the ``method`` used.
++
++        This is a convenience method that requires the least amount of manual
++        effort. It can be used in most situations, while still having the
++        option to drop down to more specific methods when necessary, such as
++        :meth:`request_encode_url`, :meth:`request_encode_body`,
++        or even the lowest level :meth:`urlopen`.
++        """
++        method = method.upper()
++
++        urlopen_kw["request_url"] = url
++
++        if method in self._encode_url_methods:
++            return self.request_encode_url(
++                method, url, fields=fields, headers=headers, **urlopen_kw
++            )
++        else:
++            return self.request_encode_body(
++                method, url, fields=fields, headers=headers, **urlopen_kw
++            )
++
++    def request_encode_url(self, method, url, fields=None, headers=None, **urlopen_kw):
++        """
++        Make a request using :meth:`urlopen` with the ``fields`` encoded in
++        the url. This is useful for request methods like GET, HEAD, DELETE, etc.
++        """
++        if headers is None:
++            headers = self.headers
++
++        extra_kw = {"headers": headers}
++        extra_kw.update(urlopen_kw)
++
++        if fields:
++            url += "?" + urlencode(fields)
++
++        return self.urlopen(method, url, **extra_kw)
++
++    def request_encode_body(
++        self,
++        method,
++        url,
++        fields=None,
++        headers=None,
++        encode_multipart=True,
++        multipart_boundary=None,
++        **urlopen_kw
++    ):
++        """
++        Make a request using :meth:`urlopen` with the ``fields`` encoded in
++        the body. This is useful for request methods like POST, PUT, PATCH, etc.
++
++        When ``encode_multipart=True`` (default), then
++        :meth:`urllib3.filepost.encode_multipart_formdata` is used to encode
++        the payload with the appropriate content type. Otherwise
++        :meth:`urllib.urlencode` is used with the
++        'application/x-www-form-urlencoded' content type.
++
++        Multipart encoding must be used when posting files, and it's reasonably
++        safe to use it in other times too. However, it may break request
++        signing, such as with OAuth.
++
++        Supports an optional ``fields`` parameter of key/value strings AND
++        key/filetuple. A filetuple is a (filename, data, MIME type) tuple where
++        the MIME type is optional. For example::
++
++            fields = {
++                'foo': 'bar',
++                'fakefile': ('foofile.txt', 'contents of foofile'),
++                'realfile': ('barfile.txt', open('realfile').read()),
++                'typedfile': ('bazfile.bin', open('bazfile').read(),
++                              'image/jpeg'),
++                'nonamefile': 'contents of nonamefile field',
++            }
++
++        When uploading a file, providing a filename (the first parameter of the
++        tuple) is optional but recommended to best mimic behavior of browsers.
++
++        Note that if ``headers`` are supplied, the 'Content-Type' header will
++        be overwritten because it depends on the dynamic random boundary string
++        which is used to compose the body of the request. The random boundary
++        string can be explicitly set with the ``multipart_boundary`` parameter.
++        """
++        if headers is None:
++            headers = self.headers
++
++        extra_kw = {"headers": {}}
++
++        if fields:
++            if "body" in urlopen_kw:
++                raise TypeError(
++                    "request got values for both 'fields' and 'body', can only specify one."
++                )
++
++            if encode_multipart:
++                body, content_type = encode_multipart_formdata(
++                    fields, boundary=multipart_boundary
++                )
++            else:
++                body, content_type = (
++                    urlencode(fields),
++                    "application/x-www-form-urlencoded",
++                )
++
++            extra_kw["body"] = body
++            extra_kw["headers"] = {"Content-Type": content_type}
++
++        extra_kw["headers"].update(headers)
++        extra_kw.update(urlopen_kw)
++
++        return self.urlopen(method, url, **extra_kw)
+Index: venv/Lib/site-packages/urllib3/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/__init__.py	(date 1573549849376)
++++ venv/Lib/site-packages/urllib3/__init__.py	(date 1573549849376)
+@@ -0,0 +1,86 @@
++"""
++urllib3 - Thread-safe connection pooling and re-using.
++"""
++from __future__ import absolute_import
++import warnings
++
++from .connectionpool import HTTPConnectionPool, HTTPSConnectionPool, connection_from_url
++
++from . import exceptions
++from .filepost import encode_multipart_formdata
++from .poolmanager import PoolManager, ProxyManager, proxy_from_url
++from .response import HTTPResponse
++from .util.request import make_headers
++from .util.url import get_host
++from .util.timeout import Timeout
++from .util.retry import Retry
++
++
++# Set default logging handler to avoid "No handler found" warnings.
++import logging
++from logging import NullHandler
++
++__author__ = "Andrey Petrov (andrey.petrov@shazow.net)"
++__license__ = "MIT"
++__version__ = "1.25.7"
++
++__all__ = (
++    "HTTPConnectionPool",
++    "HTTPSConnectionPool",
++    "PoolManager",
++    "ProxyManager",
++    "HTTPResponse",
++    "Retry",
++    "Timeout",
++    "add_stderr_logger",
++    "connection_from_url",
++    "disable_warnings",
++    "encode_multipart_formdata",
++    "get_host",
++    "make_headers",
++    "proxy_from_url",
++)
++
++logging.getLogger(__name__).addHandler(NullHandler())
++
++
++def add_stderr_logger(level=logging.DEBUG):
++    """
++    Helper for quickly adding a StreamHandler to the logger. Useful for
++    debugging.
++
++    Returns the handler after adding it.
++    """
++    # This method needs to be in this __init__.py to get the __name__ correct
++    # even if urllib3 is vendored within another package.
++    logger = logging.getLogger(__name__)
++    handler = logging.StreamHandler()
++    handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
++    logger.addHandler(handler)
++    logger.setLevel(level)
++    logger.debug("Added a stderr logging handler to logger: %s", __name__)
++    return handler
++
++
++# ... Clean up.
++del NullHandler
++
++
++# All warning filters *must* be appended unless you're really certain that they
++# shouldn't be: otherwise, it's very hard for users to use most Python
++# mechanisms to silence them.
++# SecurityWarning's always go off by default.
++warnings.simplefilter("always", exceptions.SecurityWarning, append=True)
++# SubjectAltNameWarning's should go off once per host
++warnings.simplefilter("default", exceptions.SubjectAltNameWarning, append=True)
++# InsecurePlatformWarning's don't vary between requests, so we keep it default.
++warnings.simplefilter("default", exceptions.InsecurePlatformWarning, append=True)
++# SNIMissingWarnings should go off only once.
++warnings.simplefilter("default", exceptions.SNIMissingWarning, append=True)
++
++
++def disable_warnings(category=exceptions.HTTPWarning):
++    """
++    Helper for quickly disabling all urllib3 warnings.
++    """
++    warnings.simplefilter("ignore", category)
+Index: venv/Lib/site-packages/urllib3/util/queue.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/queue.py	(date 1573549849521)
++++ venv/Lib/site-packages/urllib3/util/queue.py	(date 1573549849521)
+@@ -0,0 +1,21 @@
++import collections
++from ..packages import six
++from ..packages.six.moves import queue
++
++if six.PY2:
++    # Queue is imported for side effects on MS Windows. See issue #229.
++    import Queue as _unused_module_Queue  # noqa: F401
++
++
++class LifoQueue(queue.Queue):
++    def _init(self, _):
++        self.queue = collections.deque()
++
++    def _qsize(self, len=len):
++        return len(self.queue)
++
++    def _put(self, item):
++        self.queue.append(item)
++
++    def _get(self):
++        return self.queue.pop()
+Index: venv/Lib/site-packages/urllib3/util/connection.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/connection.py	(date 1573549849516)
++++ venv/Lib/site-packages/urllib3/util/connection.py	(date 1573549849516)
+@@ -0,0 +1,138 @@
++from __future__ import absolute_import
++import socket
++from .wait import NoWayToWaitForSocketError, wait_for_read
++from ..contrib import _appengine_environ
++
++
++def is_connection_dropped(conn):  # Platform-specific
++    """
++    Returns True if the connection is dropped and should be closed.
++
++    :param conn:
++        :class:`httplib.HTTPConnection` object.
++
++    Note: For platforms like AppEngine, this will always return ``False`` to
++    let the platform handle connection recycling transparently for us.
++    """
++    sock = getattr(conn, "sock", False)
++    if sock is False:  # Platform-specific: AppEngine
++        return False
++    if sock is None:  # Connection already closed (such as by httplib).
++        return True
++    try:
++        # Returns True if readable, which here means it's been dropped
++        return wait_for_read(sock, timeout=0.0)
++    except NoWayToWaitForSocketError:  # Platform-specific: AppEngine
++        return False
++
++
++# This function is copied from socket.py in the Python 2.7 standard
++# library test suite. Added to its signature is only `socket_options`.
++# One additional modification is that we avoid binding to IPv6 servers
++# discovered in DNS if the system doesn't have IPv6 functionality.
++def create_connection(
++    address,
++    timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
++    source_address=None,
++    socket_options=None,
++):
++    """Connect to *address* and return the socket object.
++
++    Convenience function.  Connect to *address* (a 2-tuple ``(host,
++    port)``) and return the socket object.  Passing the optional
++    *timeout* parameter will set the timeout on the socket instance
++    before attempting to connect.  If no *timeout* is supplied, the
++    global default timeout setting returned by :func:`getdefaulttimeout`
++    is used.  If *source_address* is set it must be a tuple of (host, port)
++    for the socket to bind as a source address before making the connection.
++    An host of '' or port 0 tells the OS to use the default.
++    """
++
++    host, port = address
++    if host.startswith("["):
++        host = host.strip("[]")
++    err = None
++
++    # Using the value from allowed_gai_family() in the context of getaddrinfo lets
++    # us select whether to work with IPv4 DNS records, IPv6 records, or both.
++    # The original create_connection function always returns all records.
++    family = allowed_gai_family()
++
++    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):
++        af, socktype, proto, canonname, sa = res
++        sock = None
++        try:
++            sock = socket.socket(af, socktype, proto)
++
++            # If provided, set socket level options before connecting.
++            _set_socket_options(sock, socket_options)
++
++            if timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:
++                sock.settimeout(timeout)
++            if source_address:
++                sock.bind(source_address)
++            sock.connect(sa)
++            return sock
++
++        except socket.error as e:
++            err = e
++            if sock is not None:
++                sock.close()
++                sock = None
++
++    if err is not None:
++        raise err
++
++    raise socket.error("getaddrinfo returns an empty list")
++
++
++def _set_socket_options(sock, options):
++    if options is None:
++        return
++
++    for opt in options:
++        sock.setsockopt(*opt)
++
++
++def allowed_gai_family():
++    """This function is designed to work in the context of
++    getaddrinfo, where family=socket.AF_UNSPEC is the default and
++    will perform a DNS search for both IPv6 and IPv4 records."""
++
++    family = socket.AF_INET
++    if HAS_IPV6:
++        family = socket.AF_UNSPEC
++    return family
++
++
++def _has_ipv6(host):
++    """ Returns True if the system can bind an IPv6 address. """
++    sock = None
++    has_ipv6 = False
++
++    # App Engine doesn't support IPV6 sockets and actually has a quota on the
++    # number of sockets that can be used, so just early out here instead of
++    # creating a socket needlessly.
++    # See https://github.com/urllib3/urllib3/issues/1446
++    if _appengine_environ.is_appengine_sandbox():
++        return False
++
++    if socket.has_ipv6:
++        # has_ipv6 returns true if cPython was compiled with IPv6 support.
++        # It does not tell us if the system has IPv6 support enabled. To
++        # determine that we must bind to an IPv6 address.
++        # https://github.com/urllib3/urllib3/pull/611
++        # https://bugs.python.org/issue658327
++        try:
++            sock = socket.socket(socket.AF_INET6)
++            sock.bind((host, 0))
++            has_ipv6 = True
++        except Exception:
++            pass
++
++    if sock:
++        sock.close()
++    return has_ipv6
++
++
++HAS_IPV6 = _has_ipv6("::1")
+Index: venv/Lib/site-packages/urllib3/util/ssl_.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/ssl_.py	(date 1573549849541)
++++ venv/Lib/site-packages/urllib3/util/ssl_.py	(date 1573549849541)
+@@ -0,0 +1,407 @@
++from __future__ import absolute_import
++import errno
++import warnings
++import hmac
++import sys
++
++from binascii import hexlify, unhexlify
++from hashlib import md5, sha1, sha256
++
++from .url import IPV4_RE, BRACELESS_IPV6_ADDRZ_RE
++from ..exceptions import SSLError, InsecurePlatformWarning, SNIMissingWarning
++from ..packages import six
++
++
++SSLContext = None
++HAS_SNI = False
++IS_PYOPENSSL = False
++IS_SECURETRANSPORT = False
++
++# Maps the length of a digest to a possible hash function producing this digest
++HASHFUNC_MAP = {32: md5, 40: sha1, 64: sha256}
++
++
++def _const_compare_digest_backport(a, b):
++    """
++    Compare two digests of equal length in constant time.
++
++    The digests must be of type str/bytes.
++    Returns True if the digests match, and False otherwise.
++    """
++    result = abs(len(a) - len(b))
++    for l, r in zip(bytearray(a), bytearray(b)):
++        result |= l ^ r
++    return result == 0
++
++
++_const_compare_digest = getattr(hmac, "compare_digest", _const_compare_digest_backport)
++
++try:  # Test for SSL features
++    import ssl
++    from ssl import wrap_socket, CERT_REQUIRED
++    from ssl import HAS_SNI  # Has SNI?
++except ImportError:
++    pass
++
++try:  # Platform-specific: Python 3.6
++    from ssl import PROTOCOL_TLS
++
++    PROTOCOL_SSLv23 = PROTOCOL_TLS
++except ImportError:
++    try:
++        from ssl import PROTOCOL_SSLv23 as PROTOCOL_TLS
++
++        PROTOCOL_SSLv23 = PROTOCOL_TLS
++    except ImportError:
++        PROTOCOL_SSLv23 = PROTOCOL_TLS = 2
++
++
++try:
++    from ssl import OP_NO_SSLv2, OP_NO_SSLv3, OP_NO_COMPRESSION
++except ImportError:
++    OP_NO_SSLv2, OP_NO_SSLv3 = 0x1000000, 0x2000000
++    OP_NO_COMPRESSION = 0x20000
++
++
++# A secure default.
++# Sources for more information on TLS ciphers:
++#
++# - https://wiki.mozilla.org/Security/Server_Side_TLS
++# - https://www.ssllabs.com/projects/best-practices/index.html
++# - https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
++#
++# The general intent is:
++# - prefer cipher suites that offer perfect forward secrecy (DHE/ECDHE),
++# - prefer ECDHE over DHE for better performance,
++# - prefer any AES-GCM and ChaCha20 over any AES-CBC for better performance and
++#   security,
++# - prefer AES-GCM over ChaCha20 because hardware-accelerated AES is common,
++# - disable NULL authentication, MD5 MACs, DSS, and other
++#   insecure ciphers for security reasons.
++# - NOTE: TLS 1.3 cipher suites are managed through a different interface
++#   not exposed by CPython (yet!) and are enabled by default if they're available.
++DEFAULT_CIPHERS = ":".join(
++    [
++        "ECDHE+AESGCM",
++        "ECDHE+CHACHA20",
++        "DHE+AESGCM",
++        "DHE+CHACHA20",
++        "ECDH+AESGCM",
++        "DH+AESGCM",
++        "ECDH+AES",
++        "DH+AES",
++        "RSA+AESGCM",
++        "RSA+AES",
++        "!aNULL",
++        "!eNULL",
++        "!MD5",
++        "!DSS",
++    ]
++)
++
++try:
++    from ssl import SSLContext  # Modern SSL?
++except ImportError:
++
++    class SSLContext(object):  # Platform-specific: Python 2
++        def __init__(self, protocol_version):
++            self.protocol = protocol_version
++            # Use default values from a real SSLContext
++            self.check_hostname = False
++            self.verify_mode = ssl.CERT_NONE
++            self.ca_certs = None
++            self.options = 0
++            self.certfile = None
++            self.keyfile = None
++            self.ciphers = None
++
++        def load_cert_chain(self, certfile, keyfile):
++            self.certfile = certfile
++            self.keyfile = keyfile
++
++        def load_verify_locations(self, cafile=None, capath=None):
++            self.ca_certs = cafile
++
++            if capath is not None:
++                raise SSLError("CA directories not supported in older Pythons")
++
++        def set_ciphers(self, cipher_suite):
++            self.ciphers = cipher_suite
++
++        def wrap_socket(self, socket, server_hostname=None, server_side=False):
++            warnings.warn(
++                "A true SSLContext object is not available. This prevents "
++                "urllib3 from configuring SSL appropriately and may cause "
++                "certain SSL connections to fail. You can upgrade to a newer "
++                "version of Python to solve this. For more information, see "
++                "https://urllib3.readthedocs.io/en/latest/advanced-usage.html"
++                "#ssl-warnings",
++                InsecurePlatformWarning,
++            )
++            kwargs = {
++                "keyfile": self.keyfile,
++                "certfile": self.certfile,
++                "ca_certs": self.ca_certs,
++                "cert_reqs": self.verify_mode,
++                "ssl_version": self.protocol,
++                "server_side": server_side,
++            }
++            return wrap_socket(socket, ciphers=self.ciphers, **kwargs)
++
++
++def assert_fingerprint(cert, fingerprint):
++    """
++    Checks if given fingerprint matches the supplied certificate.
++
++    :param cert:
++        Certificate as bytes object.
++    :param fingerprint:
++        Fingerprint as string of hexdigits, can be interspersed by colons.
++    """
++
++    fingerprint = fingerprint.replace(":", "").lower()
++    digest_length = len(fingerprint)
++    hashfunc = HASHFUNC_MAP.get(digest_length)
++    if not hashfunc:
++        raise SSLError("Fingerprint of invalid length: {0}".format(fingerprint))
++
++    # We need encode() here for py32; works on py2 and p33.
++    fingerprint_bytes = unhexlify(fingerprint.encode())
++
++    cert_digest = hashfunc(cert).digest()
++
++    if not _const_compare_digest(cert_digest, fingerprint_bytes):
++        raise SSLError(
++            'Fingerprints did not match. Expected "{0}", got "{1}".'.format(
++                fingerprint, hexlify(cert_digest)
++            )
++        )
++
++
++def resolve_cert_reqs(candidate):
++    """
++    Resolves the argument to a numeric constant, which can be passed to
++    the wrap_socket function/method from the ssl module.
++    Defaults to :data:`ssl.CERT_NONE`.
++    If given a string it is assumed to be the name of the constant in the
++    :mod:`ssl` module or its abbreviation.
++    (So you can specify `REQUIRED` instead of `CERT_REQUIRED`.
++    If it's neither `None` nor a string we assume it is already the numeric
++    constant which can directly be passed to wrap_socket.
++    """
++    if candidate is None:
++        return CERT_REQUIRED
++
++    if isinstance(candidate, str):
++        res = getattr(ssl, candidate, None)
++        if res is None:
++            res = getattr(ssl, "CERT_" + candidate)
++        return res
++
++    return candidate
++
++
++def resolve_ssl_version(candidate):
++    """
++    like resolve_cert_reqs
++    """
++    if candidate is None:
++        return PROTOCOL_TLS
++
++    if isinstance(candidate, str):
++        res = getattr(ssl, candidate, None)
++        if res is None:
++            res = getattr(ssl, "PROTOCOL_" + candidate)
++        return res
++
++    return candidate
++
++
++def create_urllib3_context(
++    ssl_version=None, cert_reqs=None, options=None, ciphers=None
++):
++    """All arguments have the same meaning as ``ssl_wrap_socket``.
++
++    By default, this function does a lot of the same work that
++    ``ssl.create_default_context`` does on Python 3.4+. It:
++
++    - Disables SSLv2, SSLv3, and compression
++    - Sets a restricted set of server ciphers
++
++    If you wish to enable SSLv3, you can do::
++
++        from urllib3.util import ssl_
++        context = ssl_.create_urllib3_context()
++        context.options &= ~ssl_.OP_NO_SSLv3
++
++    You can do the same to enable compression (substituting ``COMPRESSION``
++    for ``SSLv3`` in the last line above).
++
++    :param ssl_version:
++        The desired protocol version to use. This will default to
++        PROTOCOL_SSLv23 which will negotiate the highest protocol that both
++        the server and your installation of OpenSSL support.
++    :param cert_reqs:
++        Whether to require the certificate verification. This defaults to
++        ``ssl.CERT_REQUIRED``.
++    :param options:
++        Specific OpenSSL options. These default to ``ssl.OP_NO_SSLv2``,
++        ``ssl.OP_NO_SSLv3``, ``ssl.OP_NO_COMPRESSION``.
++    :param ciphers:
++        Which cipher suites to allow the server to select.
++    :returns:
++        Constructed SSLContext object with specified options
++    :rtype: SSLContext
++    """
++    context = SSLContext(ssl_version or PROTOCOL_TLS)
++
++    context.set_ciphers(ciphers or DEFAULT_CIPHERS)
++
++    # Setting the default here, as we may have no ssl module on import
++    cert_reqs = ssl.CERT_REQUIRED if cert_reqs is None else cert_reqs
++
++    if options is None:
++        options = 0
++        # SSLv2 is easily broken and is considered harmful and dangerous
++        options |= OP_NO_SSLv2
++        # SSLv3 has several problems and is now dangerous
++        options |= OP_NO_SSLv3
++        # Disable compression to prevent CRIME attacks for OpenSSL 1.0+
++        # (issue #309)
++        options |= OP_NO_COMPRESSION
++
++    context.options |= options
++
++    # Enable post-handshake authentication for TLS 1.3, see GH #1634. PHA is
++    # necessary for conditional client cert authentication with TLS 1.3.
++    # The attribute is None for OpenSSL <= 1.1.0 or does not exist in older
++    # versions of Python.  We only enable on Python 3.7.4+ or if certificate
++    # verification is enabled to work around Python issue #37428
++    # See: https://bugs.python.org/issue37428
++    if (cert_reqs == ssl.CERT_REQUIRED or sys.version_info >= (3, 7, 4)) and getattr(
++        context, "post_handshake_auth", None
++    ) is not None:
++        context.post_handshake_auth = True
++
++    context.verify_mode = cert_reqs
++    if (
++        getattr(context, "check_hostname", None) is not None
++    ):  # Platform-specific: Python 3.2
++        # We do our own verification, including fingerprints and alternative
++        # hostnames. So disable it here
++        context.check_hostname = False
++    return context
++
++
++def ssl_wrap_socket(
++    sock,
++    keyfile=None,
++    certfile=None,
++    cert_reqs=None,
++    ca_certs=None,
++    server_hostname=None,
++    ssl_version=None,
++    ciphers=None,
++    ssl_context=None,
++    ca_cert_dir=None,
++    key_password=None,
++):
++    """
++    All arguments except for server_hostname, ssl_context, and ca_cert_dir have
++    the same meaning as they do when using :func:`ssl.wrap_socket`.
++
++    :param server_hostname:
++        When SNI is supported, the expected hostname of the certificate
++    :param ssl_context:
++        A pre-made :class:`SSLContext` object. If none is provided, one will
++        be created using :func:`create_urllib3_context`.
++    :param ciphers:
++        A string of ciphers we wish the client to support.
++    :param ca_cert_dir:
++        A directory containing CA certificates in multiple separate files, as
++        supported by OpenSSL's -CApath flag or the capath argument to
++        SSLContext.load_verify_locations().
++    :param key_password:
++        Optional password if the keyfile is encrypted.
++    """
++    context = ssl_context
++    if context is None:
++        # Note: This branch of code and all the variables in it are no longer
++        # used by urllib3 itself. We should consider deprecating and removing
++        # this code.
++        context = create_urllib3_context(ssl_version, cert_reqs, ciphers=ciphers)
++
++    if ca_certs or ca_cert_dir:
++        try:
++            context.load_verify_locations(ca_certs, ca_cert_dir)
++        except IOError as e:  # Platform-specific: Python 2.7
++            raise SSLError(e)
++        # Py33 raises FileNotFoundError which subclasses OSError
++        # These are not equivalent unless we check the errno attribute
++        except OSError as e:  # Platform-specific: Python 3.3 and beyond
++            if e.errno == errno.ENOENT:
++                raise SSLError(e)
++            raise
++
++    elif ssl_context is None and hasattr(context, "load_default_certs"):
++        # try to load OS default certs; works well on Windows (require Python3.4+)
++        context.load_default_certs()
++
++    # Attempt to detect if we get the goofy behavior of the
++    # keyfile being encrypted and OpenSSL asking for the
++    # passphrase via the terminal and instead error out.
++    if keyfile and key_password is None and _is_key_file_encrypted(keyfile):
++        raise SSLError("Client private key is encrypted, password is required")
++
++    if certfile:
++        if key_password is None:
++            context.load_cert_chain(certfile, keyfile)
++        else:
++            context.load_cert_chain(certfile, keyfile, key_password)
++
++    # If we detect server_hostname is an IP address then the SNI
++    # extension should not be used according to RFC3546 Section 3.1
++    # We shouldn't warn the user if SNI isn't available but we would
++    # not be using SNI anyways due to IP address for server_hostname.
++    if (
++        server_hostname is not None and not is_ipaddress(server_hostname)
++    ) or IS_SECURETRANSPORT:
++        if HAS_SNI and server_hostname is not None:
++            return context.wrap_socket(sock, server_hostname=server_hostname)
++
++        warnings.warn(
++            "An HTTPS request has been made, but the SNI (Server Name "
++            "Indication) extension to TLS is not available on this platform. "
++            "This may cause the server to present an incorrect TLS "
++            "certificate, which can cause validation failures. You can upgrade to "
++            "a newer version of Python to solve this. For more information, see "
++            "https://urllib3.readthedocs.io/en/latest/advanced-usage.html"
++            "#ssl-warnings",
++            SNIMissingWarning,
++        )
++
++    return context.wrap_socket(sock)
++
++
++def is_ipaddress(hostname):
++    """Detects whether the hostname given is an IPv4 or IPv6 address.
++    Also detects IPv6 addresses with Zone IDs.
++
++    :param str hostname: Hostname to examine.
++    :return: True if the hostname is an IP address, False otherwise.
++    """
++    if not six.PY2 and isinstance(hostname, bytes):
++        # IDN A-label bytes are ASCII compatible.
++        hostname = hostname.decode("ascii")
++    return bool(IPV4_RE.match(hostname) or BRACELESS_IPV6_ADDRZ_RE.match(hostname))
++
++
++def _is_key_file_encrypted(key_file):
++    """Detects if a key file is encrypted or not."""
++    with open(key_file, "r") as f:
++        for line in f:
++            # Look for Proc-Type: 4,ENCRYPTED
++            if "ENCRYPTED" in line:
++                return True
++
++    return False
+Index: venv/Lib/site-packages/urllib3/util/retry.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/retry.py	(date 1573549849536)
++++ venv/Lib/site-packages/urllib3/util/retry.py	(date 1573549849536)
+@@ -0,0 +1,450 @@
++from __future__ import absolute_import
++import time
++import logging
++from collections import namedtuple
++from itertools import takewhile
++import email
++import re
++
++from ..exceptions import (
++    ConnectTimeoutError,
++    MaxRetryError,
++    ProtocolError,
++    ReadTimeoutError,
++    ResponseError,
++    InvalidHeader,
++)
++from ..packages import six
++
++
++log = logging.getLogger(__name__)
++
++
++# Data structure for representing the metadata of requests that result in a retry.
++RequestHistory = namedtuple(
++    "RequestHistory", ["method", "url", "error", "status", "redirect_location"]
++)
++
++
++class Retry(object):
++    """ Retry configuration.
++
++    Each retry attempt will create a new Retry object with updated values, so
++    they can be safely reused.
++
++    Retries can be defined as a default for a pool::
++
++        retries = Retry(connect=5, read=2, redirect=5)
++        http = PoolManager(retries=retries)
++        response = http.request('GET', 'http://example.com/')
++
++    Or per-request (which overrides the default for the pool)::
++
++        response = http.request('GET', 'http://example.com/', retries=Retry(10))
++
++    Retries can be disabled by passing ``False``::
++
++        response = http.request('GET', 'http://example.com/', retries=False)
++
++    Errors will be wrapped in :class:`~urllib3.exceptions.MaxRetryError` unless
++    retries are disabled, in which case the causing exception will be raised.
++
++    :param int total:
++        Total number of retries to allow. Takes precedence over other counts.
++
++        Set to ``None`` to remove this constraint and fall back on other
++        counts. It's a good idea to set this to some sensibly-high value to
++        account for unexpected edge cases and avoid infinite retry loops.
++
++        Set to ``0`` to fail on the first retry.
++
++        Set to ``False`` to disable and imply ``raise_on_redirect=False``.
++
++    :param int connect:
++        How many connection-related errors to retry on.
++
++        These are errors raised before the request is sent to the remote server,
++        which we assume has not triggered the server to process the request.
++
++        Set to ``0`` to fail on the first retry of this type.
++
++    :param int read:
++        How many times to retry on read errors.
++
++        These errors are raised after the request was sent to the server, so the
++        request may have side-effects.
++
++        Set to ``0`` to fail on the first retry of this type.
++
++    :param int redirect:
++        How many redirects to perform. Limit this to avoid infinite redirect
++        loops.
++
++        A redirect is a HTTP response with a status code 301, 302, 303, 307 or
++        308.
++
++        Set to ``0`` to fail on the first retry of this type.
++
++        Set to ``False`` to disable and imply ``raise_on_redirect=False``.
++
++    :param int status:
++        How many times to retry on bad status codes.
++
++        These are retries made on responses, where status code matches
++        ``status_forcelist``.
++
++        Set to ``0`` to fail on the first retry of this type.
++
++    :param iterable method_whitelist:
++        Set of uppercased HTTP method verbs that we should retry on.
++
++        By default, we only retry on methods which are considered to be
++        idempotent (multiple requests with the same parameters end with the
++        same state). See :attr:`Retry.DEFAULT_METHOD_WHITELIST`.
++
++        Set to a ``False`` value to retry on any verb.
++
++    :param iterable status_forcelist:
++        A set of integer HTTP status codes that we should force a retry on.
++        A retry is initiated if the request method is in ``method_whitelist``
++        and the response status code is in ``status_forcelist``.
++
++        By default, this is disabled with ``None``.
++
++    :param float backoff_factor:
++        A backoff factor to apply between attempts after the second try
++        (most errors are resolved immediately by a second try without a
++        delay). urllib3 will sleep for::
++
++            {backoff factor} * (2 ** ({number of total retries} - 1))
++
++        seconds. If the backoff_factor is 0.1, then :func:`.sleep` will sleep
++        for [0.0s, 0.2s, 0.4s, ...] between retries. It will never be longer
++        than :attr:`Retry.BACKOFF_MAX`.
++
++        By default, backoff is disabled (set to 0).
++
++    :param bool raise_on_redirect: Whether, if the number of redirects is
++        exhausted, to raise a MaxRetryError, or to return a response with a
++        response code in the 3xx range.
++
++    :param bool raise_on_status: Similar meaning to ``raise_on_redirect``:
++        whether we should raise an exception, or return a response,
++        if status falls in ``status_forcelist`` range and retries have
++        been exhausted.
++
++    :param tuple history: The history of the request encountered during
++        each call to :meth:`~Retry.increment`. The list is in the order
++        the requests occurred. Each list item is of class :class:`RequestHistory`.
++
++    :param bool respect_retry_after_header:
++        Whether to respect Retry-After header on status codes defined as
++        :attr:`Retry.RETRY_AFTER_STATUS_CODES` or not.
++
++    :param iterable remove_headers_on_redirect:
++        Sequence of headers to remove from the request when a response
++        indicating a redirect is returned before firing off the redirected
++        request.
++    """
++
++    DEFAULT_METHOD_WHITELIST = frozenset(
++        ["HEAD", "GET", "PUT", "DELETE", "OPTIONS", "TRACE"]
++    )
++
++    RETRY_AFTER_STATUS_CODES = frozenset([413, 429, 503])
++
++    DEFAULT_REDIRECT_HEADERS_BLACKLIST = frozenset(["Authorization"])
++
++    #: Maximum backoff time.
++    BACKOFF_MAX = 120
++
++    def __init__(
++        self,
++        total=10,
++        connect=None,
++        read=None,
++        redirect=None,
++        status=None,
++        method_whitelist=DEFAULT_METHOD_WHITELIST,
++        status_forcelist=None,
++        backoff_factor=0,
++        raise_on_redirect=True,
++        raise_on_status=True,
++        history=None,
++        respect_retry_after_header=True,
++        remove_headers_on_redirect=DEFAULT_REDIRECT_HEADERS_BLACKLIST,
++    ):
++
++        self.total = total
++        self.connect = connect
++        self.read = read
++        self.status = status
++
++        if redirect is False or total is False:
++            redirect = 0
++            raise_on_redirect = False
++
++        self.redirect = redirect
++        self.status_forcelist = status_forcelist or set()
++        self.method_whitelist = method_whitelist
++        self.backoff_factor = backoff_factor
++        self.raise_on_redirect = raise_on_redirect
++        self.raise_on_status = raise_on_status
++        self.history = history or tuple()
++        self.respect_retry_after_header = respect_retry_after_header
++        self.remove_headers_on_redirect = frozenset(
++            [h.lower() for h in remove_headers_on_redirect]
++        )
++
++    def new(self, **kw):
++        params = dict(
++            total=self.total,
++            connect=self.connect,
++            read=self.read,
++            redirect=self.redirect,
++            status=self.status,
++            method_whitelist=self.method_whitelist,
++            status_forcelist=self.status_forcelist,
++            backoff_factor=self.backoff_factor,
++            raise_on_redirect=self.raise_on_redirect,
++            raise_on_status=self.raise_on_status,
++            history=self.history,
++            remove_headers_on_redirect=self.remove_headers_on_redirect,
++            respect_retry_after_header=self.respect_retry_after_header,
++        )
++        params.update(kw)
++        return type(self)(**params)
++
++    @classmethod
++    def from_int(cls, retries, redirect=True, default=None):
++        """ Backwards-compatibility for the old retries format."""
++        if retries is None:
++            retries = default if default is not None else cls.DEFAULT
++
++        if isinstance(retries, Retry):
++            return retries
++
++        redirect = bool(redirect) and None
++        new_retries = cls(retries, redirect=redirect)
++        log.debug("Converted retries value: %r -> %r", retries, new_retries)
++        return new_retries
++
++    def get_backoff_time(self):
++        """ Formula for computing the current backoff
++
++        :rtype: float
++        """
++        # We want to consider only the last consecutive errors sequence (Ignore redirects).
++        consecutive_errors_len = len(
++            list(
++                takewhile(lambda x: x.redirect_location is None, reversed(self.history))
++            )
++        )
++        if consecutive_errors_len <= 1:
++            return 0
++
++        backoff_value = self.backoff_factor * (2 ** (consecutive_errors_len - 1))
++        return min(self.BACKOFF_MAX, backoff_value)
++
++    def parse_retry_after(self, retry_after):
++        # Whitespace: https://tools.ietf.org/html/rfc7230#section-3.2.4
++        if re.match(r"^\s*[0-9]+\s*$", retry_after):
++            seconds = int(retry_after)
++        else:
++            retry_date_tuple = email.utils.parsedate(retry_after)
++            if retry_date_tuple is None:
++                raise InvalidHeader("Invalid Retry-After header: %s" % retry_after)
++            retry_date = time.mktime(retry_date_tuple)
++            seconds = retry_date - time.time()
++
++        if seconds < 0:
++            seconds = 0
++
++        return seconds
++
++    def get_retry_after(self, response):
++        """ Get the value of Retry-After in seconds. """
++
++        retry_after = response.getheader("Retry-After")
++
++        if retry_after is None:
++            return None
++
++        return self.parse_retry_after(retry_after)
++
++    def sleep_for_retry(self, response=None):
++        retry_after = self.get_retry_after(response)
++        if retry_after:
++            time.sleep(retry_after)
++            return True
++
++        return False
++
++    def _sleep_backoff(self):
++        backoff = self.get_backoff_time()
++        if backoff <= 0:
++            return
++        time.sleep(backoff)
++
++    def sleep(self, response=None):
++        """ Sleep between retry attempts.
++
++        This method will respect a server's ``Retry-After`` response header
++        and sleep the duration of the time requested. If that is not present, it
++        will use an exponential backoff. By default, the backoff factor is 0 and
++        this method will return immediately.
++        """
++
++        if self.respect_retry_after_header and response:
++            slept = self.sleep_for_retry(response)
++            if slept:
++                return
++
++        self._sleep_backoff()
++
++    def _is_connection_error(self, err):
++        """ Errors when we're fairly sure that the server did not receive the
++        request, so it should be safe to retry.
++        """
++        return isinstance(err, ConnectTimeoutError)
++
++    def _is_read_error(self, err):
++        """ Errors that occur after the request has been started, so we should
++        assume that the server began processing it.
++        """
++        return isinstance(err, (ReadTimeoutError, ProtocolError))
++
++    def _is_method_retryable(self, method):
++        """ Checks if a given HTTP method should be retried upon, depending if
++        it is included on the method whitelist.
++        """
++        if self.method_whitelist and method.upper() not in self.method_whitelist:
++            return False
++
++        return True
++
++    def is_retry(self, method, status_code, has_retry_after=False):
++        """ Is this method/status code retryable? (Based on whitelists and control
++        variables such as the number of total retries to allow, whether to
++        respect the Retry-After header, whether this header is present, and
++        whether the returned status code is on the list of status codes to
++        be retried upon on the presence of the aforementioned header)
++        """
++        if not self._is_method_retryable(method):
++            return False
++
++        if self.status_forcelist and status_code in self.status_forcelist:
++            return True
++
++        return (
++            self.total
++            and self.respect_retry_after_header
++            and has_retry_after
++            and (status_code in self.RETRY_AFTER_STATUS_CODES)
++        )
++
++    def is_exhausted(self):
++        """ Are we out of retries? """
++        retry_counts = (self.total, self.connect, self.read, self.redirect, self.status)
++        retry_counts = list(filter(None, retry_counts))
++        if not retry_counts:
++            return False
++
++        return min(retry_counts) < 0
++
++    def increment(
++        self,
++        method=None,
++        url=None,
++        response=None,
++        error=None,
++        _pool=None,
++        _stacktrace=None,
++    ):
++        """ Return a new Retry object with incremented retry counters.
++
++        :param response: A response object, or None, if the server did not
++            return a response.
++        :type response: :class:`~urllib3.response.HTTPResponse`
++        :param Exception error: An error encountered during the request, or
++            None if the response was received successfully.
++
++        :return: A new ``Retry`` object.
++        """
++        if self.total is False and error:
++            # Disabled, indicate to re-raise the error.
++            raise six.reraise(type(error), error, _stacktrace)
++
++        total = self.total
++        if total is not None:
++            total -= 1
++
++        connect = self.connect
++        read = self.read
++        redirect = self.redirect
++        status_count = self.status
++        cause = "unknown"
++        status = None
++        redirect_location = None
++
++        if error and self._is_connection_error(error):
++            # Connect retry?
++            if connect is False:
++                raise six.reraise(type(error), error, _stacktrace)
++            elif connect is not None:
++                connect -= 1
++
++        elif error and self._is_read_error(error):
++            # Read retry?
++            if read is False or not self._is_method_retryable(method):
++                raise six.reraise(type(error), error, _stacktrace)
++            elif read is not None:
++                read -= 1
++
++        elif response and response.get_redirect_location():
++            # Redirect retry?
++            if redirect is not None:
++                redirect -= 1
++            cause = "too many redirects"
++            redirect_location = response.get_redirect_location()
++            status = response.status
++
++        else:
++            # Incrementing because of a server error like a 500 in
++            # status_forcelist and a the given method is in the whitelist
++            cause = ResponseError.GENERIC_ERROR
++            if response and response.status:
++                if status_count is not None:
++                    status_count -= 1
++                cause = ResponseError.SPECIFIC_ERROR.format(status_code=response.status)
++                status = response.status
++
++        history = self.history + (
++            RequestHistory(method, url, error, status, redirect_location),
++        )
++
++        new_retry = self.new(
++            total=total,
++            connect=connect,
++            read=read,
++            redirect=redirect,
++            status=status_count,
++            history=history,
++        )
++
++        if new_retry.is_exhausted():
++            raise MaxRetryError(_pool, url, error or ResponseError(cause))
++
++        log.debug("Incremented Retry for (url='%s'): %r", url, new_retry)
++
++        return new_retry
++
++    def __repr__(self):
++        return (
++            "{cls.__name__}(total={self.total}, connect={self.connect}, "
++            "read={self.read}, redirect={self.redirect}, status={self.status})"
++        ).format(cls=type(self), self=self)
++
++
++# For backwards compatibility (equivalent to pre-v1.9):
++Retry.DEFAULT = Retry(3)
+Index: venv/Lib/site-packages/urllib3/util/url.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/url.py	(date 1573549849552)
++++ venv/Lib/site-packages/urllib3/util/url.py	(date 1573549849552)
+@@ -0,0 +1,436 @@
++from __future__ import absolute_import
++import re
++from collections import namedtuple
++
++from ..exceptions import LocationParseError
++from ..packages import six
++
++
++url_attrs = ["scheme", "auth", "host", "port", "path", "query", "fragment"]
++
++# We only want to normalize urls with an HTTP(S) scheme.
++# urllib3 infers URLs without a scheme (None) to be http.
++NORMALIZABLE_SCHEMES = ("http", "https", None)
++
++# Almost all of these patterns were derived from the
++# 'rfc3986' module: https://github.com/python-hyper/rfc3986
++PERCENT_RE = re.compile(r"%[a-fA-F0-9]{2}")
++SCHEME_RE = re.compile(r"^(?:[a-zA-Z][a-zA-Z0-9+-]*:|/)")
++URI_RE = re.compile(
++    r"^(?:([a-zA-Z][a-zA-Z0-9+.-]*):)?"
++    r"(?://([^/?#]*))?"
++    r"([^?#]*)"
++    r"(?:\?([^#]*))?"
++    r"(?:#(.*))?$",
++    re.UNICODE | re.DOTALL,
++)
++
++IPV4_PAT = r"(?:[0-9]{1,3}\.){3}[0-9]{1,3}"
++HEX_PAT = "[0-9A-Fa-f]{1,4}"
++LS32_PAT = "(?:{hex}:{hex}|{ipv4})".format(hex=HEX_PAT, ipv4=IPV4_PAT)
++_subs = {"hex": HEX_PAT, "ls32": LS32_PAT}
++_variations = [
++    #                            6( h16 ":" ) ls32
++    "(?:%(hex)s:){6}%(ls32)s",
++    #                       "::" 5( h16 ":" ) ls32
++    "::(?:%(hex)s:){5}%(ls32)s",
++    # [               h16 ] "::" 4( h16 ":" ) ls32
++    "(?:%(hex)s)?::(?:%(hex)s:){4}%(ls32)s",
++    # [ *1( h16 ":" ) h16 ] "::" 3( h16 ":" ) ls32
++    "(?:(?:%(hex)s:)?%(hex)s)?::(?:%(hex)s:){3}%(ls32)s",
++    # [ *2( h16 ":" ) h16 ] "::" 2( h16 ":" ) ls32
++    "(?:(?:%(hex)s:){0,2}%(hex)s)?::(?:%(hex)s:){2}%(ls32)s",
++    # [ *3( h16 ":" ) h16 ] "::"    h16 ":"   ls32
++    "(?:(?:%(hex)s:){0,3}%(hex)s)?::%(hex)s:%(ls32)s",
++    # [ *4( h16 ":" ) h16 ] "::"              ls32
++    "(?:(?:%(hex)s:){0,4}%(hex)s)?::%(ls32)s",
++    # [ *5( h16 ":" ) h16 ] "::"              h16
++    "(?:(?:%(hex)s:){0,5}%(hex)s)?::%(hex)s",
++    # [ *6( h16 ":" ) h16 ] "::"
++    "(?:(?:%(hex)s:){0,6}%(hex)s)?::",
++]
++
++UNRESERVED_PAT = r"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._!\-~"
++IPV6_PAT = "(?:" + "|".join([x % _subs for x in _variations]) + ")"
++ZONE_ID_PAT = "(?:%25|%)(?:[" + UNRESERVED_PAT + "]|%[a-fA-F0-9]{2})+"
++IPV6_ADDRZ_PAT = r"\[" + IPV6_PAT + r"(?:" + ZONE_ID_PAT + r")?\]"
++REG_NAME_PAT = r"(?:[^\[\]%:/?#]|%[a-fA-F0-9]{2})*"
++TARGET_RE = re.compile(r"^(/[^?#]*)(?:\?([^#]*))?(?:#.*)?$")
++
++IPV4_RE = re.compile("^" + IPV4_PAT + "$")
++IPV6_RE = re.compile("^" + IPV6_PAT + "$")
++IPV6_ADDRZ_RE = re.compile("^" + IPV6_ADDRZ_PAT + "$")
++BRACELESS_IPV6_ADDRZ_RE = re.compile("^" + IPV6_ADDRZ_PAT[2:-2] + "$")
++ZONE_ID_RE = re.compile("(" + ZONE_ID_PAT + r")\]$")
++
++SUBAUTHORITY_PAT = (u"^(?:(.*)@)?(%s|%s|%s)(?::([0-9]{0,5}))?$") % (
++    REG_NAME_PAT,
++    IPV4_PAT,
++    IPV6_ADDRZ_PAT,
++)
++SUBAUTHORITY_RE = re.compile(SUBAUTHORITY_PAT, re.UNICODE | re.DOTALL)
++
++UNRESERVED_CHARS = set(
++    "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._-~"
++)
++SUB_DELIM_CHARS = set("!$&'()*+,;=")
++USERINFO_CHARS = UNRESERVED_CHARS | SUB_DELIM_CHARS | {":"}
++PATH_CHARS = USERINFO_CHARS | {"@", "/"}
++QUERY_CHARS = FRAGMENT_CHARS = PATH_CHARS | {"?"}
++
++
++class Url(namedtuple("Url", url_attrs)):
++    """
++    Data structure for representing an HTTP URL. Used as a return value for
++    :func:`parse_url`. Both the scheme and host are normalized as they are
++    both case-insensitive according to RFC 3986.
++    """
++
++    __slots__ = ()
++
++    def __new__(
++        cls,
++        scheme=None,
++        auth=None,
++        host=None,
++        port=None,
++        path=None,
++        query=None,
++        fragment=None,
++    ):
++        if path and not path.startswith("/"):
++            path = "/" + path
++        if scheme is not None:
++            scheme = scheme.lower()
++        return super(Url, cls).__new__(
++            cls, scheme, auth, host, port, path, query, fragment
++        )
++
++    @property
++    def hostname(self):
++        """For backwards-compatibility with urlparse. We're nice like that."""
++        return self.host
++
++    @property
++    def request_uri(self):
++        """Absolute path including the query string."""
++        uri = self.path or "/"
++
++        if self.query is not None:
++            uri += "?" + self.query
++
++        return uri
++
++    @property
++    def netloc(self):
++        """Network location including host and port"""
++        if self.port:
++            return "%s:%d" % (self.host, self.port)
++        return self.host
++
++    @property
++    def url(self):
++        """
++        Convert self into a url
++
++        This function should more or less round-trip with :func:`.parse_url`. The
++        returned url may not be exactly the same as the url inputted to
++        :func:`.parse_url`, but it should be equivalent by the RFC (e.g., urls
++        with a blank port will have : removed).
++
++        Example: ::
++
++            >>> U = parse_url('http://google.com/mail/')
++            >>> U.url
++            'http://google.com/mail/'
++            >>> Url('http', 'username:password', 'host.com', 80,
++            ... '/path', 'query', 'fragment').url
++            'http://username:password@host.com:80/path?query#fragment'
++        """
++        scheme, auth, host, port, path, query, fragment = self
++        url = u""
++
++        # We use "is not None" we want things to happen with empty strings (or 0 port)
++        if scheme is not None:
++            url += scheme + u"://"
++        if auth is not None:
++            url += auth + u"@"
++        if host is not None:
++            url += host
++        if port is not None:
++            url += u":" + str(port)
++        if path is not None:
++            url += path
++        if query is not None:
++            url += u"?" + query
++        if fragment is not None:
++            url += u"#" + fragment
++
++        return url
++
++    def __str__(self):
++        return self.url
++
++
++def split_first(s, delims):
++    """
++    .. deprecated:: 1.25
++
++    Given a string and an iterable of delimiters, split on the first found
++    delimiter. Return two split parts and the matched delimiter.
++
++    If not found, then the first part is the full input string.
++
++    Example::
++
++        >>> split_first('foo/bar?baz', '?/=')
++        ('foo', 'bar?baz', '/')
++        >>> split_first('foo/bar?baz', '123')
++        ('foo/bar?baz', '', None)
++
++    Scales linearly with number of delims. Not ideal for large number of delims.
++    """
++    min_idx = None
++    min_delim = None
++    for d in delims:
++        idx = s.find(d)
++        if idx < 0:
++            continue
++
++        if min_idx is None or idx < min_idx:
++            min_idx = idx
++            min_delim = d
++
++    if min_idx is None or min_idx < 0:
++        return s, "", None
++
++    return s[:min_idx], s[min_idx + 1 :], min_delim
++
++
++def _encode_invalid_chars(component, allowed_chars, encoding="utf-8"):
++    """Percent-encodes a URI component without reapplying
++    onto an already percent-encoded component.
++    """
++    if component is None:
++        return component
++
++    component = six.ensure_text(component)
++
++    # Try to see if the component we're encoding is already percent-encoded
++    # so we can skip all '%' characters but still encode all others.
++    percent_encodings = PERCENT_RE.findall(component)
++
++    # Normalize existing percent-encoded bytes.
++    for enc in percent_encodings:
++        if not enc.isupper():
++            component = component.replace(enc, enc.upper())
++
++    uri_bytes = component.encode("utf-8", "surrogatepass")
++    is_percent_encoded = len(percent_encodings) == uri_bytes.count(b"%")
++
++    encoded_component = bytearray()
++
++    for i in range(0, len(uri_bytes)):
++        # Will return a single character bytestring on both Python 2 & 3
++        byte = uri_bytes[i : i + 1]
++        byte_ord = ord(byte)
++        if (is_percent_encoded and byte == b"%") or (
++            byte_ord < 128 and byte.decode() in allowed_chars
++        ):
++            encoded_component.extend(byte)
++            continue
++        encoded_component.extend(b"%" + (hex(byte_ord)[2:].encode().zfill(2).upper()))
++
++    return encoded_component.decode(encoding)
++
++
++def _remove_path_dot_segments(path):
++    # See http://tools.ietf.org/html/rfc3986#section-5.2.4 for pseudo-code
++    segments = path.split("/")  # Turn the path into a list of segments
++    output = []  # Initialize the variable to use to store output
++
++    for segment in segments:
++        # '.' is the current directory, so ignore it, it is superfluous
++        if segment == ".":
++            continue
++        # Anything other than '..', should be appended to the output
++        elif segment != "..":
++            output.append(segment)
++        # In this case segment == '..', if we can, we should pop the last
++        # element
++        elif output:
++            output.pop()
++
++    # If the path starts with '/' and the output is empty or the first string
++    # is non-empty
++    if path.startswith("/") and (not output or output[0]):
++        output.insert(0, "")
++
++    # If the path starts with '/.' or '/..' ensure we add one more empty
++    # string to add a trailing '/'
++    if path.endswith(("/.", "/..")):
++        output.append("")
++
++    return "/".join(output)
++
++
++def _normalize_host(host, scheme):
++    if host:
++        if isinstance(host, six.binary_type):
++            host = six.ensure_str(host)
++
++        if scheme in NORMALIZABLE_SCHEMES:
++            is_ipv6 = IPV6_ADDRZ_RE.match(host)
++            if is_ipv6:
++                match = ZONE_ID_RE.search(host)
++                if match:
++                    start, end = match.span(1)
++                    zone_id = host[start:end]
++
++                    if zone_id.startswith("%25") and zone_id != "%25":
++                        zone_id = zone_id[3:]
++                    else:
++                        zone_id = zone_id[1:]
++                    zone_id = "%" + _encode_invalid_chars(zone_id, UNRESERVED_CHARS)
++                    return host[:start].lower() + zone_id + host[end:]
++                else:
++                    return host.lower()
++            elif not IPV4_RE.match(host):
++                return six.ensure_str(
++                    b".".join([_idna_encode(label) for label in host.split(".")])
++                )
++    return host
++
++
++def _idna_encode(name):
++    if name and any([ord(x) > 128 for x in name]):
++        try:
++            import idna
++        except ImportError:
++            six.raise_from(
++                LocationParseError("Unable to parse URL without the 'idna' module"),
++                None,
++            )
++        try:
++            return idna.encode(name.lower(), strict=True, std3_rules=True)
++        except idna.IDNAError:
++            six.raise_from(
++                LocationParseError(u"Name '%s' is not a valid IDNA label" % name), None
++            )
++    return name.lower().encode("ascii")
++
++
++def _encode_target(target):
++    """Percent-encodes a request target so that there are no invalid characters"""
++    if not target.startswith("/"):
++        return target
++
++    path, query = TARGET_RE.match(target).groups()
++    target = _encode_invalid_chars(path, PATH_CHARS)
++    query = _encode_invalid_chars(query, QUERY_CHARS)
++    if query is not None:
++        target += "?" + query
++    return target
++
++
++def parse_url(url):
++    """
++    Given a url, return a parsed :class:`.Url` namedtuple. Best-effort is
++    performed to parse incomplete urls. Fields not provided will be None.
++    This parser is RFC 3986 compliant.
++
++    The parser logic and helper functions are based heavily on
++    work done in the ``rfc3986`` module.
++
++    :param str url: URL to parse into a :class:`.Url` namedtuple.
++
++    Partly backwards-compatible with :mod:`urlparse`.
++
++    Example::
++
++        >>> parse_url('http://google.com/mail/')
++        Url(scheme='http', host='google.com', port=None, path='/mail/', ...)
++        >>> parse_url('google.com:80')
++        Url(scheme=None, host='google.com', port=80, path=None, ...)
++        >>> parse_url('/foo?bar')
++        Url(scheme=None, host=None, port=None, path='/foo', query='bar', ...)
++    """
++    if not url:
++        # Empty
++        return Url()
++
++    source_url = url
++    if not SCHEME_RE.search(url):
++        url = "//" + url
++
++    try:
++        scheme, authority, path, query, fragment = URI_RE.match(url).groups()
++        normalize_uri = scheme is None or scheme.lower() in NORMALIZABLE_SCHEMES
++
++        if scheme:
++            scheme = scheme.lower()
++
++        if authority:
++            auth, host, port = SUBAUTHORITY_RE.match(authority).groups()
++            if auth and normalize_uri:
++                auth = _encode_invalid_chars(auth, USERINFO_CHARS)
++            if port == "":
++                port = None
++        else:
++            auth, host, port = None, None, None
++
++        if port is not None:
++            port = int(port)
++            if not (0 <= port <= 65535):
++                raise LocationParseError(url)
++
++        host = _normalize_host(host, scheme)
++
++        if normalize_uri and path:
++            path = _remove_path_dot_segments(path)
++            path = _encode_invalid_chars(path, PATH_CHARS)
++        if normalize_uri and query:
++            query = _encode_invalid_chars(query, QUERY_CHARS)
++        if normalize_uri and fragment:
++            fragment = _encode_invalid_chars(fragment, FRAGMENT_CHARS)
++
++    except (ValueError, AttributeError):
++        return six.raise_from(LocationParseError(source_url), None)
++
++    # For the sake of backwards compatibility we put empty
++    # string values for path if there are any defined values
++    # beyond the path in the URL.
++    # TODO: Remove this when we break backwards compatibility.
++    if not path:
++        if query is not None or fragment is not None:
++            path = ""
++        else:
++            path = None
++
++    # Ensure that each part of the URL is a `str` for
++    # backwards compatibility.
++    if isinstance(url, six.text_type):
++        ensure_func = six.ensure_text
++    else:
++        ensure_func = six.ensure_str
++
++    def ensure_type(x):
++        return x if x is None else ensure_func(x)
++
++    return Url(
++        scheme=ensure_type(scheme),
++        auth=ensure_type(auth),
++        host=ensure_type(host),
++        port=port,
++        path=ensure_type(path),
++        query=ensure_type(query),
++        fragment=ensure_type(fragment),
++    )
++
++
++def get_host(url):
++    """
++    Deprecated. Use :func:`parse_url` instead.
++    """
++    p = parse_url(url)
++    return p.scheme or "http", p.hostname, p.port
+Index: venv/Lib/site-packages/urllib3/util/response.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/response.py	(date 1573549849530)
++++ venv/Lib/site-packages/urllib3/util/response.py	(date 1573549849530)
+@@ -0,0 +1,86 @@
++from __future__ import absolute_import
++from ..packages.six.moves import http_client as httplib
++
++from ..exceptions import HeaderParsingError
++
++
++def is_fp_closed(obj):
++    """
++    Checks whether a given file-like object is closed.
++
++    :param obj:
++        The file-like object to check.
++    """
++
++    try:
++        # Check `isclosed()` first, in case Python3 doesn't set `closed`.
++        # GH Issue #928
++        return obj.isclosed()
++    except AttributeError:
++        pass
++
++    try:
++        # Check via the official file-like-object way.
++        return obj.closed
++    except AttributeError:
++        pass
++
++    try:
++        # Check if the object is a container for another file-like object that
++        # gets released on exhaustion (e.g. HTTPResponse).
++        return obj.fp is None
++    except AttributeError:
++        pass
++
++    raise ValueError("Unable to determine whether fp is closed.")
++
++
++def assert_header_parsing(headers):
++    """
++    Asserts whether all headers have been successfully parsed.
++    Extracts encountered errors from the result of parsing headers.
++
++    Only works on Python 3.
++
++    :param headers: Headers to verify.
++    :type headers: `httplib.HTTPMessage`.
++
++    :raises urllib3.exceptions.HeaderParsingError:
++        If parsing errors are found.
++    """
++
++    # This will fail silently if we pass in the wrong kind of parameter.
++    # To make debugging easier add an explicit check.
++    if not isinstance(headers, httplib.HTTPMessage):
++        raise TypeError("expected httplib.Message, got {0}.".format(type(headers)))
++
++    defects = getattr(headers, "defects", None)
++    get_payload = getattr(headers, "get_payload", None)
++
++    unparsed_data = None
++    if get_payload:
++        # get_payload is actually email.message.Message.get_payload;
++        # we're only interested in the result if it's not a multipart message
++        if not headers.is_multipart():
++            payload = get_payload()
++
++            if isinstance(payload, (bytes, str)):
++                unparsed_data = payload
++
++    if defects or unparsed_data:
++        raise HeaderParsingError(defects=defects, unparsed_data=unparsed_data)
++
++
++def is_response_to_head(response):
++    """
++    Checks whether the request of a response has been a HEAD-request.
++    Handles the quirks of AppEngine.
++
++    :param conn:
++    :type conn: :class:`httplib.HTTPResponse`
++    """
++    # FIXME: Can we do this somehow without accessing private httplib _method?
++    method = response._method
++    if isinstance(method, int):  # Platform-specific: Appengine
++        return method == 3
++    return method.upper() == "HEAD"
+Index: venv/Lib/site-packages/urllib3/util/timeout.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/timeout.py	(date 1573549849546)
++++ venv/Lib/site-packages/urllib3/util/timeout.py	(date 1573549849546)
+@@ -0,0 +1,258 @@
++from __future__ import absolute_import
++
++# The default socket timeout, used by httplib to indicate that no timeout was
++# specified by the user
++from socket import _GLOBAL_DEFAULT_TIMEOUT
++import time
++
++from ..exceptions import TimeoutStateError
++
++# A sentinel value to indicate that no timeout was specified by the user in
++# urllib3
++_Default = object()
++
++
++# Use time.monotonic if available.
++current_time = getattr(time, "monotonic", time.time)
++
++
++class Timeout(object):
++    """ Timeout configuration.
++
++    Timeouts can be defined as a default for a pool::
++
++        timeout = Timeout(connect=2.0, read=7.0)
++        http = PoolManager(timeout=timeout)
++        response = http.request('GET', 'http://example.com/')
++
++    Or per-request (which overrides the default for the pool)::
++
++        response = http.request('GET', 'http://example.com/', timeout=Timeout(10))
++
++    Timeouts can be disabled by setting all the parameters to ``None``::
++
++        no_timeout = Timeout(connect=None, read=None)
++        response = http.request('GET', 'http://example.com/, timeout=no_timeout)
++
++
++    :param total:
++        This combines the connect and read timeouts into one; the read timeout
++        will be set to the time leftover from the connect attempt. In the
++        event that both a connect timeout and a total are specified, or a read
++        timeout and a total are specified, the shorter timeout will be applied.
++
++        Defaults to None.
++
++    :type total: integer, float, or None
++
++    :param connect:
++        The maximum amount of time (in seconds) to wait for a connection
++        attempt to a server to succeed. Omitting the parameter will default the
++        connect timeout to the system default, probably `the global default
++        timeout in socket.py
++        <http://hg.python.org/cpython/file/603b4d593758/Lib/socket.py#l535>`_.
++        None will set an infinite timeout for connection attempts.
++
++    :type connect: integer, float, or None
++
++    :param read:
++        The maximum amount of time (in seconds) to wait between consecutive
++        read operations for a response from the server. Omitting the parameter
++        will default the read timeout to the system default, probably `the
++        global default timeout in socket.py
++        <http://hg.python.org/cpython/file/603b4d593758/Lib/socket.py#l535>`_.
++        None will set an infinite timeout.
++
++    :type read: integer, float, or None
++
++    .. note::
++
++        Many factors can affect the total amount of time for urllib3 to return
++        an HTTP response.
++
++        For example, Python's DNS resolver does not obey the timeout specified
++        on the socket. Other factors that can affect total request time include
++        high CPU load, high swap, the program running at a low priority level,
++        or other behaviors.
++
++        In addition, the read and total timeouts only measure the time between
++        read operations on the socket connecting the client and the server,
++        not the total amount of time for the request to return a complete
++        response. For most requests, the timeout is raised because the server
++        has not sent the first byte in the specified time. This is not always
++        the case; if a server streams one byte every fifteen seconds, a timeout
++        of 20 seconds will not trigger, even though the request will take
++        several minutes to complete.
++
++        If your goal is to cut off any request after a set amount of wall clock
++        time, consider having a second "watcher" thread to cut off a slow
++        request.
++    """
++
++    #: A sentinel object representing the default timeout value
++    DEFAULT_TIMEOUT = _GLOBAL_DEFAULT_TIMEOUT
++
++    def __init__(self, total=None, connect=_Default, read=_Default):
++        self._connect = self._validate_timeout(connect, "connect")
++        self._read = self._validate_timeout(read, "read")
++        self.total = self._validate_timeout(total, "total")
++        self._start_connect = None
++
++    def __str__(self):
++        return "%s(connect=%r, read=%r, total=%r)" % (
++            type(self).__name__,
++            self._connect,
++            self._read,
++            self.total,
++        )
++
++    @classmethod
++    def _validate_timeout(cls, value, name):
++        """ Check that a timeout attribute is valid.
++
++        :param value: The timeout value to validate
++        :param name: The name of the timeout attribute to validate. This is
++            used to specify in error messages.
++        :return: The validated and casted version of the given value.
++        :raises ValueError: If it is a numeric value less than or equal to
++            zero, or the type is not an integer, float, or None.
++        """
++        if value is _Default:
++            return cls.DEFAULT_TIMEOUT
++
++        if value is None or value is cls.DEFAULT_TIMEOUT:
++            return value
++
++        if isinstance(value, bool):
++            raise ValueError(
++                "Timeout cannot be a boolean value. It must "
++                "be an int, float or None."
++            )
++        try:
++            float(value)
++        except (TypeError, ValueError):
++            raise ValueError(
++                "Timeout value %s was %s, but it must be an "
++                "int, float or None." % (name, value)
++            )
++
++        try:
++            if value <= 0:
++                raise ValueError(
++                    "Attempted to set %s timeout to %s, but the "
++                    "timeout cannot be set to a value less "
++                    "than or equal to 0." % (name, value)
++                )
++        except TypeError:
++            # Python 3
++            raise ValueError(
++                "Timeout value %s was %s, but it must be an "
++                "int, float or None." % (name, value)
++            )
++
++        return value
++
++    @classmethod
++    def from_float(cls, timeout):
++        """ Create a new Timeout from a legacy timeout value.
++
++        The timeout value used by httplib.py sets the same timeout on the
++        connect(), and recv() socket requests. This creates a :class:`Timeout`
++        object that sets the individual timeouts to the ``timeout`` value
++        passed to this function.
++
++        :param timeout: The legacy timeout value.
++        :type timeout: integer, float, sentinel default object, or None
++        :return: Timeout object
++        :rtype: :class:`Timeout`
++        """
++        return Timeout(read=timeout, connect=timeout)
++
++    def clone(self):
++        """ Create a copy of the timeout object
++
++        Timeout properties are stored per-pool but each request needs a fresh
++        Timeout object to ensure each one has its own start/stop configured.
++
++        :return: a copy of the timeout object
++        :rtype: :class:`Timeout`
++        """
++        # We can't use copy.deepcopy because that will also create a new object
++        # for _GLOBAL_DEFAULT_TIMEOUT, which socket.py uses as a sentinel to
++        # detect the user default.
++        return Timeout(connect=self._connect, read=self._read, total=self.total)
++
++    def start_connect(self):
++        """ Start the timeout clock, used during a connect() attempt
++
++        :raises urllib3.exceptions.TimeoutStateError: if you attempt
++            to start a timer that has been started already.
++        """
++        if self._start_connect is not None:
++            raise TimeoutStateError("Timeout timer has already been started.")
++        self._start_connect = current_time()
++        return self._start_connect
++
++    def get_connect_duration(self):
++        """ Gets the time elapsed since the call to :meth:`start_connect`.
++
++        :return: Elapsed time in seconds.
++        :rtype: float
++        :raises urllib3.exceptions.TimeoutStateError: if you attempt
++            to get duration for a timer that hasn't been started.
++        """
++        if self._start_connect is None:
++            raise TimeoutStateError(
++                "Can't get connect duration for timer that has not started."
++            )
++        return current_time() - self._start_connect
++
++    @property
++    def connect_timeout(self):
++        """ Get the value to use when setting a connection timeout.
++
++        This will be a positive float or integer, the value None
++        (never timeout), or the default system timeout.
++
++        :return: Connect timeout.
++        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None
++        """
++        if self.total is None:
++            return self._connect
++
++        if self._connect is None or self._connect is self.DEFAULT_TIMEOUT:
++            return self.total
++
++        return min(self._connect, self.total)
++
++    @property
++    def read_timeout(self):
++        """ Get the value for the read timeout.
++
++        This assumes some time has elapsed in the connection timeout and
++        computes the read timeout appropriately.
++
++        If self.total is set, the read timeout is dependent on the amount of
++        time taken by the connect timeout. If the connection time has not been
++        established, a :exc:`~urllib3.exceptions.TimeoutStateError` will be
++        raised.
++
++        :return: Value to use for the read timeout.
++        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None
++        :raises urllib3.exceptions.TimeoutStateError: If :meth:`start_connect`
++            has not yet been called on this object.
++        """
++        if (
++            self.total is not None
++            and self.total is not self.DEFAULT_TIMEOUT
++            and self._read is not None
++            and self._read is not self.DEFAULT_TIMEOUT
++        ):
++            # In case the connect timeout has not yet been established.
++            if self._start_connect is None:
++                return self._read
++            return max(0, min(self.total - self.get_connect_duration(), self._read))
++        elif self.total is not None and self.total is not self.DEFAULT_TIMEOUT:
++            return max(0, self.total - self.get_connect_duration())
++        else:
++            return self._read
+Index: venv/Lib/site-packages/urllib3/util/wait.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/wait.py	(date 1573549849556)
++++ venv/Lib/site-packages/urllib3/util/wait.py	(date 1573549849556)
+@@ -0,0 +1,153 @@
++import errno
++from functools import partial
++import select
++import sys
++
++try:
++    from time import monotonic
++except ImportError:
++    from time import time as monotonic
++
++__all__ = ["NoWayToWaitForSocketError", "wait_for_read", "wait_for_write"]
++
++
++class NoWayToWaitForSocketError(Exception):
++    pass
++
++
++# How should we wait on sockets?
++#
++# There are two types of APIs you can use for waiting on sockets: the fancy
++# modern stateful APIs like epoll/kqueue, and the older stateless APIs like
++# select/poll. The stateful APIs are more efficient when you have a lots of
++# sockets to keep track of, because you can set them up once and then use them
++# lots of times. But we only ever want to wait on a single socket at a time
++# and don't want to keep track of state, so the stateless APIs are actually
++# more efficient. So we want to use select() or poll().
++#
++# Now, how do we choose between select() and poll()? On traditional Unixes,
++# select() has a strange calling convention that makes it slow, or fail
++# altogether, for high-numbered file descriptors. The point of poll() is to fix
++# that, so on Unixes, we prefer poll().
++#
++# On Windows, there is no poll() (or at least Python doesn't provide a wrapper
++# for it), but that's OK, because on Windows, select() doesn't have this
++# strange calling convention; plain select() works fine.
++#
++# So: on Windows we use select(), and everywhere else we use poll(). We also
++# fall back to select() in case poll() is somehow broken or missing.
++
++if sys.version_info >= (3, 5):
++    # Modern Python, that retries syscalls by default
++    def _retry_on_intr(fn, timeout):
++        return fn(timeout)
++
++
++else:
++    # Old and broken Pythons.
++    def _retry_on_intr(fn, timeout):
++        if timeout is None:
++            deadline = float("inf")
++        else:
++            deadline = monotonic() + timeout
++
++        while True:
++            try:
++                return fn(timeout)
++            # OSError for 3 <= pyver < 3.5, select.error for pyver <= 2.7
++            except (OSError, select.error) as e:
++                # 'e.args[0]' incantation works for both OSError and select.error
++                if e.args[0] != errno.EINTR:
++                    raise
++                else:
++                    timeout = deadline - monotonic()
++                    if timeout < 0:
++                        timeout = 0
++                    if timeout == float("inf"):
++                        timeout = None
++                    continue
++
++
++def select_wait_for_socket(sock, read=False, write=False, timeout=None):
++    if not read and not write:
++        raise RuntimeError("must specify at least one of read=True, write=True")
++    rcheck = []
++    wcheck = []
++    if read:
++        rcheck.append(sock)
++    if write:
++        wcheck.append(sock)
++    # When doing a non-blocking connect, most systems signal success by
++    # marking the socket writable. Windows, though, signals success by marked
++    # it as "exceptional". We paper over the difference by checking the write
++    # sockets for both conditions. (The stdlib selectors module does the same
++    # thing.)
++    fn = partial(select.select, rcheck, wcheck, wcheck)
++    rready, wready, xready = _retry_on_intr(fn, timeout)
++    return bool(rready or wready or xready)
++
++
++def poll_wait_for_socket(sock, read=False, write=False, timeout=None):
++    if not read and not write:
++        raise RuntimeError("must specify at least one of read=True, write=True")
++    mask = 0
++    if read:
++        mask |= select.POLLIN
++    if write:
++        mask |= select.POLLOUT
++    poll_obj = select.poll()
++    poll_obj.register(sock, mask)
++
++    # For some reason, poll() takes timeout in milliseconds
++    def do_poll(t):
++        if t is not None:
++            t *= 1000
++        return poll_obj.poll(t)
++
++    return bool(_retry_on_intr(do_poll, timeout))
++
++
++def null_wait_for_socket(*args, **kwargs):
++    raise NoWayToWaitForSocketError("no select-equivalent available")
++
++
++def _have_working_poll():
++    # Apparently some systems have a select.poll that fails as soon as you try
++    # to use it, either due to strange configuration or broken monkeypatching
++    # from libraries like eventlet/greenlet.
++    try:
++        poll_obj = select.poll()
++        _retry_on_intr(poll_obj.poll, 0)
++    except (AttributeError, OSError):
++        return False
++    else:
++        return True
++
++
++def wait_for_socket(*args, **kwargs):
++    # We delay choosing which implementation to use until the first time we're
++    # called. We could do it at import time, but then we might make the wrong
++    # decision if someone goes wild with monkeypatching select.poll after
++    # we're imported.
++    global wait_for_socket
++    if _have_working_poll():
++        wait_for_socket = poll_wait_for_socket
++    elif hasattr(select, "select"):
++        wait_for_socket = select_wait_for_socket
++    else:  # Platform-specific: Appengine.
++        wait_for_socket = null_wait_for_socket
++    return wait_for_socket(*args, **kwargs)
++
++
++def wait_for_read(sock, timeout=None):
++    """ Waits for reading to be available on a given socket.
++    Returns True if the socket is readable, or False if the timeout expired.
++    """
++    return wait_for_socket(sock, read=True, timeout=timeout)
++
++
++def wait_for_write(sock, timeout=None):
++    """ Waits for writing to be available on a given socket.
++    Returns True if the socket is readable, or False if the timeout expired.
++    """
++    return wait_for_socket(sock, write=True, timeout=timeout)
+Index: venv/Lib/site-packages/urllib3/util/request.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/request.py	(date 1573549849525)
++++ venv/Lib/site-packages/urllib3/util/request.py	(date 1573549849525)
+@@ -0,0 +1,135 @@
++from __future__ import absolute_import
++from base64 import b64encode
++
++from ..packages.six import b, integer_types
++from ..exceptions import UnrewindableBodyError
++
++ACCEPT_ENCODING = "gzip,deflate"
++try:
++    import brotli as _unused_module_brotli  # noqa: F401
++except ImportError:
++    pass
++else:
++    ACCEPT_ENCODING += ",br"
++
++_FAILEDTELL = object()
++
++
++def make_headers(
++    keep_alive=None,
++    accept_encoding=None,
++    user_agent=None,
++    basic_auth=None,
++    proxy_basic_auth=None,
++    disable_cache=None,
++):
++    """
++    Shortcuts for generating request headers.
++
++    :param keep_alive:
++        If ``True``, adds 'connection: keep-alive' header.
++
++    :param accept_encoding:
++        Can be a boolean, list, or string.
++        ``True`` translates to 'gzip,deflate'.
++        List will get joined by comma.
++        String will be used as provided.
++
++    :param user_agent:
++        String representing the user-agent you want, such as
++        "python-urllib3/0.6"
++
++    :param basic_auth:
++        Colon-separated username:password string for 'authorization: basic ...'
++        auth header.
++
++    :param proxy_basic_auth:
++        Colon-separated username:password string for 'proxy-authorization: basic ...'
++        auth header.
++
++    :param disable_cache:
++        If ``True``, adds 'cache-control: no-cache' header.
++
++    Example::
++
++        >>> make_headers(keep_alive=True, user_agent="Batman/1.0")
++        {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}
++        >>> make_headers(accept_encoding=True)
++        {'accept-encoding': 'gzip,deflate'}
++    """
++    headers = {}
++    if accept_encoding:
++        if isinstance(accept_encoding, str):
++            pass
++        elif isinstance(accept_encoding, list):
++            accept_encoding = ",".join(accept_encoding)
++        else:
++            accept_encoding = ACCEPT_ENCODING
++        headers["accept-encoding"] = accept_encoding
++
++    if user_agent:
++        headers["user-agent"] = user_agent
++
++    if keep_alive:
++        headers["connection"] = "keep-alive"
++
++    if basic_auth:
++        headers["authorization"] = "Basic " + b64encode(b(basic_auth)).decode("utf-8")
++
++    if proxy_basic_auth:
++        headers["proxy-authorization"] = "Basic " + b64encode(
++            b(proxy_basic_auth)
++        ).decode("utf-8")
++
++    if disable_cache:
++        headers["cache-control"] = "no-cache"
++
++    return headers
++
++
++def set_file_position(body, pos):
++    """
++    If a position is provided, move file to that point.
++    Otherwise, we'll attempt to record a position for future use.
++    """
++    if pos is not None:
++        rewind_body(body, pos)
++    elif getattr(body, "tell", None) is not None:
++        try:
++            pos = body.tell()
++        except (IOError, OSError):
++            # This differentiates from None, allowing us to catch
++            # a failed `tell()` later when trying to rewind the body.
++            pos = _FAILEDTELL
++
++    return pos
++
++
++def rewind_body(body, body_pos):
++    """
++    Attempt to rewind body to a certain position.
++    Primarily used for request redirects and retries.
++
++    :param body:
++        File-like object that supports seek.
++
++    :param int pos:
++        Position to seek to in file.
++    """
++    body_seek = getattr(body, "seek", None)
++    if body_seek is not None and isinstance(body_pos, integer_types):
++        try:
++            body_seek(body_pos)
++        except (IOError, OSError):
++            raise UnrewindableBodyError(
++                "An error occurred when rewinding request body for redirect/retry."
++            )
++    elif body_pos is _FAILEDTELL:
++        raise UnrewindableBodyError(
++            "Unable to record file position for rewinding "
++            "request body during a redirect/retry."
++        )
++    else:
++        raise ValueError(
++            "body_pos must be of type integer, instead it was %s." % type(body_pos)
++        )
+Index: venv/Lib/site-packages/urllib3/util/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/util/__init__.py	(date 1573549849510)
++++ venv/Lib/site-packages/urllib3/util/__init__.py	(date 1573549849510)
+@@ -0,0 +1,46 @@
++from __future__ import absolute_import
++
++# For backwards compatibility, provide imports that used to be here.
++from .connection import is_connection_dropped
++from .request import make_headers
++from .response import is_fp_closed
++from .ssl_ import (
++    SSLContext,
++    HAS_SNI,
++    IS_PYOPENSSL,
++    IS_SECURETRANSPORT,
++    assert_fingerprint,
++    resolve_cert_reqs,
++    resolve_ssl_version,
++    ssl_wrap_socket,
++    PROTOCOL_TLS,
++)
++from .timeout import current_time, Timeout
++
++from .retry import Retry
++from .url import get_host, parse_url, split_first, Url
++from .wait import wait_for_read, wait_for_write
++
++__all__ = (
++    "HAS_SNI",
++    "IS_PYOPENSSL",
++    "IS_SECURETRANSPORT",
++    "SSLContext",
++    "PROTOCOL_TLS",
++    "Retry",
++    "Timeout",
++    "Url",
++    "assert_fingerprint",
++    "current_time",
++    "is_connection_dropped",
++    "is_fp_closed",
++    "get_host",
++    "parse_url",
++    "make_headers",
++    "resolve_cert_reqs",
++    "resolve_ssl_version",
++    "split_first",
++    "ssl_wrap_socket",
++    "wait_for_read",
++    "wait_for_write",
++)
+Index: venv/Lib/site-packages/urllib3/contrib/ntlmpool.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/ntlmpool.py	(date 1573549849439)
++++ venv/Lib/site-packages/urllib3/contrib/ntlmpool.py	(date 1573549849439)
+@@ -0,0 +1,121 @@
++"""
++NTLM authenticating pool, contributed by erikcederstran
++
++Issue #10, see: http://code.google.com/p/urllib3/issues/detail?id=10
++"""
++from __future__ import absolute_import
++
++from logging import getLogger
++from ntlm import ntlm
++
++from .. import HTTPSConnectionPool
++from ..packages.six.moves.http_client import HTTPSConnection
++
++
++log = getLogger(__name__)
++
++
++class NTLMConnectionPool(HTTPSConnectionPool):
++    """
++    Implements an NTLM authentication version of an urllib3 connection pool
++    """
++
++    scheme = "https"
++
++    def __init__(self, user, pw, authurl, *args, **kwargs):
++        """
++        authurl is a random URL on the server that is protected by NTLM.
++        user is the Windows user, probably in the DOMAIN\\username format.
++        pw is the password for the user.
++        """
++        super(NTLMConnectionPool, self).__init__(*args, **kwargs)
++        self.authurl = authurl
++        self.rawuser = user
++        user_parts = user.split("\\", 1)
++        self.domain = user_parts[0].upper()
++        self.user = user_parts[1]
++        self.pw = pw
++
++    def _new_conn(self):
++        # Performs the NTLM handshake that secures the connection. The socket
++        # must be kept open while requests are performed.
++        self.num_connections += 1
++        log.debug(
++            "Starting NTLM HTTPS connection no. %d: https://%s%s",
++            self.num_connections,
++            self.host,
++            self.authurl,
++        )
++
++        headers = {"Connection": "Keep-Alive"}
++        req_header = "Authorization"
++        resp_header = "www-authenticate"
++
++        conn = HTTPSConnection(host=self.host, port=self.port)
++
++        # Send negotiation message
++        headers[req_header] = "NTLM %s" % ntlm.create_NTLM_NEGOTIATE_MESSAGE(
++            self.rawuser
++        )
++        log.debug("Request headers: %s", headers)
++        conn.request("GET", self.authurl, None, headers)
++        res = conn.getresponse()
++        reshdr = dict(res.getheaders())
++        log.debug("Response status: %s %s", res.status, res.reason)
++        log.debug("Response headers: %s", reshdr)
++        log.debug("Response data: %s [...]", res.read(100))
++
++        # Remove the reference to the socket, so that it can not be closed by
++        # the response object (we want to keep the socket open)
++        res.fp = None
++
++        # Server should respond with a challenge message
++        auth_header_values = reshdr[resp_header].split(", ")
++        auth_header_value = None
++        for s in auth_header_values:
++            if s[:5] == "NTLM ":
++                auth_header_value = s[5:]
++        if auth_header_value is None:
++            raise Exception(
++                "Unexpected %s response header: %s" % (resp_header, reshdr[resp_header])
++            )
++
++        # Send authentication message
++        ServerChallenge, NegotiateFlags = ntlm.parse_NTLM_CHALLENGE_MESSAGE(
++            auth_header_value
++        )
++        auth_msg = ntlm.create_NTLM_AUTHENTICATE_MESSAGE(
++            ServerChallenge, self.user, self.domain, self.pw, NegotiateFlags
++        )
++        headers[req_header] = "NTLM %s" % auth_msg
++        log.debug("Request headers: %s", headers)
++        conn.request("GET", self.authurl, None, headers)
++        res = conn.getresponse()
++        log.debug("Response status: %s %s", res.status, res.reason)
++        log.debug("Response headers: %s", dict(res.getheaders()))
++        log.debug("Response data: %s [...]", res.read()[:100])
++        if res.status != 200:
++            if res.status == 401:
++                raise Exception("Server rejected request: wrong username or password")
++            raise Exception("Wrong server response: %s %s" % (res.status, res.reason))
++
++        res.fp = None
++        log.debug("Connection established")
++        return conn
++
++    def urlopen(
++        self,
++        method,
++        url,
++        body=None,
++        headers=None,
++        retries=3,
++        redirect=True,
++        assert_same_host=True,
++    ):
++        if headers is None:
++            headers = {}
++        headers["Connection"] = "Keep-Alive"
++        return super(NTLMConnectionPool, self).urlopen(
++            method, url, body, headers, retries, redirect, assert_same_host
++        )
+Index: venv/Lib/site-packages/urllib3/contrib/_appengine_environ.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/_appengine_environ.py	(date 1573549849431)
++++ venv/Lib/site-packages/urllib3/contrib/_appengine_environ.py	(date 1573549849431)
+@@ -0,0 +1,36 @@
++"""
++This module provides means to detect the App Engine environment.
++"""
++
++import os
++
++
++def is_appengine():
++    return "APPENGINE_RUNTIME" in os.environ
++
++
++def is_appengine_sandbox():
++    """Reports if the app is running in the first generation sandbox.
++
++    The second generation runtimes are technically still in a sandbox, but it
++    is much less restrictive, so generally you shouldn't need to check for it.
++    see https://cloud.google.com/appengine/docs/standard/runtimes
++    """
++    return is_appengine() and os.environ["APPENGINE_RUNTIME"] == "python27"
++
++
++def is_local_appengine():
++    return is_appengine() and os.environ.get("SERVER_SOFTWARE", "").startswith(
++        "Development/"
++    )
++
++
++def is_prod_appengine():
++    return is_appengine() and os.environ.get("SERVER_SOFTWARE", "").startswith(
++        "Google App Engine/"
++    )
++
++
++def is_prod_appengine_mvms():
++    """Deprecated."""
++    return False
+Index: venv/Lib/site-packages/urllib3/contrib/socks.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/socks.py	(date 1573549849454)
++++ venv/Lib/site-packages/urllib3/contrib/socks.py	(date 1573549849454)
+@@ -0,0 +1,210 @@
++# -*- coding: utf-8 -*-
++"""
++This module contains provisional support for SOCKS proxies from within
++urllib3. This module supports SOCKS4, SOCKS4A (an extension of SOCKS4), and
++SOCKS5. To enable its functionality, either install PySocks or install this
++module with the ``socks`` extra.
++
++The SOCKS implementation supports the full range of urllib3 features. It also
++supports the following SOCKS features:
++
++- SOCKS4A (``proxy_url='socks4a://...``)
++- SOCKS4 (``proxy_url='socks4://...``)
++- SOCKS5 with remote DNS (``proxy_url='socks5h://...``)
++- SOCKS5 with local DNS (``proxy_url='socks5://...``)
++- Usernames and passwords for the SOCKS proxy
++
++ .. note::
++    It is recommended to use ``socks5h://`` or ``socks4a://`` schemes in
++    your ``proxy_url`` to ensure that DNS resolution is done from the remote
++    server instead of client-side when connecting to a domain name.
++
++SOCKS4 supports IPv4 and domain names with the SOCKS4A extension. SOCKS5
++supports IPv4, IPv6, and domain names.
++
++When connecting to a SOCKS4 proxy the ``username`` portion of the ``proxy_url``
++will be sent as the ``userid`` section of the SOCKS request::
++
++    proxy_url="socks4a://<userid>@proxy-host"
++
++When connecting to a SOCKS5 proxy the ``username`` and ``password`` portion
++of the ``proxy_url`` will be sent as the username/password to authenticate
++with the proxy::
++
++    proxy_url="socks5h://<username>:<password>@proxy-host"
++
++"""
++from __future__ import absolute_import
++
++try:
++    import socks
++except ImportError:
++    import warnings
++    from ..exceptions import DependencyWarning
++
++    warnings.warn(
++        (
++            "SOCKS support in urllib3 requires the installation of optional "
++            "dependencies: specifically, PySocks.  For more information, see "
++            "https://urllib3.readthedocs.io/en/latest/contrib.html#socks-proxies"
++        ),
++        DependencyWarning,
++    )
++    raise
++
++from socket import error as SocketError, timeout as SocketTimeout
++
++from ..connection import HTTPConnection, HTTPSConnection
++from ..connectionpool import HTTPConnectionPool, HTTPSConnectionPool
++from ..exceptions import ConnectTimeoutError, NewConnectionError
++from ..poolmanager import PoolManager
++from ..util.url import parse_url
++
++try:
++    import ssl
++except ImportError:
++    ssl = None
++
++
++class SOCKSConnection(HTTPConnection):
++    """
++    A plain-text HTTP connection that connects via a SOCKS proxy.
++    """
++
++    def __init__(self, *args, **kwargs):
++        self._socks_options = kwargs.pop("_socks_options")
++        super(SOCKSConnection, self).__init__(*args, **kwargs)
++
++    def _new_conn(self):
++        """
++        Establish a new connection via the SOCKS proxy.
++        """
++        extra_kw = {}
++        if self.source_address:
++            extra_kw["source_address"] = self.source_address
++
++        if self.socket_options:
++            extra_kw["socket_options"] = self.socket_options
++
++        try:
++            conn = socks.create_connection(
++                (self.host, self.port),
++                proxy_type=self._socks_options["socks_version"],
++                proxy_addr=self._socks_options["proxy_host"],
++                proxy_port=self._socks_options["proxy_port"],
++                proxy_username=self._socks_options["username"],
++                proxy_password=self._socks_options["password"],
++                proxy_rdns=self._socks_options["rdns"],
++                timeout=self.timeout,
++                **extra_kw
++            )
++
++        except SocketTimeout:
++            raise ConnectTimeoutError(
++                self,
++                "Connection to %s timed out. (connect timeout=%s)"
++                % (self.host, self.timeout),
++            )
++
++        except socks.ProxyError as e:
++            # This is fragile as hell, but it seems to be the only way to raise
++            # useful errors here.
++            if e.socket_err:
++                error = e.socket_err
++                if isinstance(error, SocketTimeout):
++                    raise ConnectTimeoutError(
++                        self,
++                        "Connection to %s timed out. (connect timeout=%s)"
++                        % (self.host, self.timeout),
++                    )
++                else:
++                    raise NewConnectionError(
++                        self, "Failed to establish a new connection: %s" % error
++                    )
++            else:
++                raise NewConnectionError(
++                    self, "Failed to establish a new connection: %s" % e
++                )
++
++        except SocketError as e:  # Defensive: PySocks should catch all these.
++            raise NewConnectionError(
++                self, "Failed to establish a new connection: %s" % e
++            )
++
++        return conn
++
++
++# We don't need to duplicate the Verified/Unverified distinction from
++# urllib3/connection.py here because the HTTPSConnection will already have been
++# correctly set to either the Verified or Unverified form by that module. This
++# means the SOCKSHTTPSConnection will automatically be the correct type.
++class SOCKSHTTPSConnection(SOCKSConnection, HTTPSConnection):
++    pass
++
++
++class SOCKSHTTPConnectionPool(HTTPConnectionPool):
++    ConnectionCls = SOCKSConnection
++
++
++class SOCKSHTTPSConnectionPool(HTTPSConnectionPool):
++    ConnectionCls = SOCKSHTTPSConnection
++
++
++class SOCKSProxyManager(PoolManager):
++    """
++    A version of the urllib3 ProxyManager that routes connections via the
++    defined SOCKS proxy.
++    """
++
++    pool_classes_by_scheme = {
++        "http": SOCKSHTTPConnectionPool,
++        "https": SOCKSHTTPSConnectionPool,
++    }
++
++    def __init__(
++        self,
++        proxy_url,
++        username=None,
++        password=None,
++        num_pools=10,
++        headers=None,
++        **connection_pool_kw
++    ):
++        parsed = parse_url(proxy_url)
++
++        if username is None and password is None and parsed.auth is not None:
++            split = parsed.auth.split(":")
++            if len(split) == 2:
++                username, password = split
++        if parsed.scheme == "socks5":
++            socks_version = socks.PROXY_TYPE_SOCKS5
++            rdns = False
++        elif parsed.scheme == "socks5h":
++            socks_version = socks.PROXY_TYPE_SOCKS5
++            rdns = True
++        elif parsed.scheme == "socks4":
++            socks_version = socks.PROXY_TYPE_SOCKS4
++            rdns = False
++        elif parsed.scheme == "socks4a":
++            socks_version = socks.PROXY_TYPE_SOCKS4
++            rdns = True
++        else:
++            raise ValueError("Unable to determine SOCKS version from %s" % proxy_url)
++
++        self.proxy_url = proxy_url
++
++        socks_options = {
++            "socks_version": socks_version,
++            "proxy_host": parsed.host,
++            "proxy_port": parsed.port,
++            "username": username,
++            "password": password,
++            "rdns": rdns,
++        }
++        connection_pool_kw["_socks_options"] = socks_options
++
++        super(SOCKSProxyManager, self).__init__(
++            num_pools, headers, **connection_pool_kw
++        )
++
++        self.pool_classes_by_scheme = SOCKSProxyManager.pool_classes_by_scheme
+Index: venv/Lib/site-packages/urllib3/contrib/securetransport.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/securetransport.py	(date 1573549849448)
++++ venv/Lib/site-packages/urllib3/contrib/securetransport.py	(date 1573549849448)
+@@ -0,0 +1,859 @@
++"""
++SecureTranport support for urllib3 via ctypes.
++
++This makes platform-native TLS available to urllib3 users on macOS without the
++use of a compiler. This is an important feature because the Python Package
++Index is moving to become a TLSv1.2-or-higher server, and the default OpenSSL
++that ships with macOS is not capable of doing TLSv1.2. The only way to resolve
++this is to give macOS users an alternative solution to the problem, and that
++solution is to use SecureTransport.
++
++We use ctypes here because this solution must not require a compiler. That's
++because pip is not allowed to require a compiler either.
++
++This is not intended to be a seriously long-term solution to this problem.
++The hope is that PEP 543 will eventually solve this issue for us, at which
++point we can retire this contrib module. But in the short term, we need to
++solve the impending tire fire that is Python on Mac without this kind of
++contrib module. So...here we are.
++
++To use this module, simply import and inject it::
++
++    import urllib3.contrib.securetransport
++    urllib3.contrib.securetransport.inject_into_urllib3()
++
++Happy TLSing!
++
++This code is a bastardised version of the code found in Will Bond's oscrypto
++library. An enormous debt is owed to him for blazing this trail for us. For
++that reason, this code should be considered to be covered both by urllib3's
++license and by oscrypto's:
++
++    Copyright (c) 2015-2016 Will Bond <will@wbond.net>
++
++    Permission is hereby granted, free of charge, to any person obtaining a
++    copy of this software and associated documentation files (the "Software"),
++    to deal in the Software without restriction, including without limitation
++    the rights to use, copy, modify, merge, publish, distribute, sublicense,
++    and/or sell copies of the Software, and to permit persons to whom the
++    Software is furnished to do so, subject to the following conditions:
++
++    The above copyright notice and this permission notice shall be included in
++    all copies or substantial portions of the Software.
++
++    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
++    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
++    DEALINGS IN THE SOFTWARE.
++"""
++from __future__ import absolute_import
++
++import contextlib
++import ctypes
++import errno
++import os.path
++import shutil
++import socket
++import ssl
++import threading
++import weakref
++
++from .. import util
++from ._securetransport.bindings import Security, SecurityConst, CoreFoundation
++from ._securetransport.low_level import (
++    _assert_no_error,
++    _cert_array_from_pem,
++    _temporary_keychain,
++    _load_client_cert_chain,
++)
++
++try:  # Platform-specific: Python 2
++    from socket import _fileobject
++except ImportError:  # Platform-specific: Python 3
++    _fileobject = None
++    from ..packages.backports.makefile import backport_makefile
++
++__all__ = ["inject_into_urllib3", "extract_from_urllib3"]
++
++# SNI always works
++HAS_SNI = True
++
++orig_util_HAS_SNI = util.HAS_SNI
++orig_util_SSLContext = util.ssl_.SSLContext
++
++# This dictionary is used by the read callback to obtain a handle to the
++# calling wrapped socket. This is a pretty silly approach, but for now it'll
++# do. I feel like I should be able to smuggle a handle to the wrapped socket
++# directly in the SSLConnectionRef, but for now this approach will work I
++# guess.
++#
++# We need to lock around this structure for inserts, but we don't do it for
++# reads/writes in the callbacks. The reasoning here goes as follows:
++#
++#    1. It is not possible to call into the callbacks before the dictionary is
++#       populated, so once in the callback the id must be in the dictionary.
++#    2. The callbacks don't mutate the dictionary, they only read from it, and
++#       so cannot conflict with any of the insertions.
++#
++# This is good: if we had to lock in the callbacks we'd drastically slow down
++# the performance of this code.
++_connection_refs = weakref.WeakValueDictionary()
++_connection_ref_lock = threading.Lock()
++
++# Limit writes to 16kB. This is OpenSSL's limit, but we'll cargo-cult it over
++# for no better reason than we need *a* limit, and this one is right there.
++SSL_WRITE_BLOCKSIZE = 16384
++
++# This is our equivalent of util.ssl_.DEFAULT_CIPHERS, but expanded out to
++# individual cipher suites. We need to do this because this is how
++# SecureTransport wants them.
++CIPHER_SUITES = [
++    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,
++    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,
++    SecurityConst.TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,
++    SecurityConst.TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,
++    SecurityConst.TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,
++    SecurityConst.TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,
++    SecurityConst.TLS_DHE_RSA_WITH_AES_256_GCM_SHA384,
++    SecurityConst.TLS_DHE_RSA_WITH_AES_128_GCM_SHA256,
++    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384,
++    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,
++    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,
++    SecurityConst.TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,
++    SecurityConst.TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384,
++    SecurityConst.TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,
++    SecurityConst.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256,
++    SecurityConst.TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,
++    SecurityConst.TLS_DHE_RSA_WITH_AES_256_CBC_SHA256,
++    SecurityConst.TLS_DHE_RSA_WITH_AES_256_CBC_SHA,
++    SecurityConst.TLS_DHE_RSA_WITH_AES_128_CBC_SHA256,
++    SecurityConst.TLS_DHE_RSA_WITH_AES_128_CBC_SHA,
++    SecurityConst.TLS_AES_256_GCM_SHA384,
++    SecurityConst.TLS_AES_128_GCM_SHA256,
++    SecurityConst.TLS_RSA_WITH_AES_256_GCM_SHA384,
++    SecurityConst.TLS_RSA_WITH_AES_128_GCM_SHA256,
++    SecurityConst.TLS_AES_128_CCM_8_SHA256,
++    SecurityConst.TLS_AES_128_CCM_SHA256,
++    SecurityConst.TLS_RSA_WITH_AES_256_CBC_SHA256,
++    SecurityConst.TLS_RSA_WITH_AES_128_CBC_SHA256,
++    SecurityConst.TLS_RSA_WITH_AES_256_CBC_SHA,
++    SecurityConst.TLS_RSA_WITH_AES_128_CBC_SHA,
++]
++
++# Basically this is simple: for PROTOCOL_SSLv23 we turn it into a low of
++# TLSv1 and a high of TLSv1.2. For everything else, we pin to that version.
++# TLSv1 to 1.2 are supported on macOS 10.8+
++_protocol_to_min_max = {
++    util.PROTOCOL_TLS: (SecurityConst.kTLSProtocol1, SecurityConst.kTLSProtocol12)
++}
++
++if hasattr(ssl, "PROTOCOL_SSLv2"):
++    _protocol_to_min_max[ssl.PROTOCOL_SSLv2] = (
++        SecurityConst.kSSLProtocol2,
++        SecurityConst.kSSLProtocol2,
++    )
++if hasattr(ssl, "PROTOCOL_SSLv3"):
++    _protocol_to_min_max[ssl.PROTOCOL_SSLv3] = (
++        SecurityConst.kSSLProtocol3,
++        SecurityConst.kSSLProtocol3,
++    )
++if hasattr(ssl, "PROTOCOL_TLSv1"):
++    _protocol_to_min_max[ssl.PROTOCOL_TLSv1] = (
++        SecurityConst.kTLSProtocol1,
++        SecurityConst.kTLSProtocol1,
++    )
++if hasattr(ssl, "PROTOCOL_TLSv1_1"):
++    _protocol_to_min_max[ssl.PROTOCOL_TLSv1_1] = (
++        SecurityConst.kTLSProtocol11,
++        SecurityConst.kTLSProtocol11,
++    )
++if hasattr(ssl, "PROTOCOL_TLSv1_2"):
++    _protocol_to_min_max[ssl.PROTOCOL_TLSv1_2] = (
++        SecurityConst.kTLSProtocol12,
++        SecurityConst.kTLSProtocol12,
++    )
++
++
++def inject_into_urllib3():
++    """
++    Monkey-patch urllib3 with SecureTransport-backed SSL-support.
++    """
++    util.SSLContext = SecureTransportContext
++    util.ssl_.SSLContext = SecureTransportContext
++    util.HAS_SNI = HAS_SNI
++    util.ssl_.HAS_SNI = HAS_SNI
++    util.IS_SECURETRANSPORT = True
++    util.ssl_.IS_SECURETRANSPORT = True
++
++
++def extract_from_urllib3():
++    """
++    Undo monkey-patching by :func:`inject_into_urllib3`.
++    """
++    util.SSLContext = orig_util_SSLContext
++    util.ssl_.SSLContext = orig_util_SSLContext
++    util.HAS_SNI = orig_util_HAS_SNI
++    util.ssl_.HAS_SNI = orig_util_HAS_SNI
++    util.IS_SECURETRANSPORT = False
++    util.ssl_.IS_SECURETRANSPORT = False
++
++
++def _read_callback(connection_id, data_buffer, data_length_pointer):
++    """
++    SecureTransport read callback. This is called by ST to request that data
++    be returned from the socket.
++    """
++    wrapped_socket = None
++    try:
++        wrapped_socket = _connection_refs.get(connection_id)
++        if wrapped_socket is None:
++            return SecurityConst.errSSLInternal
++        base_socket = wrapped_socket.socket
++
++        requested_length = data_length_pointer[0]
++
++        timeout = wrapped_socket.gettimeout()
++        error = None
++        read_count = 0
++
++        try:
++            while read_count < requested_length:
++                if timeout is None or timeout >= 0:
++                    if not util.wait_for_read(base_socket, timeout):
++                        raise socket.error(errno.EAGAIN, "timed out")
++
++                remaining = requested_length - read_count
++                buffer = (ctypes.c_char * remaining).from_address(
++                    data_buffer + read_count
++                )
++                chunk_size = base_socket.recv_into(buffer, remaining)
++                read_count += chunk_size
++                if not chunk_size:
++                    if not read_count:
++                        return SecurityConst.errSSLClosedGraceful
++                    break
++        except (socket.error) as e:
++            error = e.errno
++
++            if error is not None and error != errno.EAGAIN:
++                data_length_pointer[0] = read_count
++                if error == errno.ECONNRESET or error == errno.EPIPE:
++                    return SecurityConst.errSSLClosedAbort
++                raise
++
++        data_length_pointer[0] = read_count
++
++        if read_count != requested_length:
++            return SecurityConst.errSSLWouldBlock
++
++        return 0
++    except Exception as e:
++        if wrapped_socket is not None:
++            wrapped_socket._exception = e
++        return SecurityConst.errSSLInternal
++
++
++def _write_callback(connection_id, data_buffer, data_length_pointer):
++    """
++    SecureTransport write callback. This is called by ST to request that data
++    actually be sent on the network.
++    """
++    wrapped_socket = None
++    try:
++        wrapped_socket = _connection_refs.get(connection_id)
++        if wrapped_socket is None:
++            return SecurityConst.errSSLInternal
++        base_socket = wrapped_socket.socket
++
++        bytes_to_write = data_length_pointer[0]
++        data = ctypes.string_at(data_buffer, bytes_to_write)
++
++        timeout = wrapped_socket.gettimeout()
++        error = None
++        sent = 0
++
++        try:
++            while sent < bytes_to_write:
++                if timeout is None or timeout >= 0:
++                    if not util.wait_for_write(base_socket, timeout):
++                        raise socket.error(errno.EAGAIN, "timed out")
++                chunk_sent = base_socket.send(data)
++                sent += chunk_sent
++
++                # This has some needless copying here, but I'm not sure there's
++                # much value in optimising this data path.
++                data = data[chunk_sent:]
++        except (socket.error) as e:
++            error = e.errno
++
++            if error is not None and error != errno.EAGAIN:
++                data_length_pointer[0] = sent
++                if error == errno.ECONNRESET or error == errno.EPIPE:
++                    return SecurityConst.errSSLClosedAbort
++                raise
++
++        data_length_pointer[0] = sent
++
++        if sent != bytes_to_write:
++            return SecurityConst.errSSLWouldBlock
++
++        return 0
++    except Exception as e:
++        if wrapped_socket is not None:
++            wrapped_socket._exception = e
++        return SecurityConst.errSSLInternal
++
++
++# We need to keep these two objects references alive: if they get GC'd while
++# in use then SecureTransport could attempt to call a function that is in freed
++# memory. That would be...uh...bad. Yeah, that's the word. Bad.
++_read_callback_pointer = Security.SSLReadFunc(_read_callback)
++_write_callback_pointer = Security.SSLWriteFunc(_write_callback)
++
++
++class WrappedSocket(object):
++    """
++    API-compatibility wrapper for Python's OpenSSL wrapped socket object.
++
++    Note: _makefile_refs, _drop(), and _reuse() are needed for the garbage
++    collector of PyPy.
++    """
++
++    def __init__(self, socket):
++        self.socket = socket
++        self.context = None
++        self._makefile_refs = 0
++        self._closed = False
++        self._exception = None
++        self._keychain = None
++        self._keychain_dir = None
++        self._client_cert_chain = None
++
++        # We save off the previously-configured timeout and then set it to
++        # zero. This is done because we use select and friends to handle the
++        # timeouts, but if we leave the timeout set on the lower socket then
++        # Python will "kindly" call select on that socket again for us. Avoid
++        # that by forcing the timeout to zero.
++        self._timeout = self.socket.gettimeout()
++        self.socket.settimeout(0)
++
++    @contextlib.contextmanager
++    def _raise_on_error(self):
++        """
++        A context manager that can be used to wrap calls that do I/O from
++        SecureTransport. If any of the I/O callbacks hit an exception, this
++        context manager will correctly propagate the exception after the fact.
++        This avoids silently swallowing those exceptions.
++
++        It also correctly forces the socket closed.
++        """
++        self._exception = None
++
++        # We explicitly don't catch around this yield because in the unlikely
++        # event that an exception was hit in the block we don't want to swallow
++        # it.
++        yield
++        if self._exception is not None:
++            exception, self._exception = self._exception, None
++            self.close()
++            raise exception
++
++    def _set_ciphers(self):
++        """
++        Sets up the allowed ciphers. By default this matches the set in
++        util.ssl_.DEFAULT_CIPHERS, at least as supported by macOS. This is done
++        custom and doesn't allow changing at this time, mostly because parsing
++        OpenSSL cipher strings is going to be a freaking nightmare.
++        """
++        ciphers = (Security.SSLCipherSuite * len(CIPHER_SUITES))(*CIPHER_SUITES)
++        result = Security.SSLSetEnabledCiphers(
++            self.context, ciphers, len(CIPHER_SUITES)
++        )
++        _assert_no_error(result)
++
++    def _custom_validate(self, verify, trust_bundle):
++        """
++        Called when we have set custom validation. We do this in two cases:
++        first, when cert validation is entirely disabled; and second, when
++        using a custom trust DB.
++        """
++        # If we disabled cert validation, just say: cool.
++        if not verify:
++            return
++
++        # We want data in memory, so load it up.
++        if os.path.isfile(trust_bundle):
++            with open(trust_bundle, "rb") as f:
++                trust_bundle = f.read()
++
++        cert_array = None
++        trust = Security.SecTrustRef()
++
++        try:
++            # Get a CFArray that contains the certs we want.
++            cert_array = _cert_array_from_pem(trust_bundle)
++
++            # Ok, now the hard part. We want to get the SecTrustRef that ST has
++            # created for this connection, shove our CAs into it, tell ST to
++            # ignore everything else it knows, and then ask if it can build a
++            # chain. This is a buuuunch of code.
++            result = Security.SSLCopyPeerTrust(self.context, ctypes.byref(trust))
++            _assert_no_error(result)
++            if not trust:
++                raise ssl.SSLError("Failed to copy trust reference")
++
++            result = Security.SecTrustSetAnchorCertificates(trust, cert_array)
++            _assert_no_error(result)
++
++            result = Security.SecTrustSetAnchorCertificatesOnly(trust, True)
++            _assert_no_error(result)
++
++            trust_result = Security.SecTrustResultType()
++            result = Security.SecTrustEvaluate(trust, ctypes.byref(trust_result))
++            _assert_no_error(result)
++        finally:
++            if trust:
++                CoreFoundation.CFRelease(trust)
++
++            if cert_array is not None:
++                CoreFoundation.CFRelease(cert_array)
++
++        # Ok, now we can look at what the result was.
++        successes = (
++            SecurityConst.kSecTrustResultUnspecified,
++            SecurityConst.kSecTrustResultProceed,
++        )
++        if trust_result.value not in successes:
++            raise ssl.SSLError(
++                "certificate verify failed, error code: %d" % trust_result.value
++            )
++
++    def handshake(
++        self,
++        server_hostname,
++        verify,
++        trust_bundle,
++        min_version,
++        max_version,
++        client_cert,
++        client_key,
++        client_key_passphrase,
++    ):
++        """
++        Actually performs the TLS handshake. This is run automatically by
++        wrapped socket, and shouldn't be needed in user code.
++        """
++        # First, we do the initial bits of connection setup. We need to create
++        # a context, set its I/O funcs, and set the connection reference.
++        self.context = Security.SSLCreateContext(
++            None, SecurityConst.kSSLClientSide, SecurityConst.kSSLStreamType
++        )
++        result = Security.SSLSetIOFuncs(
++            self.context, _read_callback_pointer, _write_callback_pointer
++        )
++        _assert_no_error(result)
++
++        # Here we need to compute the handle to use. We do this by taking the
++        # id of self modulo 2**31 - 1. If this is already in the dictionary, we
++        # just keep incrementing by one until we find a free space.
++        with _connection_ref_lock:
++            handle = id(self) % 2147483647
++            while handle in _connection_refs:
++                handle = (handle + 1) % 2147483647
++            _connection_refs[handle] = self
++
++        result = Security.SSLSetConnection(self.context, handle)
++        _assert_no_error(result)
++
++        # If we have a server hostname, we should set that too.
++        if server_hostname:
++            if not isinstance(server_hostname, bytes):
++                server_hostname = server_hostname.encode("utf-8")
++
++            result = Security.SSLSetPeerDomainName(
++                self.context, server_hostname, len(server_hostname)
++            )
++            _assert_no_error(result)
++
++        # Setup the ciphers.
++        self._set_ciphers()
++
++        # Set the minimum and maximum TLS versions.
++        result = Security.SSLSetProtocolVersionMin(self.context, min_version)
++        _assert_no_error(result)
++
++        result = Security.SSLSetProtocolVersionMax(self.context, max_version)
++        _assert_no_error(result)
++
++        # If there's a trust DB, we need to use it. We do that by telling
++        # SecureTransport to break on server auth. We also do that if we don't
++        # want to validate the certs at all: we just won't actually do any
++        # authing in that case.
++        if not verify or trust_bundle is not None:
++            result = Security.SSLSetSessionOption(
++                self.context, SecurityConst.kSSLSessionOptionBreakOnServerAuth, True
++            )
++            _assert_no_error(result)
++
++        # If there's a client cert, we need to use it.
++        if client_cert:
++            self._keychain, self._keychain_dir = _temporary_keychain()
++            self._client_cert_chain = _load_client_cert_chain(
++                self._keychain, client_cert, client_key
++            )
++            result = Security.SSLSetCertificate(self.context, self._client_cert_chain)
++            _assert_no_error(result)
++
++        while True:
++            with self._raise_on_error():
++                result = Security.SSLHandshake(self.context)
++
++                if result == SecurityConst.errSSLWouldBlock:
++                    raise socket.timeout("handshake timed out")
++                elif result == SecurityConst.errSSLServerAuthCompleted:
++                    self._custom_validate(verify, trust_bundle)
++                    continue
++                else:
++                    _assert_no_error(result)
++                    break
++
++    def fileno(self):
++        return self.socket.fileno()
++
++    # Copy-pasted from Python 3.5 source code
++    def _decref_socketios(self):
++        if self._makefile_refs > 0:
++            self._makefile_refs -= 1
++        if self._closed:
++            self.close()
++
++    def recv(self, bufsiz):
++        buffer = ctypes.create_string_buffer(bufsiz)
++        bytes_read = self.recv_into(buffer, bufsiz)
++        data = buffer[:bytes_read]
++        return data
++
++    def recv_into(self, buffer, nbytes=None):
++        # Read short on EOF.
++        if self._closed:
++            return 0
++
++        if nbytes is None:
++            nbytes = len(buffer)
++
++        buffer = (ctypes.c_char * nbytes).from_buffer(buffer)
++        processed_bytes = ctypes.c_size_t(0)
++
++        with self._raise_on_error():
++            result = Security.SSLRead(
++                self.context, buffer, nbytes, ctypes.byref(processed_bytes)
++            )
++
++        # There are some result codes that we want to treat as "not always
++        # errors". Specifically, those are errSSLWouldBlock,
++        # errSSLClosedGraceful, and errSSLClosedNoNotify.
++        if result == SecurityConst.errSSLWouldBlock:
++            # If we didn't process any bytes, then this was just a time out.
++            # However, we can get errSSLWouldBlock in situations when we *did*
++            # read some data, and in those cases we should just read "short"
++            # and return.
++            if processed_bytes.value == 0:
++                # Timed out, no data read.
++                raise socket.timeout("recv timed out")
++        elif result in (
++            SecurityConst.errSSLClosedGraceful,
++            SecurityConst.errSSLClosedNoNotify,
++        ):
++            # The remote peer has closed this connection. We should do so as
++            # well. Note that we don't actually return here because in
++            # principle this could actually be fired along with return data.
++            # It's unlikely though.
++            self.close()
++        else:
++            _assert_no_error(result)
++
++        # Ok, we read and probably succeeded. We should return whatever data
++        # was actually read.
++        return processed_bytes.value
++
++    def settimeout(self, timeout):
++        self._timeout = timeout
++
++    def gettimeout(self):
++        return self._timeout
++
++    def send(self, data):
++        processed_bytes = ctypes.c_size_t(0)
++
++        with self._raise_on_error():
++            result = Security.SSLWrite(
++                self.context, data, len(data), ctypes.byref(processed_bytes)
++            )
++
++        if result == SecurityConst.errSSLWouldBlock and processed_bytes.value == 0:
++            # Timed out
++            raise socket.timeout("send timed out")
++        else:
++            _assert_no_error(result)
++
++        # We sent, and probably succeeded. Tell them how much we sent.
++        return processed_bytes.value
++
++    def sendall(self, data):
++        total_sent = 0
++        while total_sent < len(data):
++            sent = self.send(data[total_sent : total_sent + SSL_WRITE_BLOCKSIZE])
++            total_sent += sent
++
++    def shutdown(self):
++        with self._raise_on_error():
++            Security.SSLClose(self.context)
++
++    def close(self):
++        # TODO: should I do clean shutdown here? Do I have to?
++        if self._makefile_refs < 1:
++            self._closed = True
++            if self.context:
++                CoreFoundation.CFRelease(self.context)
++                self.context = None
++            if self._client_cert_chain:
++                CoreFoundation.CFRelease(self._client_cert_chain)
++                self._client_cert_chain = None
++            if self._keychain:
++                Security.SecKeychainDelete(self._keychain)
++                CoreFoundation.CFRelease(self._keychain)
++                shutil.rmtree(self._keychain_dir)
++                self._keychain = self._keychain_dir = None
++            return self.socket.close()
++        else:
++            self._makefile_refs -= 1
++
++    def getpeercert(self, binary_form=False):
++        # Urgh, annoying.
++        #
++        # Here's how we do this:
++        #
++        # 1. Call SSLCopyPeerTrust to get hold of the trust object for this
++        #    connection.
++        # 2. Call SecTrustGetCertificateAtIndex for index 0 to get the leaf.
++        # 3. To get the CN, call SecCertificateCopyCommonName and process that
++        #    string so that it's of the appropriate type.
++        # 4. To get the SAN, we need to do something a bit more complex:
++        #    a. Call SecCertificateCopyValues to get the data, requesting
++        #       kSecOIDSubjectAltName.
++        #    b. Mess about with this dictionary to try to get the SANs out.
++        #
++        # This is gross. Really gross. It's going to be a few hundred LoC extra
++        # just to repeat something that SecureTransport can *already do*. So my
++        # operating assumption at this time is that what we want to do is
++        # instead to just flag to urllib3 that it shouldn't do its own hostname
++        # validation when using SecureTransport.
++        if not binary_form:
++            raise ValueError("SecureTransport only supports dumping binary certs")
++        trust = Security.SecTrustRef()
++        certdata = None
++        der_bytes = None
++
++        try:
++            # Grab the trust store.
++            result = Security.SSLCopyPeerTrust(self.context, ctypes.byref(trust))
++            _assert_no_error(result)
++            if not trust:
++                # Probably we haven't done the handshake yet. No biggie.
++                return None
++
++            cert_count = Security.SecTrustGetCertificateCount(trust)
++            if not cert_count:
++                # Also a case that might happen if we haven't handshaked.
++                # Handshook? Handshaken?
++                return None
++
++            leaf = Security.SecTrustGetCertificateAtIndex(trust, 0)
++            assert leaf
++
++            # Ok, now we want the DER bytes.
++            certdata = Security.SecCertificateCopyData(leaf)
++            assert certdata
++
++            data_length = CoreFoundation.CFDataGetLength(certdata)
++            data_buffer = CoreFoundation.CFDataGetBytePtr(certdata)
++            der_bytes = ctypes.string_at(data_buffer, data_length)
++        finally:
++            if certdata:
++                CoreFoundation.CFRelease(certdata)
++            if trust:
++                CoreFoundation.CFRelease(trust)
++
++        return der_bytes
++
++    def version(self):
++        protocol = Security.SSLProtocol()
++        result = Security.SSLGetNegotiatedProtocolVersion(
++            self.context, ctypes.byref(protocol)
++        )
++        _assert_no_error(result)
++        if protocol.value == SecurityConst.kTLSProtocol13:
++            raise ssl.SSLError("SecureTransport does not support TLS 1.3")
++        elif protocol.value == SecurityConst.kTLSProtocol12:
++            return "TLSv1.2"
++        elif protocol.value == SecurityConst.kTLSProtocol11:
++            return "TLSv1.1"
++        elif protocol.value == SecurityConst.kTLSProtocol1:
++            return "TLSv1"
++        elif protocol.value == SecurityConst.kSSLProtocol3:
++            return "SSLv3"
++        elif protocol.value == SecurityConst.kSSLProtocol2:
++            return "SSLv2"
++        else:
++            raise ssl.SSLError("Unknown TLS version: %r" % protocol)
++
++    def _reuse(self):
++        self._makefile_refs += 1
++
++    def _drop(self):
++        if self._makefile_refs < 1:
++            self.close()
++        else:
++            self._makefile_refs -= 1
++
++
++if _fileobject:  # Platform-specific: Python 2
++
++    def makefile(self, mode, bufsize=-1):
++        self._makefile_refs += 1
++        return _fileobject(self, mode, bufsize, close=True)
++
++
++else:  # Platform-specific: Python 3
++
++    def makefile(self, mode="r", buffering=None, *args, **kwargs):
++        # We disable buffering with SecureTransport because it conflicts with
++        # the buffering that ST does internally (see issue #1153 for more).
++        buffering = 0
++        return backport_makefile(self, mode, buffering, *args, **kwargs)
++
++
++WrappedSocket.makefile = makefile
++
++
++class SecureTransportContext(object):
++    """
++    I am a wrapper class for the SecureTransport library, to translate the
++    interface of the standard library ``SSLContext`` object to calls into
++    SecureTransport.
++    """
++
++    def __init__(self, protocol):
++        self._min_version, self._max_version = _protocol_to_min_max[protocol]
++        self._options = 0
++        self._verify = False
++        self._trust_bundle = None
++        self._client_cert = None
++        self._client_key = None
++        self._client_key_passphrase = None
++
++    @property
++    def check_hostname(self):
++        """
++        SecureTransport cannot have its hostname checking disabled. For more,
++        see the comment on getpeercert() in this file.
++        """
++        return True
++
++    @check_hostname.setter
++    def check_hostname(self, value):
++        """
++        SecureTransport cannot have its hostname checking disabled. For more,
++        see the comment on getpeercert() in this file.
++        """
++        pass
++
++    @property
++    def options(self):
++        # TODO: Well, crap.
++        #
++        # So this is the bit of the code that is the most likely to cause us
++        # trouble. Essentially we need to enumerate all of the SSL options that
++        # users might want to use and try to see if we can sensibly translate
++        # them, or whether we should just ignore them.
++        return self._options
++
++    @options.setter
++    def options(self, value):
++        # TODO: Update in line with above.
++        self._options = value
++
++    @property
++    def verify_mode(self):
++        return ssl.CERT_REQUIRED if self._verify else ssl.CERT_NONE
++
++    @verify_mode.setter
++    def verify_mode(self, value):
++        self._verify = True if value == ssl.CERT_REQUIRED else False
++
++    def set_default_verify_paths(self):
++        # So, this has to do something a bit weird. Specifically, what it does
++        # is nothing.
++        #
++        # This means that, if we had previously had load_verify_locations
++        # called, this does not undo that. We need to do that because it turns
++        # out that the rest of the urllib3 code will attempt to load the
++        # default verify paths if it hasn't been told about any paths, even if
++        # the context itself was sometime earlier. We resolve that by just
++        # ignoring it.
++        pass
++
++    def load_default_certs(self):
++        return self.set_default_verify_paths()
++
++    def set_ciphers(self, ciphers):
++        # For now, we just require the default cipher string.
++        if ciphers != util.ssl_.DEFAULT_CIPHERS:
++            raise ValueError("SecureTransport doesn't support custom cipher strings")
++
++    def load_verify_locations(self, cafile=None, capath=None, cadata=None):
++        # OK, we only really support cadata and cafile.
++        if capath is not None:
++            raise ValueError("SecureTransport does not support cert directories")
++
++        self._trust_bundle = cafile or cadata
++
++    def load_cert_chain(self, certfile, keyfile=None, password=None):
++        self._client_cert = certfile
++        self._client_key = keyfile
++        self._client_cert_passphrase = password
++
++    def wrap_socket(
++        self,
++        sock,
++        server_side=False,
++        do_handshake_on_connect=True,
++        suppress_ragged_eofs=True,
++        server_hostname=None,
++    ):
++        # So, what do we do here? Firstly, we assert some properties. This is a
++        # stripped down shim, so there is some functionality we don't support.
++        # See PEP 543 for the real deal.
++        assert not server_side
++        assert do_handshake_on_connect
++        assert suppress_ragged_eofs
++
++        # Ok, we're good to go. Now we want to create the wrapped socket object
++        # and store it in the appropriate place.
++        wrapped_socket = WrappedSocket(sock)
++
++        # Now we can handshake
++        wrapped_socket.handshake(
++            server_hostname,
++            self._verify,
++            self._trust_bundle,
++            self._min_version,
++            self._max_version,
++            self._client_cert,
++            self._client_key,
++            self._client_key_passphrase,
++        )
++        return wrapped_socket
+Index: venv/Lib/site-packages/urllib3/contrib/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/__init__.py	(date 1573549849426)
++++ venv/Lib/site-packages/urllib3/contrib/__init__.py	(date 1573549849426)
+@@ -0,0 +1,0 @@
+Index: venv/Lib/site-packages/urllib3/contrib/appengine.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/appengine.py	(date 1573549849435)
++++ venv/Lib/site-packages/urllib3/contrib/appengine.py	(date 1573549849435)
+@@ -0,0 +1,314 @@
++"""
++This module provides a pool manager that uses Google App Engine's
++`URLFetch Service <https://cloud.google.com/appengine/docs/python/urlfetch>`_.
++
++Example usage::
++
++    from urllib3 import PoolManager
++    from urllib3.contrib.appengine import AppEngineManager, is_appengine_sandbox
++
++    if is_appengine_sandbox():
++        # AppEngineManager uses AppEngine's URLFetch API behind the scenes
++        http = AppEngineManager()
++    else:
++        # PoolManager uses a socket-level API behind the scenes
++        http = PoolManager()
++
++    r = http.request('GET', 'https://google.com/')
++
++There are `limitations <https://cloud.google.com/appengine/docs/python/\
++urlfetch/#Python_Quotas_and_limits>`_ to the URLFetch service and it may not be
++the best choice for your application. There are three options for using
++urllib3 on Google App Engine:
++
++1. You can use :class:`AppEngineManager` with URLFetch. URLFetch is
++   cost-effective in many circumstances as long as your usage is within the
++   limitations.
++2. You can use a normal :class:`~urllib3.PoolManager` by enabling sockets.
++   Sockets also have `limitations and restrictions
++   <https://cloud.google.com/appengine/docs/python/sockets/\
++   #limitations-and-restrictions>`_ and have a lower free quota than URLFetch.
++   To use sockets, be sure to specify the following in your ``app.yaml``::
++
++        env_variables:
++            GAE_USE_SOCKETS_HTTPLIB : 'true'
++
++3. If you are using `App Engine Flexible
++<https://cloud.google.com/appengine/docs/flexible/>`_, you can use the standard
++:class:`PoolManager` without any configuration or special environment variables.
++"""
++
++from __future__ import absolute_import
++import io
++import logging
++import warnings
++from ..packages.six.moves.urllib.parse import urljoin
++
++from ..exceptions import (
++    HTTPError,
++    HTTPWarning,
++    MaxRetryError,
++    ProtocolError,
++    TimeoutError,
++    SSLError,
++)
++
++from ..request import RequestMethods
++from ..response import HTTPResponse
++from ..util.timeout import Timeout
++from ..util.retry import Retry
++from . import _appengine_environ
++
++try:
++    from google.appengine.api import urlfetch
++except ImportError:
++    urlfetch = None
++
++
++log = logging.getLogger(__name__)
++
++
++class AppEnginePlatformWarning(HTTPWarning):
++    pass
++
++
++class AppEnginePlatformError(HTTPError):
++    pass
++
++
++class AppEngineManager(RequestMethods):
++    """
++    Connection manager for Google App Engine sandbox applications.
++
++    This manager uses the URLFetch service directly instead of using the
++    emulated httplib, and is subject to URLFetch limitations as described in
++    the App Engine documentation `here
++    <https://cloud.google.com/appengine/docs/python/urlfetch>`_.
++
++    Notably it will raise an :class:`AppEnginePlatformError` if:
++        * URLFetch is not available.
++        * If you attempt to use this on App Engine Flexible, as full socket
++          support is available.
++        * If a request size is more than 10 megabytes.
++        * If a response size is more than 32 megabtyes.
++        * If you use an unsupported request method such as OPTIONS.
++
++    Beyond those cases, it will raise normal urllib3 errors.
++    """
++
++    def __init__(
++        self,
++        headers=None,
++        retries=None,
++        validate_certificate=True,
++        urlfetch_retries=True,
++    ):
++        if not urlfetch:
++            raise AppEnginePlatformError(
++                "URLFetch is not available in this environment."
++            )
++
++        warnings.warn(
++            "urllib3 is using URLFetch on Google App Engine sandbox instead "
++            "of sockets. To use sockets directly instead of URLFetch see "
++            "https://urllib3.readthedocs.io/en/latest/reference/urllib3.contrib.html.",
++            AppEnginePlatformWarning,
++        )
++
++        RequestMethods.__init__(self, headers)
++        self.validate_certificate = validate_certificate
++        self.urlfetch_retries = urlfetch_retries
++
++        self.retries = retries or Retry.DEFAULT
++
++    def __enter__(self):
++        return self
++
++    def __exit__(self, exc_type, exc_val, exc_tb):
++        # Return False to re-raise any potential exceptions
++        return False
++
++    def urlopen(
++        self,
++        method,
++        url,
++        body=None,
++        headers=None,
++        retries=None,
++        redirect=True,
++        timeout=Timeout.DEFAULT_TIMEOUT,
++        **response_kw
++    ):
++
++        retries = self._get_retries(retries, redirect)
++
++        try:
++            follow_redirects = redirect and retries.redirect != 0 and retries.total
++            response = urlfetch.fetch(
++                url,
++                payload=body,
++                method=method,
++                headers=headers or {},
++                allow_truncated=False,
++                follow_redirects=self.urlfetch_retries and follow_redirects,
++                deadline=self._get_absolute_timeout(timeout),
++                validate_certificate=self.validate_certificate,
++            )
++        except urlfetch.DeadlineExceededError as e:
++            raise TimeoutError(self, e)
++
++        except urlfetch.InvalidURLError as e:
++            if "too large" in str(e):
++                raise AppEnginePlatformError(
++                    "URLFetch request too large, URLFetch only "
++                    "supports requests up to 10mb in size.",
++                    e,
++                )
++            raise ProtocolError(e)
++
++        except urlfetch.DownloadError as e:
++            if "Too many redirects" in str(e):
++                raise MaxRetryError(self, url, reason=e)
++            raise ProtocolError(e)
++
++        except urlfetch.ResponseTooLargeError as e:
++            raise AppEnginePlatformError(
++                "URLFetch response too large, URLFetch only supports"
++                "responses up to 32mb in size.",
++                e,
++            )
++
++        except urlfetch.SSLCertificateError as e:
++            raise SSLError(e)
++
++        except urlfetch.InvalidMethodError as e:
++            raise AppEnginePlatformError(
++                "URLFetch does not support method: %s" % method, e
++            )
++
++        http_response = self._urlfetch_response_to_http_response(
++            response, retries=retries, **response_kw
++        )
++
++        # Handle redirect?
++        redirect_location = redirect and http_response.get_redirect_location()
++        if redirect_location:
++            # Check for redirect response
++            if self.urlfetch_retries and retries.raise_on_redirect:
++                raise MaxRetryError(self, url, "too many redirects")
++            else:
++                if http_response.status == 303:
++                    method = "GET"
++
++                try:
++                    retries = retries.increment(
++                        method, url, response=http_response, _pool=self
++                    )
++                except MaxRetryError:
++                    if retries.raise_on_redirect:
++                        raise MaxRetryError(self, url, "too many redirects")
++                    return http_response
++
++                retries.sleep_for_retry(http_response)
++                log.debug("Redirecting %s -> %s", url, redirect_location)
++                redirect_url = urljoin(url, redirect_location)
++                return self.urlopen(
++                    method,
++                    redirect_url,
++                    body,
++                    headers,
++                    retries=retries,
++                    redirect=redirect,
++                    timeout=timeout,
++                    **response_kw
++                )
++
++        # Check if we should retry the HTTP response.
++        has_retry_after = bool(http_response.getheader("Retry-After"))
++        if retries.is_retry(method, http_response.status, has_retry_after):
++            retries = retries.increment(method, url, response=http_response, _pool=self)
++            log.debug("Retry: %s", url)
++            retries.sleep(http_response)
++            return self.urlopen(
++                method,
++                url,
++                body=body,
++                headers=headers,
++                retries=retries,
++                redirect=redirect,
++                timeout=timeout,
++                **response_kw
++            )
++
++        return http_response
++
++    def _urlfetch_response_to_http_response(self, urlfetch_resp, **response_kw):
++
++        if is_prod_appengine():
++            # Production GAE handles deflate encoding automatically, but does
++            # not remove the encoding header.
++            content_encoding = urlfetch_resp.headers.get("content-encoding")
++
++            if content_encoding == "deflate":
++                del urlfetch_resp.headers["content-encoding"]
++
++        transfer_encoding = urlfetch_resp.headers.get("transfer-encoding")
++        # We have a full response's content,
++        # so let's make sure we don't report ourselves as chunked data.
++        if transfer_encoding == "chunked":
++            encodings = transfer_encoding.split(",")
++            encodings.remove("chunked")
++            urlfetch_resp.headers["transfer-encoding"] = ",".join(encodings)
++
++        original_response = HTTPResponse(
++            # In order for decoding to work, we must present the content as
++            # a file-like object.
++            body=io.BytesIO(urlfetch_resp.content),
++            msg=urlfetch_resp.header_msg,
++            headers=urlfetch_resp.headers,
++            status=urlfetch_resp.status_code,
++            **response_kw
++        )
++
++        return HTTPResponse(
++            body=io.BytesIO(urlfetch_resp.content),
++            headers=urlfetch_resp.headers,
++            status=urlfetch_resp.status_code,
++            original_response=original_response,
++            **response_kw
++        )
++
++    def _get_absolute_timeout(self, timeout):
++        if timeout is Timeout.DEFAULT_TIMEOUT:
++            return None  # Defer to URLFetch's default.
++        if isinstance(timeout, Timeout):
++            if timeout._read is not None or timeout._connect is not None:
++                warnings.warn(
++                    "URLFetch does not support granular timeout settings, "
++                    "reverting to total or default URLFetch timeout.",
++                    AppEnginePlatformWarning,
++                )
++            return timeout.total
++        return timeout
++
++    def _get_retries(self, retries, redirect):
++        if not isinstance(retries, Retry):
++            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)
++
++        if retries.connect or retries.read or retries.redirect:
++            warnings.warn(
++                "URLFetch only supports total retries and does not "
++                "recognize connect, read, or redirect retry parameters.",
++                AppEnginePlatformWarning,
++            )
++
++        return retries
++
++
++# Alias methods from _appengine_environ to maintain public API interface.
++
++is_appengine = _appengine_environ.is_appengine
++is_appengine_sandbox = _appengine_environ.is_appengine_sandbox
++is_local_appengine = _appengine_environ.is_local_appengine
++is_prod_appengine = _appengine_environ.is_prod_appengine
++is_prod_appengine_mvms = _appengine_environ.is_prod_appengine_mvms
+Index: venv/Lib/site-packages/urllib3/contrib/pyopenssl.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/pyopenssl.py	(date 1573549849443)
++++ venv/Lib/site-packages/urllib3/contrib/pyopenssl.py	(date 1573549849443)
+@@ -0,0 +1,498 @@
++"""
++SSL with SNI_-support for Python 2. Follow these instructions if you would
++like to verify SSL certificates in Python 2. Note, the default libraries do
++*not* do certificate checking; you need to do additional work to validate
++certificates yourself.
++
++This needs the following packages installed:
++
++* pyOpenSSL (tested with 16.0.0)
++* cryptography (minimum 1.3.4, from pyopenssl)
++* idna (minimum 2.0, from cryptography)
++
++However, pyopenssl depends on cryptography, which depends on idna, so while we
++use all three directly here we end up having relatively few packages required.
++
++You can install them with the following command:
++
++    pip install pyopenssl cryptography idna
++
++To activate certificate checking, call
++:func:`~urllib3.contrib.pyopenssl.inject_into_urllib3` from your Python code
++before you begin making HTTP requests. This can be done in a ``sitecustomize``
++module, or at any other time before your application begins using ``urllib3``,
++like this::
++
++    try:
++        import urllib3.contrib.pyopenssl
++        urllib3.contrib.pyopenssl.inject_into_urllib3()
++    except ImportError:
++        pass
++
++Now you can use :mod:`urllib3` as you normally would, and it will support SNI
++when the required modules are installed.
++
++Activating this module also has the positive side effect of disabling SSL/TLS
++compression in Python 2 (see `CRIME attack`_).
++
++If you want to configure the default list of supported cipher suites, you can
++set the ``urllib3.contrib.pyopenssl.DEFAULT_SSL_CIPHER_LIST`` variable.
++
++.. _sni: https://en.wikipedia.org/wiki/Server_Name_Indication
++.. _crime attack: https://en.wikipedia.org/wiki/CRIME_(security_exploit)
++"""
++from __future__ import absolute_import
++
++import OpenSSL.SSL
++from cryptography import x509
++from cryptography.hazmat.backends.openssl import backend as openssl_backend
++from cryptography.hazmat.backends.openssl.x509 import _Certificate
++
++try:
++    from cryptography.x509 import UnsupportedExtension
++except ImportError:
++    # UnsupportedExtension is gone in cryptography >= 2.1.0
++    class UnsupportedExtension(Exception):
++        pass
++
++
++from socket import timeout, error as SocketError
++from io import BytesIO
++
++try:  # Platform-specific: Python 2
++    from socket import _fileobject
++except ImportError:  # Platform-specific: Python 3
++    _fileobject = None
++    from ..packages.backports.makefile import backport_makefile
++
++import logging
++import ssl
++from ..packages import six
++import sys
++
++from .. import util
++
++
++__all__ = ["inject_into_urllib3", "extract_from_urllib3"]
++
++# SNI always works.
++HAS_SNI = True
++
++# Map from urllib3 to PyOpenSSL compatible parameter-values.
++_openssl_versions = {
++    util.PROTOCOL_TLS: OpenSSL.SSL.SSLv23_METHOD,
++    ssl.PROTOCOL_TLSv1: OpenSSL.SSL.TLSv1_METHOD,
++}
++
++if hasattr(ssl, "PROTOCOL_SSLv3") and hasattr(OpenSSL.SSL, "SSLv3_METHOD"):
++    _openssl_versions[ssl.PROTOCOL_SSLv3] = OpenSSL.SSL.SSLv3_METHOD
++
++if hasattr(ssl, "PROTOCOL_TLSv1_1") and hasattr(OpenSSL.SSL, "TLSv1_1_METHOD"):
++    _openssl_versions[ssl.PROTOCOL_TLSv1_1] = OpenSSL.SSL.TLSv1_1_METHOD
++
++if hasattr(ssl, "PROTOCOL_TLSv1_2") and hasattr(OpenSSL.SSL, "TLSv1_2_METHOD"):
++    _openssl_versions[ssl.PROTOCOL_TLSv1_2] = OpenSSL.SSL.TLSv1_2_METHOD
++
++
++_stdlib_to_openssl_verify = {
++    ssl.CERT_NONE: OpenSSL.SSL.VERIFY_NONE,
++    ssl.CERT_OPTIONAL: OpenSSL.SSL.VERIFY_PEER,
++    ssl.CERT_REQUIRED: OpenSSL.SSL.VERIFY_PEER
++    + OpenSSL.SSL.VERIFY_FAIL_IF_NO_PEER_CERT,
++}
++_openssl_to_stdlib_verify = dict((v, k) for k, v in _stdlib_to_openssl_verify.items())
++
++# OpenSSL will only write 16K at a time
++SSL_WRITE_BLOCKSIZE = 16384
++
++orig_util_HAS_SNI = util.HAS_SNI
++orig_util_SSLContext = util.ssl_.SSLContext
++
++
++log = logging.getLogger(__name__)
++
++
++def inject_into_urllib3():
++    "Monkey-patch urllib3 with PyOpenSSL-backed SSL-support."
++
++    _validate_dependencies_met()
++
++    util.SSLContext = PyOpenSSLContext
++    util.ssl_.SSLContext = PyOpenSSLContext
++    util.HAS_SNI = HAS_SNI
++    util.ssl_.HAS_SNI = HAS_SNI
++    util.IS_PYOPENSSL = True
++    util.ssl_.IS_PYOPENSSL = True
++
++
++def extract_from_urllib3():
++    "Undo monkey-patching by :func:`inject_into_urllib3`."
++
++    util.SSLContext = orig_util_SSLContext
++    util.ssl_.SSLContext = orig_util_SSLContext
++    util.HAS_SNI = orig_util_HAS_SNI
++    util.ssl_.HAS_SNI = orig_util_HAS_SNI
++    util.IS_PYOPENSSL = False
++    util.ssl_.IS_PYOPENSSL = False
++
++
++def _validate_dependencies_met():
++    """
++    Verifies that PyOpenSSL's package-level dependencies have been met.
++    Throws `ImportError` if they are not met.
++    """
++    # Method added in `cryptography==1.1`; not available in older versions
++    from cryptography.x509.extensions import Extensions
++
++    if getattr(Extensions, "get_extension_for_class", None) is None:
++        raise ImportError(
++            "'cryptography' module missing required functionality.  "
++            "Try upgrading to v1.3.4 or newer."
++        )
++
++    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509
++    # attribute is only present on those versions.
++    from OpenSSL.crypto import X509
++
++    x509 = X509()
++    if getattr(x509, "_x509", None) is None:
++        raise ImportError(
++            "'pyOpenSSL' module missing required functionality. "
++            "Try upgrading to v0.14 or newer."
++        )
++
++
++def _dnsname_to_stdlib(name):
++    """
++    Converts a dNSName SubjectAlternativeName field to the form used by the
++    standard library on the given Python version.
++
++    Cryptography produces a dNSName as a unicode string that was idna-decoded
++    from ASCII bytes. We need to idna-encode that string to get it back, and
++    then on Python 3 we also need to convert to unicode via UTF-8 (the stdlib
++    uses PyUnicode_FromStringAndSize on it, which decodes via UTF-8).
++
++    If the name cannot be idna-encoded then we return None signalling that
++    the name given should be skipped.
++    """
++
++    def idna_encode(name):
++        """
++        Borrowed wholesale from the Python Cryptography Project. It turns out
++        that we can't just safely call `idna.encode`: it can explode for
++        wildcard names. This avoids that problem.
++        """
++        import idna
++
++        try:
++            for prefix in [u"*.", u"."]:
++                if name.startswith(prefix):
++                    name = name[len(prefix) :]
++                    return prefix.encode("ascii") + idna.encode(name)
++            return idna.encode(name)
++        except idna.core.IDNAError:
++            return None
++
++    # Don't send IPv6 addresses through the IDNA encoder.
++    if ":" in name:
++        return name
++
++    name = idna_encode(name)
++    if name is None:
++        return None
++    elif sys.version_info >= (3, 0):
++        name = name.decode("utf-8")
++    return name
++
++
++def get_subj_alt_name(peer_cert):
++    """
++    Given an PyOpenSSL certificate, provides all the subject alternative names.
++    """
++    # Pass the cert to cryptography, which has much better APIs for this.
++    if hasattr(peer_cert, "to_cryptography"):
++        cert = peer_cert.to_cryptography()
++    else:
++        # This is technically using private APIs, but should work across all
++        # relevant versions before PyOpenSSL got a proper API for this.
++        cert = _Certificate(openssl_backend, peer_cert._x509)
++
++    # We want to find the SAN extension. Ask Cryptography to locate it (it's
++    # faster than looping in Python)
++    try:
++        ext = cert.extensions.get_extension_for_class(x509.SubjectAlternativeName).value
++    except x509.ExtensionNotFound:
++        # No such extension, return the empty list.
++        return []
++    except (
++        x509.DuplicateExtension,
++        UnsupportedExtension,
++        x509.UnsupportedGeneralNameType,
++        UnicodeError,
++    ) as e:
++        # A problem has been found with the quality of the certificate. Assume
++        # no SAN field is present.
++        log.warning(
++            "A problem was encountered with the certificate that prevented "
++            "urllib3 from finding the SubjectAlternativeName field. This can "
++            "affect certificate validation. The error was %s",
++            e,
++        )
++        return []
++
++    # We want to return dNSName and iPAddress fields. We need to cast the IPs
++    # back to strings because the match_hostname function wants them as
++    # strings.
++    # Sadly the DNS names need to be idna encoded and then, on Python 3, UTF-8
++    # decoded. This is pretty frustrating, but that's what the standard library
++    # does with certificates, and so we need to attempt to do the same.
++    # We also want to skip over names which cannot be idna encoded.
++    names = [
++        ("DNS", name)
++        for name in map(_dnsname_to_stdlib, ext.get_values_for_type(x509.DNSName))
++        if name is not None
++    ]
++    names.extend(
++        ("IP Address", str(name)) for name in ext.get_values_for_type(x509.IPAddress)
++    )
++
++    return names
++
++
++class WrappedSocket(object):
++    """API-compatibility wrapper for Python OpenSSL's Connection-class.
++
++    Note: _makefile_refs, _drop() and _reuse() are needed for the garbage
++    collector of pypy.
++    """
++
++    def __init__(self, connection, socket, suppress_ragged_eofs=True):
++        self.connection = connection
++        self.socket = socket
++        self.suppress_ragged_eofs = suppress_ragged_eofs
++        self._makefile_refs = 0
++        self._closed = False
++
++    def fileno(self):
++        return self.socket.fileno()
++
++    # Copy-pasted from Python 3.5 source code
++    def _decref_socketios(self):
++        if self._makefile_refs > 0:
++            self._makefile_refs -= 1
++        if self._closed:
++            self.close()
++
++    def recv(self, *args, **kwargs):
++        try:
++            data = self.connection.recv(*args, **kwargs)
++        except OpenSSL.SSL.SysCallError as e:
++            if self.suppress_ragged_eofs and e.args == (-1, "Unexpected EOF"):
++                return b""
++            else:
++                raise SocketError(str(e))
++        except OpenSSL.SSL.ZeroReturnError:
++            if self.connection.get_shutdown() == OpenSSL.SSL.RECEIVED_SHUTDOWN:
++                return b""
++            else:
++                raise
++        except OpenSSL.SSL.WantReadError:
++            if not util.wait_for_read(self.socket, self.socket.gettimeout()):
++                raise timeout("The read operation timed out")
++            else:
++                return self.recv(*args, **kwargs)
++
++        # TLS 1.3 post-handshake authentication
++        except OpenSSL.SSL.Error as e:
++            raise ssl.SSLError("read error: %r" % e)
++        else:
++            return data
++
++    def recv_into(self, *args, **kwargs):
++        try:
++            return self.connection.recv_into(*args, **kwargs)
++        except OpenSSL.SSL.SysCallError as e:
++            if self.suppress_ragged_eofs and e.args == (-1, "Unexpected EOF"):
++                return 0
++            else:
++                raise SocketError(str(e))
++        except OpenSSL.SSL.ZeroReturnError:
++            if self.connection.get_shutdown() == OpenSSL.SSL.RECEIVED_SHUTDOWN:
++                return 0
++            else:
++                raise
++        except OpenSSL.SSL.WantReadError:
++            if not util.wait_for_read(self.socket, self.socket.gettimeout()):
++                raise timeout("The read operation timed out")
++            else:
++                return self.recv_into(*args, **kwargs)
++
++        # TLS 1.3 post-handshake authentication
++        except OpenSSL.SSL.Error as e:
++            raise ssl.SSLError("read error: %r" % e)
++
++    def settimeout(self, timeout):
++        return self.socket.settimeout(timeout)
++
++    def _send_until_done(self, data):
++        while True:
++            try:
++                return self.connection.send(data)
++            except OpenSSL.SSL.WantWriteError:
++                if not util.wait_for_write(self.socket, self.socket.gettimeout()):
++                    raise timeout()
++                continue
++            except OpenSSL.SSL.SysCallError as e:
++                raise SocketError(str(e))
++
++    def sendall(self, data):
++        total_sent = 0
++        while total_sent < len(data):
++            sent = self._send_until_done(
++                data[total_sent : total_sent + SSL_WRITE_BLOCKSIZE]
++            )
++            total_sent += sent
++
++    def shutdown(self):
++        # FIXME rethrow compatible exceptions should we ever use this
++        self.connection.shutdown()
++
++    def close(self):
++        if self._makefile_refs < 1:
++            try:
++                self._closed = True
++                return self.connection.close()
++            except OpenSSL.SSL.Error:
++                return
++        else:
++            self._makefile_refs -= 1
++
++    def getpeercert(self, binary_form=False):
++        x509 = self.connection.get_peer_certificate()
++
++        if not x509:
++            return x509
++
++        if binary_form:
++            return OpenSSL.crypto.dump_certificate(OpenSSL.crypto.FILETYPE_ASN1, x509)
++
++        return {
++            "subject": ((("commonName", x509.get_subject().CN),),),
++            "subjectAltName": get_subj_alt_name(x509),
++        }
++
++    def version(self):
++        return self.connection.get_protocol_version_name()
++
++    def _reuse(self):
++        self._makefile_refs += 1
++
++    def _drop(self):
++        if self._makefile_refs < 1:
++            self.close()
++        else:
++            self._makefile_refs -= 1
++
++
++if _fileobject:  # Platform-specific: Python 2
++
++    def makefile(self, mode, bufsize=-1):
++        self._makefile_refs += 1
++        return _fileobject(self, mode, bufsize, close=True)
++
++
++else:  # Platform-specific: Python 3
++    makefile = backport_makefile
++
++WrappedSocket.makefile = makefile
++
++
++class PyOpenSSLContext(object):
++    """
++    I am a wrapper class for the PyOpenSSL ``Context`` object. I am responsible
++    for translating the interface of the standard library ``SSLContext`` object
++    to calls into PyOpenSSL.
++    """
++
++    def __init__(self, protocol):
++        self.protocol = _openssl_versions[protocol]
++        self._ctx = OpenSSL.SSL.Context(self.protocol)
++        self._options = 0
++        self.check_hostname = False
++
++    @property
++    def options(self):
++        return self._options
++
++    @options.setter
++    def options(self, value):
++        self._options = value
++        self._ctx.set_options(value)
++
++    @property
++    def verify_mode(self):
++        return _openssl_to_stdlib_verify[self._ctx.get_verify_mode()]
++
++    @verify_mode.setter
++    def verify_mode(self, value):
++        self._ctx.set_verify(_stdlib_to_openssl_verify[value], _verify_callback)
++
++    def set_default_verify_paths(self):
++        self._ctx.set_default_verify_paths()
++
++    def set_ciphers(self, ciphers):
++        if isinstance(ciphers, six.text_type):
++            ciphers = ciphers.encode("utf-8")
++        self._ctx.set_cipher_list(ciphers)
++
++    def load_verify_locations(self, cafile=None, capath=None, cadata=None):
++        if cafile is not None:
++            cafile = cafile.encode("utf-8")
++        if capath is not None:
++            capath = capath.encode("utf-8")
++        self._ctx.load_verify_locations(cafile, capath)
++        if cadata is not None:
++            self._ctx.load_verify_locations(BytesIO(cadata))
++
++    def load_cert_chain(self, certfile, keyfile=None, password=None):
++        self._ctx.use_certificate_chain_file(certfile)
++        if password is not None:
++            if not isinstance(password, six.binary_type):
++                password = password.encode("utf-8")
++            self._ctx.set_passwd_cb(lambda *_: password)
++        self._ctx.use_privatekey_file(keyfile or certfile)
++
++    def wrap_socket(
++        self,
++        sock,
++        server_side=False,
++        do_handshake_on_connect=True,
++        suppress_ragged_eofs=True,
++        server_hostname=None,
++    ):
++        cnx = OpenSSL.SSL.Connection(self._ctx, sock)
++
++        if isinstance(server_hostname, six.text_type):  # Platform-specific: Python 3
++            server_hostname = server_hostname.encode("utf-8")
++
++        if server_hostname is not None:
++            cnx.set_tlsext_host_name(server_hostname)
++
++        cnx.set_connect_state()
++
++        while True:
++            try:
++                cnx.do_handshake()
++            except OpenSSL.SSL.WantReadError:
++                if not util.wait_for_read(sock, sock.gettimeout()):
++                    raise timeout("select timed out")
++                continue
++            except OpenSSL.SSL.Error as e:
++                raise ssl.SSLError("bad handshake: %r" % e)
++            break
++
++        return WrappedSocket(cnx, sock)
++
++
++def _verify_callback(cnx, x509, err_no, err_depth, return_code):
++    return err_no == 0
+Index: venv/Lib/site-packages/urllib3/contrib/_securetransport/bindings.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/_securetransport/bindings.py	(date 1573549849465)
++++ venv/Lib/site-packages/urllib3/contrib/_securetransport/bindings.py	(date 1573549849465)
+@@ -0,0 +1,493 @@
++"""
++This module uses ctypes to bind a whole bunch of functions and constants from
++SecureTransport. The goal here is to provide the low-level API to
++SecureTransport. These are essentially the C-level functions and constants, and
++they're pretty gross to work with.
++
++This code is a bastardised version of the code found in Will Bond's oscrypto
++library. An enormous debt is owed to him for blazing this trail for us. For
++that reason, this code should be considered to be covered both by urllib3's
++license and by oscrypto's:
++
++    Copyright (c) 2015-2016 Will Bond <will@wbond.net>
++
++    Permission is hereby granted, free of charge, to any person obtaining a
++    copy of this software and associated documentation files (the "Software"),
++    to deal in the Software without restriction, including without limitation
++    the rights to use, copy, modify, merge, publish, distribute, sublicense,
++    and/or sell copies of the Software, and to permit persons to whom the
++    Software is furnished to do so, subject to the following conditions:
++
++    The above copyright notice and this permission notice shall be included in
++    all copies or substantial portions of the Software.
++
++    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
++    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
++    FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
++    DEALINGS IN THE SOFTWARE.
++"""
++from __future__ import absolute_import
++
++import platform
++from ctypes.util import find_library
++from ctypes import (
++    c_void_p,
++    c_int32,
++    c_char_p,
++    c_size_t,
++    c_byte,
++    c_uint32,
++    c_ulong,
++    c_long,
++    c_bool,
++)
++from ctypes import CDLL, POINTER, CFUNCTYPE
++
++
++security_path = find_library("Security")
++if not security_path:
++    raise ImportError("The library Security could not be found")
++
++
++core_foundation_path = find_library("CoreFoundation")
++if not core_foundation_path:
++    raise ImportError("The library CoreFoundation could not be found")
++
++
++version = platform.mac_ver()[0]
++version_info = tuple(map(int, version.split(".")))
++if version_info < (10, 8):
++    raise OSError(
++        "Only OS X 10.8 and newer are supported, not %s.%s"
++        % (version_info[0], version_info[1])
++    )
++
++Security = CDLL(security_path, use_errno=True)
++CoreFoundation = CDLL(core_foundation_path, use_errno=True)
++
++Boolean = c_bool
++CFIndex = c_long
++CFStringEncoding = c_uint32
++CFData = c_void_p
++CFString = c_void_p
++CFArray = c_void_p
++CFMutableArray = c_void_p
++CFDictionary = c_void_p
++CFError = c_void_p
++CFType = c_void_p
++CFTypeID = c_ulong
++
++CFTypeRef = POINTER(CFType)
++CFAllocatorRef = c_void_p
++
++OSStatus = c_int32
++
++CFDataRef = POINTER(CFData)
++CFStringRef = POINTER(CFString)
++CFArrayRef = POINTER(CFArray)
++CFMutableArrayRef = POINTER(CFMutableArray)
++CFDictionaryRef = POINTER(CFDictionary)
++CFArrayCallBacks = c_void_p
++CFDictionaryKeyCallBacks = c_void_p
++CFDictionaryValueCallBacks = c_void_p
++
++SecCertificateRef = POINTER(c_void_p)
++SecExternalFormat = c_uint32
++SecExternalItemType = c_uint32
++SecIdentityRef = POINTER(c_void_p)
++SecItemImportExportFlags = c_uint32
++SecItemImportExportKeyParameters = c_void_p
++SecKeychainRef = POINTER(c_void_p)
++SSLProtocol = c_uint32
++SSLCipherSuite = c_uint32
++SSLContextRef = POINTER(c_void_p)
++SecTrustRef = POINTER(c_void_p)
++SSLConnectionRef = c_uint32
++SecTrustResultType = c_uint32
++SecTrustOptionFlags = c_uint32
++SSLProtocolSide = c_uint32
++SSLConnectionType = c_uint32
++SSLSessionOption = c_uint32
++
++
++try:
++    Security.SecItemImport.argtypes = [
++        CFDataRef,
++        CFStringRef,
++        POINTER(SecExternalFormat),
++        POINTER(SecExternalItemType),
++        SecItemImportExportFlags,
++        POINTER(SecItemImportExportKeyParameters),
++        SecKeychainRef,
++        POINTER(CFArrayRef),
++    ]
++    Security.SecItemImport.restype = OSStatus
++
++    Security.SecCertificateGetTypeID.argtypes = []
++    Security.SecCertificateGetTypeID.restype = CFTypeID
++
++    Security.SecIdentityGetTypeID.argtypes = []
++    Security.SecIdentityGetTypeID.restype = CFTypeID
++
++    Security.SecKeyGetTypeID.argtypes = []
++    Security.SecKeyGetTypeID.restype = CFTypeID
++
++    Security.SecCertificateCreateWithData.argtypes = [CFAllocatorRef, CFDataRef]
++    Security.SecCertificateCreateWithData.restype = SecCertificateRef
++
++    Security.SecCertificateCopyData.argtypes = [SecCertificateRef]
++    Security.SecCertificateCopyData.restype = CFDataRef
++
++    Security.SecCopyErrorMessageString.argtypes = [OSStatus, c_void_p]
++    Security.SecCopyErrorMessageString.restype = CFStringRef
++
++    Security.SecIdentityCreateWithCertificate.argtypes = [
++        CFTypeRef,
++        SecCertificateRef,
++        POINTER(SecIdentityRef),
++    ]
++    Security.SecIdentityCreateWithCertificate.restype = OSStatus
++
++    Security.SecKeychainCreate.argtypes = [
++        c_char_p,
++        c_uint32,
++        c_void_p,
++        Boolean,
++        c_void_p,
++        POINTER(SecKeychainRef),
++    ]
++    Security.SecKeychainCreate.restype = OSStatus
++
++    Security.SecKeychainDelete.argtypes = [SecKeychainRef]
++    Security.SecKeychainDelete.restype = OSStatus
++
++    Security.SecPKCS12Import.argtypes = [
++        CFDataRef,
++        CFDictionaryRef,
++        POINTER(CFArrayRef),
++    ]
++    Security.SecPKCS12Import.restype = OSStatus
++
++    SSLReadFunc = CFUNCTYPE(OSStatus, SSLConnectionRef, c_void_p, POINTER(c_size_t))
++    SSLWriteFunc = CFUNCTYPE(
++        OSStatus, SSLConnectionRef, POINTER(c_byte), POINTER(c_size_t)
++    )
++
++    Security.SSLSetIOFuncs.argtypes = [SSLContextRef, SSLReadFunc, SSLWriteFunc]
++    Security.SSLSetIOFuncs.restype = OSStatus
++
++    Security.SSLSetPeerID.argtypes = [SSLContextRef, c_char_p, c_size_t]
++    Security.SSLSetPeerID.restype = OSStatus
++
++    Security.SSLSetCertificate.argtypes = [SSLContextRef, CFArrayRef]
++    Security.SSLSetCertificate.restype = OSStatus
++
++    Security.SSLSetCertificateAuthorities.argtypes = [SSLContextRef, CFTypeRef, Boolean]
++    Security.SSLSetCertificateAuthorities.restype = OSStatus
++
++    Security.SSLSetConnection.argtypes = [SSLContextRef, SSLConnectionRef]
++    Security.SSLSetConnection.restype = OSStatus
++
++    Security.SSLSetPeerDomainName.argtypes = [SSLContextRef, c_char_p, c_size_t]
++    Security.SSLSetPeerDomainName.restype = OSStatus
++
++    Security.SSLHandshake.argtypes = [SSLContextRef]
++    Security.SSLHandshake.restype = OSStatus
++
++    Security.SSLRead.argtypes = [SSLContextRef, c_char_p, c_size_t, POINTER(c_size_t)]
++    Security.SSLRead.restype = OSStatus
++
++    Security.SSLWrite.argtypes = [SSLContextRef, c_char_p, c_size_t, POINTER(c_size_t)]
++    Security.SSLWrite.restype = OSStatus
++
++    Security.SSLClose.argtypes = [SSLContextRef]
++    Security.SSLClose.restype = OSStatus
++
++    Security.SSLGetNumberSupportedCiphers.argtypes = [SSLContextRef, POINTER(c_size_t)]
++    Security.SSLGetNumberSupportedCiphers.restype = OSStatus
++
++    Security.SSLGetSupportedCiphers.argtypes = [
++        SSLContextRef,
++        POINTER(SSLCipherSuite),
++        POINTER(c_size_t),
++    ]
++    Security.SSLGetSupportedCiphers.restype = OSStatus
++
++    Security.SSLSetEnabledCiphers.argtypes = [
++        SSLContextRef,
++        POINTER(SSLCipherSuite),
++        c_size_t,
++    ]
++    Security.SSLSetEnabledCiphers.restype = OSStatus
++
++    Security.SSLGetNumberEnabledCiphers.argtype = [SSLContextRef, POINTER(c_size_t)]
++    Security.SSLGetNumberEnabledCiphers.restype = OSStatus
++
++    Security.SSLGetEnabledCiphers.argtypes = [
++        SSLContextRef,
++        POINTER(SSLCipherSuite),
++        POINTER(c_size_t),
++    ]
++    Security.SSLGetEnabledCiphers.restype = OSStatus
++
++    Security.SSLGetNegotiatedCipher.argtypes = [SSLContextRef, POINTER(SSLCipherSuite)]
++    Security.SSLGetNegotiatedCipher.restype = OSStatus
++
++    Security.SSLGetNegotiatedProtocolVersion.argtypes = [
++        SSLContextRef,
++        POINTER(SSLProtocol),
++    ]
++    Security.SSLGetNegotiatedProtocolVersion.restype = OSStatus
++
++    Security.SSLCopyPeerTrust.argtypes = [SSLContextRef, POINTER(SecTrustRef)]
++    Security.SSLCopyPeerTrust.restype = OSStatus
++
++    Security.SecTrustSetAnchorCertificates.argtypes = [SecTrustRef, CFArrayRef]
++    Security.SecTrustSetAnchorCertificates.restype = OSStatus
++
++    Security.SecTrustSetAnchorCertificatesOnly.argstypes = [SecTrustRef, Boolean]
++    Security.SecTrustSetAnchorCertificatesOnly.restype = OSStatus
++
++    Security.SecTrustEvaluate.argtypes = [SecTrustRef, POINTER(SecTrustResultType)]
++    Security.SecTrustEvaluate.restype = OSStatus
++
++    Security.SecTrustGetCertificateCount.argtypes = [SecTrustRef]
++    Security.SecTrustGetCertificateCount.restype = CFIndex
++
++    Security.SecTrustGetCertificateAtIndex.argtypes = [SecTrustRef, CFIndex]
++    Security.SecTrustGetCertificateAtIndex.restype = SecCertificateRef
++
++    Security.SSLCreateContext.argtypes = [
++        CFAllocatorRef,
++        SSLProtocolSide,
++        SSLConnectionType,
++    ]
++    Security.SSLCreateContext.restype = SSLContextRef
++
++    Security.SSLSetSessionOption.argtypes = [SSLContextRef, SSLSessionOption, Boolean]
++    Security.SSLSetSessionOption.restype = OSStatus
++
++    Security.SSLSetProtocolVersionMin.argtypes = [SSLContextRef, SSLProtocol]
++    Security.SSLSetProtocolVersionMin.restype = OSStatus
++
++    Security.SSLSetProtocolVersionMax.argtypes = [SSLContextRef, SSLProtocol]
++    Security.SSLSetProtocolVersionMax.restype = OSStatus
++
++    Security.SecCopyErrorMessageString.argtypes = [OSStatus, c_void_p]
++    Security.SecCopyErrorMessageString.restype = CFStringRef
++
++    Security.SSLReadFunc = SSLReadFunc
++    Security.SSLWriteFunc = SSLWriteFunc
++    Security.SSLContextRef = SSLContextRef
++    Security.SSLProtocol = SSLProtocol
++    Security.SSLCipherSuite = SSLCipherSuite
++    Security.SecIdentityRef = SecIdentityRef
++    Security.SecKeychainRef = SecKeychainRef
++    Security.SecTrustRef = SecTrustRef
++    Security.SecTrustResultType = SecTrustResultType
++    Security.SecExternalFormat = SecExternalFormat
++    Security.OSStatus = OSStatus
++
++    Security.kSecImportExportPassphrase = CFStringRef.in_dll(
++        Security, "kSecImportExportPassphrase"
++    )
++    Security.kSecImportItemIdentity = CFStringRef.in_dll(
++        Security, "kSecImportItemIdentity"
++    )
++
++    # CoreFoundation time!
++    CoreFoundation.CFRetain.argtypes = [CFTypeRef]
++    CoreFoundation.CFRetain.restype = CFTypeRef
++
++    CoreFoundation.CFRelease.argtypes = [CFTypeRef]
++    CoreFoundation.CFRelease.restype = None
++
++    CoreFoundation.CFGetTypeID.argtypes = [CFTypeRef]
++    CoreFoundation.CFGetTypeID.restype = CFTypeID
++
++    CoreFoundation.CFStringCreateWithCString.argtypes = [
++        CFAllocatorRef,
++        c_char_p,
++        CFStringEncoding,
++    ]
++    CoreFoundation.CFStringCreateWithCString.restype = CFStringRef
++
++    CoreFoundation.CFStringGetCStringPtr.argtypes = [CFStringRef, CFStringEncoding]
++    CoreFoundation.CFStringGetCStringPtr.restype = c_char_p
++
++    CoreFoundation.CFStringGetCString.argtypes = [
++        CFStringRef,
++        c_char_p,
++        CFIndex,
++        CFStringEncoding,
++    ]
++    CoreFoundation.CFStringGetCString.restype = c_bool
++
++    CoreFoundation.CFDataCreate.argtypes = [CFAllocatorRef, c_char_p, CFIndex]
++    CoreFoundation.CFDataCreate.restype = CFDataRef
++
++    CoreFoundation.CFDataGetLength.argtypes = [CFDataRef]
++    CoreFoundation.CFDataGetLength.restype = CFIndex
++
++    CoreFoundation.CFDataGetBytePtr.argtypes = [CFDataRef]
++    CoreFoundation.CFDataGetBytePtr.restype = c_void_p
++
++    CoreFoundation.CFDictionaryCreate.argtypes = [
++        CFAllocatorRef,
++        POINTER(CFTypeRef),
++        POINTER(CFTypeRef),
++        CFIndex,
++        CFDictionaryKeyCallBacks,
++        CFDictionaryValueCallBacks,
++    ]
++    CoreFoundation.CFDictionaryCreate.restype = CFDictionaryRef
++
++    CoreFoundation.CFDictionaryGetValue.argtypes = [CFDictionaryRef, CFTypeRef]
++    CoreFoundation.CFDictionaryGetValue.restype = CFTypeRef
++
++    CoreFoundation.CFArrayCreate.argtypes = [
++        CFAllocatorRef,
++        POINTER(CFTypeRef),
++        CFIndex,
++        CFArrayCallBacks,
++    ]
++    CoreFoundation.CFArrayCreate.restype = CFArrayRef
++
++    CoreFoundation.CFArrayCreateMutable.argtypes = [
++        CFAllocatorRef,
++        CFIndex,
++        CFArrayCallBacks,
++    ]
++    CoreFoundation.CFArrayCreateMutable.restype = CFMutableArrayRef
++
++    CoreFoundation.CFArrayAppendValue.argtypes = [CFMutableArrayRef, c_void_p]
++    CoreFoundation.CFArrayAppendValue.restype = None
++
++    CoreFoundation.CFArrayGetCount.argtypes = [CFArrayRef]
++    CoreFoundation.CFArrayGetCount.restype = CFIndex
++
++    CoreFoundation.CFArrayGetValueAtIndex.argtypes = [CFArrayRef, CFIndex]
++    CoreFoundation.CFArrayGetValueAtIndex.restype = c_void_p
++
++    CoreFoundation.kCFAllocatorDefault = CFAllocatorRef.in_dll(
++        CoreFoundation, "kCFAllocatorDefault"
++    )
++    CoreFoundation.kCFTypeArrayCallBacks = c_void_p.in_dll(
++        CoreFoundation, "kCFTypeArrayCallBacks"
++    )
++    CoreFoundation.kCFTypeDictionaryKeyCallBacks = c_void_p.in_dll(
++        CoreFoundation, "kCFTypeDictionaryKeyCallBacks"
++    )
++    CoreFoundation.kCFTypeDictionaryValueCallBacks = c_void_p.in_dll(
++        CoreFoundation, "kCFTypeDictionaryValueCallBacks"
++    )
++
++    CoreFoundation.CFTypeRef = CFTypeRef
++    CoreFoundation.CFArrayRef = CFArrayRef
++    CoreFoundation.CFStringRef = CFStringRef
++    CoreFoundation.CFDictionaryRef = CFDictionaryRef
++
++except (AttributeError):
++    raise ImportError("Error initializing ctypes")
++
++
++class CFConst(object):
++    """
++    A class object that acts as essentially a namespace for CoreFoundation
++    constants.
++    """
++
++    kCFStringEncodingUTF8 = CFStringEncoding(0x08000100)
++
++
++class SecurityConst(object):
++    """
++    A class object that acts as essentially a namespace for Security constants.
++    """
++
++    kSSLSessionOptionBreakOnServerAuth = 0
++
++    kSSLProtocol2 = 1
++    kSSLProtocol3 = 2
++    kTLSProtocol1 = 4
++    kTLSProtocol11 = 7
++    kTLSProtocol12 = 8
++    # SecureTransport does not support TLS 1.3 even if there's a constant for it
++    kTLSProtocol13 = 10
++    kTLSProtocolMaxSupported = 999
++
++    kSSLClientSide = 1
++    kSSLStreamType = 0
++
++    kSecFormatPEMSequence = 10
++
++    kSecTrustResultInvalid = 0
++    kSecTrustResultProceed = 1
++    # This gap is present on purpose: this was kSecTrustResultConfirm, which
++    # is deprecated.
++    kSecTrustResultDeny = 3
++    kSecTrustResultUnspecified = 4
++    kSecTrustResultRecoverableTrustFailure = 5
++    kSecTrustResultFatalTrustFailure = 6
++    kSecTrustResultOtherError = 7
++
++    errSSLProtocol = -9800
++    errSSLWouldBlock = -9803
++    errSSLClosedGraceful = -9805
++    errSSLClosedNoNotify = -9816
++    errSSLClosedAbort = -9806
++
++    errSSLXCertChainInvalid = -9807
++    errSSLCrypto = -9809
++    errSSLInternal = -9810
++    errSSLCertExpired = -9814
++    errSSLCertNotYetValid = -9815
++    errSSLUnknownRootCert = -9812
++    errSSLNoRootCert = -9813
++    errSSLHostNameMismatch = -9843
++    errSSLPeerHandshakeFail = -9824
++    errSSLPeerUserCancelled = -9839
++    errSSLWeakPeerEphemeralDHKey = -9850
++    errSSLServerAuthCompleted = -9841
++    errSSLRecordOverflow = -9847
++
++    errSecVerifyFailed = -67808
++    errSecNoTrustSettings = -25263
++    errSecItemNotFound = -25300
++    errSecInvalidTrustSettings = -25262
++
++    # Cipher suites. We only pick the ones our default cipher string allows.
++    # Source: https://developer.apple.com/documentation/security/1550981-ssl_cipher_suite_values
++    TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 = 0xC02C
++    TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 = 0xC030
++    TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 = 0xC02B
++    TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 = 0xC02F
++    TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 = 0xCCA9
++    TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 = 0xCCA8
++    TLS_DHE_RSA_WITH_AES_256_GCM_SHA384 = 0x009F
++    TLS_DHE_RSA_WITH_AES_128_GCM_SHA256 = 0x009E
++    TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384 = 0xC024
++    TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384 = 0xC028
++    TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA = 0xC00A
++    TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA = 0xC014
++    TLS_DHE_RSA_WITH_AES_256_CBC_SHA256 = 0x006B
++    TLS_DHE_RSA_WITH_AES_256_CBC_SHA = 0x0039
++    TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256 = 0xC023
++    TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256 = 0xC027
++    TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA = 0xC009
++    TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA = 0xC013
++    TLS_DHE_RSA_WITH_AES_128_CBC_SHA256 = 0x0067
++    TLS_DHE_RSA_WITH_AES_128_CBC_SHA = 0x0033
++    TLS_RSA_WITH_AES_256_GCM_SHA384 = 0x009D
++    TLS_RSA_WITH_AES_128_GCM_SHA256 = 0x009C
++    TLS_RSA_WITH_AES_256_CBC_SHA256 = 0x003D
++    TLS_RSA_WITH_AES_128_CBC_SHA256 = 0x003C
++    TLS_RSA_WITH_AES_256_CBC_SHA = 0x0035
++    TLS_RSA_WITH_AES_128_CBC_SHA = 0x002F
++    TLS_AES_128_GCM_SHA256 = 0x1301
++    TLS_AES_256_GCM_SHA384 = 0x1302
++    TLS_AES_128_CCM_8_SHA256 = 0x1305
++    TLS_AES_128_CCM_SHA256 = 0x1304
+Index: venv/Lib/site-packages/urllib3/contrib/_securetransport/low_level.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/_securetransport/low_level.py	(date 1573549849470)
++++ venv/Lib/site-packages/urllib3/contrib/_securetransport/low_level.py	(date 1573549849470)
+@@ -0,0 +1,328 @@
++"""
++Low-level helpers for the SecureTransport bindings.
++
++These are Python functions that are not directly related to the high-level APIs
++but are necessary to get them to work. They include a whole bunch of low-level
++CoreFoundation messing about and memory management. The concerns in this module
++are almost entirely about trying to avoid memory leaks and providing
++appropriate and useful assistance to the higher-level code.
++"""
++import base64
++import ctypes
++import itertools
++import re
++import os
++import ssl
++import tempfile
++
++from .bindings import Security, CoreFoundation, CFConst
++
++
++# This regular expression is used to grab PEM data out of a PEM bundle.
++_PEM_CERTS_RE = re.compile(
++    b"-----BEGIN CERTIFICATE-----\n(.*?)\n-----END CERTIFICATE-----", re.DOTALL
++)
++
++
++def _cf_data_from_bytes(bytestring):
++    """
++    Given a bytestring, create a CFData object from it. This CFData object must
++    be CFReleased by the caller.
++    """
++    return CoreFoundation.CFDataCreate(
++        CoreFoundation.kCFAllocatorDefault, bytestring, len(bytestring)
++    )
++
++
++def _cf_dictionary_from_tuples(tuples):
++    """
++    Given a list of Python tuples, create an associated CFDictionary.
++    """
++    dictionary_size = len(tuples)
++
++    # We need to get the dictionary keys and values out in the same order.
++    keys = (t[0] for t in tuples)
++    values = (t[1] for t in tuples)
++    cf_keys = (CoreFoundation.CFTypeRef * dictionary_size)(*keys)
++    cf_values = (CoreFoundation.CFTypeRef * dictionary_size)(*values)
++
++    return CoreFoundation.CFDictionaryCreate(
++        CoreFoundation.kCFAllocatorDefault,
++        cf_keys,
++        cf_values,
++        dictionary_size,
++        CoreFoundation.kCFTypeDictionaryKeyCallBacks,
++        CoreFoundation.kCFTypeDictionaryValueCallBacks,
++    )
++
++
++def _cf_string_to_unicode(value):
++    """
++    Creates a Unicode string from a CFString object. Used entirely for error
++    reporting.
++
++    Yes, it annoys me quite a lot that this function is this complex.
++    """
++    value_as_void_p = ctypes.cast(value, ctypes.POINTER(ctypes.c_void_p))
++
++    string = CoreFoundation.CFStringGetCStringPtr(
++        value_as_void_p, CFConst.kCFStringEncodingUTF8
++    )
++    if string is None:
++        buffer = ctypes.create_string_buffer(1024)
++        result = CoreFoundation.CFStringGetCString(
++            value_as_void_p, buffer, 1024, CFConst.kCFStringEncodingUTF8
++        )
++        if not result:
++            raise OSError("Error copying C string from CFStringRef")
++        string = buffer.value
++    if string is not None:
++        string = string.decode("utf-8")
++    return string
++
++
++def _assert_no_error(error, exception_class=None):
++    """
++    Checks the return code and throws an exception if there is an error to
++    report
++    """
++    if error == 0:
++        return
++
++    cf_error_string = Security.SecCopyErrorMessageString(error, None)
++    output = _cf_string_to_unicode(cf_error_string)
++    CoreFoundation.CFRelease(cf_error_string)
++
++    if output is None or output == u"":
++        output = u"OSStatus %s" % error
++
++    if exception_class is None:
++        exception_class = ssl.SSLError
++
++    raise exception_class(output)
++
++
++def _cert_array_from_pem(pem_bundle):
++    """
++    Given a bundle of certs in PEM format, turns them into a CFArray of certs
++    that can be used to validate a cert chain.
++    """
++    # Normalize the PEM bundle's line endings.
++    pem_bundle = pem_bundle.replace(b"\r\n", b"\n")
++
++    der_certs = [
++        base64.b64decode(match.group(1)) for match in _PEM_CERTS_RE.finditer(pem_bundle)
++    ]
++    if not der_certs:
++        raise ssl.SSLError("No root certificates specified")
++
++    cert_array = CoreFoundation.CFArrayCreateMutable(
++        CoreFoundation.kCFAllocatorDefault,
++        0,
++        ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
++    )
++    if not cert_array:
++        raise ssl.SSLError("Unable to allocate memory!")
++
++    try:
++        for der_bytes in der_certs:
++            certdata = _cf_data_from_bytes(der_bytes)
++            if not certdata:
++                raise ssl.SSLError("Unable to allocate memory!")
++            cert = Security.SecCertificateCreateWithData(
++                CoreFoundation.kCFAllocatorDefault, certdata
++            )
++            CoreFoundation.CFRelease(certdata)
++            if not cert:
++                raise ssl.SSLError("Unable to build cert object!")
++
++            CoreFoundation.CFArrayAppendValue(cert_array, cert)
++            CoreFoundation.CFRelease(cert)
++    except Exception:
++        # We need to free the array before the exception bubbles further.
++        # We only want to do that if an error occurs: otherwise, the caller
++        # should free.
++        CoreFoundation.CFRelease(cert_array)
++
++    return cert_array
++
++
++def _is_cert(item):
++    """
++    Returns True if a given CFTypeRef is a certificate.
++    """
++    expected = Security.SecCertificateGetTypeID()
++    return CoreFoundation.CFGetTypeID(item) == expected
++
++
++def _is_identity(item):
++    """
++    Returns True if a given CFTypeRef is an identity.
++    """
++    expected = Security.SecIdentityGetTypeID()
++    return CoreFoundation.CFGetTypeID(item) == expected
++
++
++def _temporary_keychain():
++    """
++    This function creates a temporary Mac keychain that we can use to work with
++    credentials. This keychain uses a one-time password and a temporary file to
++    store the data. We expect to have one keychain per socket. The returned
++    SecKeychainRef must be freed by the caller, including calling
++    SecKeychainDelete.
++
++    Returns a tuple of the SecKeychainRef and the path to the temporary
++    directory that contains it.
++    """
++    # Unfortunately, SecKeychainCreate requires a path to a keychain. This
++    # means we cannot use mkstemp to use a generic temporary file. Instead,
++    # we're going to create a temporary directory and a filename to use there.
++    # This filename will be 8 random bytes expanded into base64. We also need
++    # some random bytes to password-protect the keychain we're creating, so we
++    # ask for 40 random bytes.
++    random_bytes = os.urandom(40)
++    filename = base64.b16encode(random_bytes[:8]).decode("utf-8")
++    password = base64.b16encode(random_bytes[8:])  # Must be valid UTF-8
++    tempdirectory = tempfile.mkdtemp()
++
++    keychain_path = os.path.join(tempdirectory, filename).encode("utf-8")
++
++    # We now want to create the keychain itself.
++    keychain = Security.SecKeychainRef()
++    status = Security.SecKeychainCreate(
++        keychain_path, len(password), password, False, None, ctypes.byref(keychain)
++    )
++    _assert_no_error(status)
++
++    # Having created the keychain, we want to pass it off to the caller.
++    return keychain, tempdirectory
++
++
++def _load_items_from_file(keychain, path):
++    """
++    Given a single file, loads all the trust objects from it into arrays and
++    the keychain.
++    Returns a tuple of lists: the first list is a list of identities, the
++    second a list of certs.
++    """
++    certificates = []
++    identities = []
++    result_array = None
++
++    with open(path, "rb") as f:
++        raw_filedata = f.read()
++
++    try:
++        filedata = CoreFoundation.CFDataCreate(
++            CoreFoundation.kCFAllocatorDefault, raw_filedata, len(raw_filedata)
++        )
++        result_array = CoreFoundation.CFArrayRef()
++        result = Security.SecItemImport(
++            filedata,  # cert data
++            None,  # Filename, leaving it out for now
++            None,  # What the type of the file is, we don't care
++            None,  # what's in the file, we don't care
++            0,  # import flags
++            None,  # key params, can include passphrase in the future
++            keychain,  # The keychain to insert into
++            ctypes.byref(result_array),  # Results
++        )
++        _assert_no_error(result)
++
++        # A CFArray is not very useful to us as an intermediary
++        # representation, so we are going to extract the objects we want
++        # and then free the array. We don't need to keep hold of keys: the
++        # keychain already has them!
++        result_count = CoreFoundation.CFArrayGetCount(result_array)
++        for index in range(result_count):
++            item = CoreFoundation.CFArrayGetValueAtIndex(result_array, index)
++            item = ctypes.cast(item, CoreFoundation.CFTypeRef)
++
++            if _is_cert(item):
++                CoreFoundation.CFRetain(item)
++                certificates.append(item)
++            elif _is_identity(item):
++                CoreFoundation.CFRetain(item)
++                identities.append(item)
++    finally:
++        if result_array:
++            CoreFoundation.CFRelease(result_array)
++
++        CoreFoundation.CFRelease(filedata)
++
++    return (identities, certificates)
++
++
++def _load_client_cert_chain(keychain, *paths):
++    """
++    Load certificates and maybe keys from a number of files. Has the end goal
++    of returning a CFArray containing one SecIdentityRef, and then zero or more
++    SecCertificateRef objects, suitable for use as a client certificate trust
++    chain.
++    """
++    # Ok, the strategy.
++    #
++    # This relies on knowing that macOS will not give you a SecIdentityRef
++    # unless you have imported a key into a keychain. This is a somewhat
++    # artificial limitation of macOS (for example, it doesn't necessarily
++    # affect iOS), but there is nothing inside Security.framework that lets you
++    # get a SecIdentityRef without having a key in a keychain.
++    #
++    # So the policy here is we take all the files and iterate them in order.
++    # Each one will use SecItemImport to have one or more objects loaded from
++    # it. We will also point at a keychain that macOS can use to work with the
++    # private key.
++    #
++    # Once we have all the objects, we'll check what we actually have. If we
++    # already have a SecIdentityRef in hand, fab: we'll use that. Otherwise,
++    # we'll take the first certificate (which we assume to be our leaf) and
++    # ask the keychain to give us a SecIdentityRef with that cert's associated
++    # key.
++    #
++    # We'll then return a CFArray containing the trust chain: one
++    # SecIdentityRef and then zero-or-more SecCertificateRef objects. The
++    # responsibility for freeing this CFArray will be with the caller. This
++    # CFArray must remain alive for the entire connection, so in practice it
++    # will be stored with a single SSLSocket, along with the reference to the
++    # keychain.
++    certificates = []
++    identities = []
++
++    # Filter out bad paths.
++    paths = (path for path in paths if path)
++
++    try:
++        for file_path in paths:
++            new_identities, new_certs = _load_items_from_file(keychain, file_path)
++            identities.extend(new_identities)
++            certificates.extend(new_certs)
++
++        # Ok, we have everything. The question is: do we have an identity? If
++        # not, we want to grab one from the first cert we have.
++        if not identities:
++            new_identity = Security.SecIdentityRef()
++            status = Security.SecIdentityCreateWithCertificate(
++                keychain, certificates[0], ctypes.byref(new_identity)
++            )
++            _assert_no_error(status)
++            identities.append(new_identity)
++
++            # We now want to release the original certificate, as we no longer
++            # need it.
++            CoreFoundation.CFRelease(certificates.pop(0))
++
++        # We now need to build a new CFArray that holds the trust chain.
++        trust_chain = CoreFoundation.CFArrayCreateMutable(
++            CoreFoundation.kCFAllocatorDefault,
++            0,
++            ctypes.byref(CoreFoundation.kCFTypeArrayCallBacks),
++        )
++        for item in itertools.chain(identities, certificates):
++            # ArrayAppendValue does a CFRetain on the item. That's fine,
++            # because the finally block will release our other refs to them.
++            CoreFoundation.CFArrayAppendValue(trust_chain, item)
++
++        return trust_chain
++    finally:
++        for obj in itertools.chain(identities, certificates):
++            CoreFoundation.CFRelease(obj)
+Index: venv/Lib/site-packages/urllib3/contrib/_securetransport/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/contrib/_securetransport/__init__.py	(date 1573549849459)
++++ venv/Lib/site-packages/urllib3/contrib/_securetransport/__init__.py	(date 1573549849459)
+@@ -0,0 +1,0 @@
+Index: venv/Lib/site-packages/urllib3/packages/six.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/packages/six.py	(date 1573549849480)
++++ venv/Lib/site-packages/urllib3/packages/six.py	(date 1573549849480)
+@@ -0,0 +1,1021 @@
++# Copyright (c) 2010-2019 Benjamin Peterson
++#
++# Permission is hereby granted, free of charge, to any person obtaining a copy
++# of this software and associated documentation files (the "Software"), to deal
++# in the Software without restriction, including without limitation the rights
++# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
++# copies of the Software, and to permit persons to whom the Software is
++# furnished to do so, subject to the following conditions:
++#
++# The above copyright notice and this permission notice shall be included in all
++# copies or substantial portions of the Software.
++#
++# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
++# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
++# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
++# SOFTWARE.
++
++"""Utilities for writing code that runs on Python 2 and 3"""
++
++from __future__ import absolute_import
++
++import functools
++import itertools
++import operator
++import sys
++import types
++
++__author__ = "Benjamin Peterson <benjamin@python.org>"
++__version__ = "1.12.0"
++
++
++# Useful for very coarse version differentiation.
++PY2 = sys.version_info[0] == 2
++PY3 = sys.version_info[0] == 3
++PY34 = sys.version_info[0:2] >= (3, 4)
++
++if PY3:
++    string_types = (str,)
++    integer_types = (int,)
++    class_types = (type,)
++    text_type = str
++    binary_type = bytes
++
++    MAXSIZE = sys.maxsize
++else:
++    string_types = (basestring,)
++    integer_types = (int, long)
++    class_types = (type, types.ClassType)
++    text_type = unicode
++    binary_type = str
++
++    if sys.platform.startswith("java"):
++        # Jython always uses 32 bits.
++        MAXSIZE = int((1 << 31) - 1)
++    else:
++        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).
++        class X(object):
++            def __len__(self):
++                return 1 << 31
++
++        try:
++            len(X())
++        except OverflowError:
++            # 32-bit
++            MAXSIZE = int((1 << 31) - 1)
++        else:
++            # 64-bit
++            MAXSIZE = int((1 << 63) - 1)
++        del X
++
++
++def _add_doc(func, doc):
++    """Add documentation to a function."""
++    func.__doc__ = doc
++
++
++def _import_module(name):
++    """Import module, returning the module after the last dot."""
++    __import__(name)
++    return sys.modules[name]
++
++
++class _LazyDescr(object):
++    def __init__(self, name):
++        self.name = name
++
++    def __get__(self, obj, tp):
++        result = self._resolve()
++        setattr(obj, self.name, result)  # Invokes __set__.
++        try:
++            # This is a bit ugly, but it avoids running this again by
++            # removing this descriptor.
++            delattr(obj.__class__, self.name)
++        except AttributeError:
++            pass
++        return result
++
++
++class MovedModule(_LazyDescr):
++    def __init__(self, name, old, new=None):
++        super(MovedModule, self).__init__(name)
++        if PY3:
++            if new is None:
++                new = name
++            self.mod = new
++        else:
++            self.mod = old
++
++    def _resolve(self):
++        return _import_module(self.mod)
++
++    def __getattr__(self, attr):
++        _module = self._resolve()
++        value = getattr(_module, attr)
++        setattr(self, attr, value)
++        return value
++
++
++class _LazyModule(types.ModuleType):
++    def __init__(self, name):
++        super(_LazyModule, self).__init__(name)
++        self.__doc__ = self.__class__.__doc__
++
++    def __dir__(self):
++        attrs = ["__doc__", "__name__"]
++        attrs += [attr.name for attr in self._moved_attributes]
++        return attrs
++
++    # Subclasses should override this
++    _moved_attributes = []
++
++
++class MovedAttribute(_LazyDescr):
++    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):
++        super(MovedAttribute, self).__init__(name)
++        if PY3:
++            if new_mod is None:
++                new_mod = name
++            self.mod = new_mod
++            if new_attr is None:
++                if old_attr is None:
++                    new_attr = name
++                else:
++                    new_attr = old_attr
++            self.attr = new_attr
++        else:
++            self.mod = old_mod
++            if old_attr is None:
++                old_attr = name
++            self.attr = old_attr
++
++    def _resolve(self):
++        module = _import_module(self.mod)
++        return getattr(module, self.attr)
++
++
++class _SixMetaPathImporter(object):
++
++    """
++    A meta path importer to import six.moves and its submodules.
++
++    This class implements a PEP302 finder and loader. It should be compatible
++    with Python 2.5 and all existing versions of Python3
++    """
++
++    def __init__(self, six_module_name):
++        self.name = six_module_name
++        self.known_modules = {}
++
++    def _add_module(self, mod, *fullnames):
++        for fullname in fullnames:
++            self.known_modules[self.name + "." + fullname] = mod
++
++    def _get_module(self, fullname):
++        return self.known_modules[self.name + "." + fullname]
++
++    def find_module(self, fullname, path=None):
++        if fullname in self.known_modules:
++            return self
++        return None
++
++    def __get_module(self, fullname):
++        try:
++            return self.known_modules[fullname]
++        except KeyError:
++            raise ImportError("This loader does not know module " + fullname)
++
++    def load_module(self, fullname):
++        try:
++            # in case of a reload
++            return sys.modules[fullname]
++        except KeyError:
++            pass
++        mod = self.__get_module(fullname)
++        if isinstance(mod, MovedModule):
++            mod = mod._resolve()
++        else:
++            mod.__loader__ = self
++        sys.modules[fullname] = mod
++        return mod
++
++    def is_package(self, fullname):
++        """
++        Return true, if the named module is a package.
++
++        We need this method to get correct spec objects with
++        Python 3.4 (see PEP451)
++        """
++        return hasattr(self.__get_module(fullname), "__path__")
++
++    def get_code(self, fullname):
++        """Return None
++
++        Required, if is_package is implemented"""
++        self.__get_module(fullname)  # eventually raises ImportError
++        return None
++
++    get_source = get_code  # same as get_code
++
++
++_importer = _SixMetaPathImporter(__name__)
++
++
++class _MovedItems(_LazyModule):
++
++    """Lazy loading of moved objects"""
++
++    __path__ = []  # mark as package
++
++
++_moved_attributes = [
++    MovedAttribute("cStringIO", "cStringIO", "io", "StringIO"),
++    MovedAttribute("filter", "itertools", "builtins", "ifilter", "filter"),
++    MovedAttribute(
++        "filterfalse", "itertools", "itertools", "ifilterfalse", "filterfalse"
++    ),
++    MovedAttribute("input", "__builtin__", "builtins", "raw_input", "input"),
++    MovedAttribute("intern", "__builtin__", "sys"),
++    MovedAttribute("map", "itertools", "builtins", "imap", "map"),
++    MovedAttribute("getcwd", "os", "os", "getcwdu", "getcwd"),
++    MovedAttribute("getcwdb", "os", "os", "getcwd", "getcwdb"),
++    MovedAttribute("getoutput", "commands", "subprocess"),
++    MovedAttribute("range", "__builtin__", "builtins", "xrange", "range"),
++    MovedAttribute(
++        "reload_module", "__builtin__", "importlib" if PY34 else "imp", "reload"
++    ),
++    MovedAttribute("reduce", "__builtin__", "functools"),
++    MovedAttribute("shlex_quote", "pipes", "shlex", "quote"),
++    MovedAttribute("StringIO", "StringIO", "io"),
++    MovedAttribute("UserDict", "UserDict", "collections"),
++    MovedAttribute("UserList", "UserList", "collections"),
++    MovedAttribute("UserString", "UserString", "collections"),
++    MovedAttribute("xrange", "__builtin__", "builtins", "xrange", "range"),
++    MovedAttribute("zip", "itertools", "builtins", "izip", "zip"),
++    MovedAttribute(
++        "zip_longest", "itertools", "itertools", "izip_longest", "zip_longest"
++    ),
++    MovedModule("builtins", "__builtin__"),
++    MovedModule("configparser", "ConfigParser"),
++    MovedModule("copyreg", "copy_reg"),
++    MovedModule("dbm_gnu", "gdbm", "dbm.gnu"),
++    MovedModule("_dummy_thread", "dummy_thread", "_dummy_thread"),
++    MovedModule("http_cookiejar", "cookielib", "http.cookiejar"),
++    MovedModule("http_cookies", "Cookie", "http.cookies"),
++    MovedModule("html_entities", "htmlentitydefs", "html.entities"),
++    MovedModule("html_parser", "HTMLParser", "html.parser"),
++    MovedModule("http_client", "httplib", "http.client"),
++    MovedModule("email_mime_base", "email.MIMEBase", "email.mime.base"),
++    MovedModule("email_mime_image", "email.MIMEImage", "email.mime.image"),
++    MovedModule("email_mime_multipart", "email.MIMEMultipart", "email.mime.multipart"),
++    MovedModule(
++        "email_mime_nonmultipart", "email.MIMENonMultipart", "email.mime.nonmultipart"
++    ),
++    MovedModule("email_mime_text", "email.MIMEText", "email.mime.text"),
++    MovedModule("BaseHTTPServer", "BaseHTTPServer", "http.server"),
++    MovedModule("CGIHTTPServer", "CGIHTTPServer", "http.server"),
++    MovedModule("SimpleHTTPServer", "SimpleHTTPServer", "http.server"),
++    MovedModule("cPickle", "cPickle", "pickle"),
++    MovedModule("queue", "Queue"),
++    MovedModule("reprlib", "repr"),
++    MovedModule("socketserver", "SocketServer"),
++    MovedModule("_thread", "thread", "_thread"),
++    MovedModule("tkinter", "Tkinter"),
++    MovedModule("tkinter_dialog", "Dialog", "tkinter.dialog"),
++    MovedModule("tkinter_filedialog", "FileDialog", "tkinter.filedialog"),
++    MovedModule("tkinter_scrolledtext", "ScrolledText", "tkinter.scrolledtext"),
++    MovedModule("tkinter_simpledialog", "SimpleDialog", "tkinter.simpledialog"),
++    MovedModule("tkinter_tix", "Tix", "tkinter.tix"),
++    MovedModule("tkinter_ttk", "ttk", "tkinter.ttk"),
++    MovedModule("tkinter_constants", "Tkconstants", "tkinter.constants"),
++    MovedModule("tkinter_dnd", "Tkdnd", "tkinter.dnd"),
++    MovedModule("tkinter_colorchooser", "tkColorChooser", "tkinter.colorchooser"),
++    MovedModule("tkinter_commondialog", "tkCommonDialog", "tkinter.commondialog"),
++    MovedModule("tkinter_tkfiledialog", "tkFileDialog", "tkinter.filedialog"),
++    MovedModule("tkinter_font", "tkFont", "tkinter.font"),
++    MovedModule("tkinter_messagebox", "tkMessageBox", "tkinter.messagebox"),
++    MovedModule("tkinter_tksimpledialog", "tkSimpleDialog", "tkinter.simpledialog"),
++    MovedModule("urllib_parse", __name__ + ".moves.urllib_parse", "urllib.parse"),
++    MovedModule("urllib_error", __name__ + ".moves.urllib_error", "urllib.error"),
++    MovedModule("urllib", __name__ + ".moves.urllib", __name__ + ".moves.urllib"),
++    MovedModule("urllib_robotparser", "robotparser", "urllib.robotparser"),
++    MovedModule("xmlrpc_client", "xmlrpclib", "xmlrpc.client"),
++    MovedModule("xmlrpc_server", "SimpleXMLRPCServer", "xmlrpc.server"),
++]
++# Add windows specific modules.
++if sys.platform == "win32":
++    _moved_attributes += [MovedModule("winreg", "_winreg")]
++
++for attr in _moved_attributes:
++    setattr(_MovedItems, attr.name, attr)
++    if isinstance(attr, MovedModule):
++        _importer._add_module(attr, "moves." + attr.name)
++del attr
++
++_MovedItems._moved_attributes = _moved_attributes
++
++moves = _MovedItems(__name__ + ".moves")
++_importer._add_module(moves, "moves")
++
++
++class Module_six_moves_urllib_parse(_LazyModule):
++
++    """Lazy loading of moved objects in six.moves.urllib_parse"""
++
++
++_urllib_parse_moved_attributes = [
++    MovedAttribute("ParseResult", "urlparse", "urllib.parse"),
++    MovedAttribute("SplitResult", "urlparse", "urllib.parse"),
++    MovedAttribute("parse_qs", "urlparse", "urllib.parse"),
++    MovedAttribute("parse_qsl", "urlparse", "urllib.parse"),
++    MovedAttribute("urldefrag", "urlparse", "urllib.parse"),
++    MovedAttribute("urljoin", "urlparse", "urllib.parse"),
++    MovedAttribute("urlparse", "urlparse", "urllib.parse"),
++    MovedAttribute("urlsplit", "urlparse", "urllib.parse"),
++    MovedAttribute("urlunparse", "urlparse", "urllib.parse"),
++    MovedAttribute("urlunsplit", "urlparse", "urllib.parse"),
++    MovedAttribute("quote", "urllib", "urllib.parse"),
++    MovedAttribute("quote_plus", "urllib", "urllib.parse"),
++    MovedAttribute("unquote", "urllib", "urllib.parse"),
++    MovedAttribute("unquote_plus", "urllib", "urllib.parse"),
++    MovedAttribute(
++        "unquote_to_bytes", "urllib", "urllib.parse", "unquote", "unquote_to_bytes"
++    ),
++    MovedAttribute("urlencode", "urllib", "urllib.parse"),
++    MovedAttribute("splitquery", "urllib", "urllib.parse"),
++    MovedAttribute("splittag", "urllib", "urllib.parse"),
++    MovedAttribute("splituser", "urllib", "urllib.parse"),
++    MovedAttribute("splitvalue", "urllib", "urllib.parse"),
++    MovedAttribute("uses_fragment", "urlparse", "urllib.parse"),
++    MovedAttribute("uses_netloc", "urlparse", "urllib.parse"),
++    MovedAttribute("uses_params", "urlparse", "urllib.parse"),
++    MovedAttribute("uses_query", "urlparse", "urllib.parse"),
++    MovedAttribute("uses_relative", "urlparse", "urllib.parse"),
++]
++for attr in _urllib_parse_moved_attributes:
++    setattr(Module_six_moves_urllib_parse, attr.name, attr)
++del attr
++
++Module_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes
++
++_importer._add_module(
++    Module_six_moves_urllib_parse(__name__ + ".moves.urllib_parse"),
++    "moves.urllib_parse",
++    "moves.urllib.parse",
++)
++
++
++class Module_six_moves_urllib_error(_LazyModule):
++
++    """Lazy loading of moved objects in six.moves.urllib_error"""
++
++
++_urllib_error_moved_attributes = [
++    MovedAttribute("URLError", "urllib2", "urllib.error"),
++    MovedAttribute("HTTPError", "urllib2", "urllib.error"),
++    MovedAttribute("ContentTooShortError", "urllib", "urllib.error"),
++]
++for attr in _urllib_error_moved_attributes:
++    setattr(Module_six_moves_urllib_error, attr.name, attr)
++del attr
++
++Module_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes
++
++_importer._add_module(
++    Module_six_moves_urllib_error(__name__ + ".moves.urllib.error"),
++    "moves.urllib_error",
++    "moves.urllib.error",
++)
++
++
++class Module_six_moves_urllib_request(_LazyModule):
++
++    """Lazy loading of moved objects in six.moves.urllib_request"""
++
++
++_urllib_request_moved_attributes = [
++    MovedAttribute("urlopen", "urllib2", "urllib.request"),
++    MovedAttribute("install_opener", "urllib2", "urllib.request"),
++    MovedAttribute("build_opener", "urllib2", "urllib.request"),
++    MovedAttribute("pathname2url", "urllib", "urllib.request"),
++    MovedAttribute("url2pathname", "urllib", "urllib.request"),
++    MovedAttribute("getproxies", "urllib", "urllib.request"),
++    MovedAttribute("Request", "urllib2", "urllib.request"),
++    MovedAttribute("OpenerDirector", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPDefaultErrorHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPRedirectHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPCookieProcessor", "urllib2", "urllib.request"),
++    MovedAttribute("ProxyHandler", "urllib2", "urllib.request"),
++    MovedAttribute("BaseHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPPasswordMgr", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPPasswordMgrWithDefaultRealm", "urllib2", "urllib.request"),
++    MovedAttribute("AbstractBasicAuthHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPBasicAuthHandler", "urllib2", "urllib.request"),
++    MovedAttribute("ProxyBasicAuthHandler", "urllib2", "urllib.request"),
++    MovedAttribute("AbstractDigestAuthHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPDigestAuthHandler", "urllib2", "urllib.request"),
++    MovedAttribute("ProxyDigestAuthHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPSHandler", "urllib2", "urllib.request"),
++    MovedAttribute("FileHandler", "urllib2", "urllib.request"),
++    MovedAttribute("FTPHandler", "urllib2", "urllib.request"),
++    MovedAttribute("CacheFTPHandler", "urllib2", "urllib.request"),
++    MovedAttribute("UnknownHandler", "urllib2", "urllib.request"),
++    MovedAttribute("HTTPErrorProcessor", "urllib2", "urllib.request"),
++    MovedAttribute("urlretrieve", "urllib", "urllib.request"),
++    MovedAttribute("urlcleanup", "urllib", "urllib.request"),
++    MovedAttribute("URLopener", "urllib", "urllib.request"),
++    MovedAttribute("FancyURLopener", "urllib", "urllib.request"),
++    MovedAttribute("proxy_bypass", "urllib", "urllib.request"),
++    MovedAttribute("parse_http_list", "urllib2", "urllib.request"),
++    MovedAttribute("parse_keqv_list", "urllib2", "urllib.request"),
++]
++for attr in _urllib_request_moved_attributes:
++    setattr(Module_six_moves_urllib_request, attr.name, attr)
++del attr
++
++Module_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes
++
++_importer._add_module(
++    Module_six_moves_urllib_request(__name__ + ".moves.urllib.request"),
++    "moves.urllib_request",
++    "moves.urllib.request",
++)
++
++
++class Module_six_moves_urllib_response(_LazyModule):
++
++    """Lazy loading of moved objects in six.moves.urllib_response"""
++
++
++_urllib_response_moved_attributes = [
++    MovedAttribute("addbase", "urllib", "urllib.response"),
++    MovedAttribute("addclosehook", "urllib", "urllib.response"),
++    MovedAttribute("addinfo", "urllib", "urllib.response"),
++    MovedAttribute("addinfourl", "urllib", "urllib.response"),
++]
++for attr in _urllib_response_moved_attributes:
++    setattr(Module_six_moves_urllib_response, attr.name, attr)
++del attr
++
++Module_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes
++
++_importer._add_module(
++    Module_six_moves_urllib_response(__name__ + ".moves.urllib.response"),
++    "moves.urllib_response",
++    "moves.urllib.response",
++)
++
++
++class Module_six_moves_urllib_robotparser(_LazyModule):
++
++    """Lazy loading of moved objects in six.moves.urllib_robotparser"""
++
++
++_urllib_robotparser_moved_attributes = [
++    MovedAttribute("RobotFileParser", "robotparser", "urllib.robotparser")
++]
++for attr in _urllib_robotparser_moved_attributes:
++    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)
++del attr
++
++Module_six_moves_urllib_robotparser._moved_attributes = (
++    _urllib_robotparser_moved_attributes
++)
++
++_importer._add_module(
++    Module_six_moves_urllib_robotparser(__name__ + ".moves.urllib.robotparser"),
++    "moves.urllib_robotparser",
++    "moves.urllib.robotparser",
++)
++
++
++class Module_six_moves_urllib(types.ModuleType):
++
++    """Create a six.moves.urllib namespace that resembles the Python 3 namespace"""
++
++    __path__ = []  # mark as package
++    parse = _importer._get_module("moves.urllib_parse")
++    error = _importer._get_module("moves.urllib_error")
++    request = _importer._get_module("moves.urllib_request")
++    response = _importer._get_module("moves.urllib_response")
++    robotparser = _importer._get_module("moves.urllib_robotparser")
++
++    def __dir__(self):
++        return ["parse", "error", "request", "response", "robotparser"]
++
++
++_importer._add_module(
++    Module_six_moves_urllib(__name__ + ".moves.urllib"), "moves.urllib"
++)
++
++
++def add_move(move):
++    """Add an item to six.moves."""
++    setattr(_MovedItems, move.name, move)
++
++
++def remove_move(name):
++    """Remove item from six.moves."""
++    try:
++        delattr(_MovedItems, name)
++    except AttributeError:
++        try:
++            del moves.__dict__[name]
++        except KeyError:
++            raise AttributeError("no such move, %r" % (name,))
++
++
++if PY3:
++    _meth_func = "__func__"
++    _meth_self = "__self__"
++
++    _func_closure = "__closure__"
++    _func_code = "__code__"
++    _func_defaults = "__defaults__"
++    _func_globals = "__globals__"
++else:
++    _meth_func = "im_func"
++    _meth_self = "im_self"
++
++    _func_closure = "func_closure"
++    _func_code = "func_code"
++    _func_defaults = "func_defaults"
++    _func_globals = "func_globals"
++
++
++try:
++    advance_iterator = next
++except NameError:
++
++    def advance_iterator(it):
++        return it.next()
++
++
++next = advance_iterator
++
++
++try:
++    callable = callable
++except NameError:
++
++    def callable(obj):
++        return any("__call__" in klass.__dict__ for klass in type(obj).__mro__)
++
++
++if PY3:
++
++    def get_unbound_function(unbound):
++        return unbound
++
++    create_bound_method = types.MethodType
++
++    def create_unbound_method(func, cls):
++        return func
++
++    Iterator = object
++else:
++
++    def get_unbound_function(unbound):
++        return unbound.im_func
++
++    def create_bound_method(func, obj):
++        return types.MethodType(func, obj, obj.__class__)
++
++    def create_unbound_method(func, cls):
++        return types.MethodType(func, None, cls)
++
++    class Iterator(object):
++        def next(self):
++            return type(self).__next__(self)
++
++    callable = callable
++_add_doc(
++    get_unbound_function, """Get the function out of a possibly unbound function"""
++)
++
++
++get_method_function = operator.attrgetter(_meth_func)
++get_method_self = operator.attrgetter(_meth_self)
++get_function_closure = operator.attrgetter(_func_closure)
++get_function_code = operator.attrgetter(_func_code)
++get_function_defaults = operator.attrgetter(_func_defaults)
++get_function_globals = operator.attrgetter(_func_globals)
++
++
++if PY3:
++
++    def iterkeys(d, **kw):
++        return iter(d.keys(**kw))
++
++    def itervalues(d, **kw):
++        return iter(d.values(**kw))
++
++    def iteritems(d, **kw):
++        return iter(d.items(**kw))
++
++    def iterlists(d, **kw):
++        return iter(d.lists(**kw))
++
++    viewkeys = operator.methodcaller("keys")
++
++    viewvalues = operator.methodcaller("values")
++
++    viewitems = operator.methodcaller("items")
++else:
++
++    def iterkeys(d, **kw):
++        return d.iterkeys(**kw)
++
++    def itervalues(d, **kw):
++        return d.itervalues(**kw)
++
++    def iteritems(d, **kw):
++        return d.iteritems(**kw)
++
++    def iterlists(d, **kw):
++        return d.iterlists(**kw)
++
++    viewkeys = operator.methodcaller("viewkeys")
++
++    viewvalues = operator.methodcaller("viewvalues")
++
++    viewitems = operator.methodcaller("viewitems")
++
++_add_doc(iterkeys, "Return an iterator over the keys of a dictionary.")
++_add_doc(itervalues, "Return an iterator over the values of a dictionary.")
++_add_doc(iteritems, "Return an iterator over the (key, value) pairs of a dictionary.")
++_add_doc(
++    iterlists, "Return an iterator over the (key, [values]) pairs of a dictionary."
++)
++
++
++if PY3:
++
++    def b(s):
++        return s.encode("latin-1")
++
++    def u(s):
++        return s
++
++    unichr = chr
++    import struct
++
++    int2byte = struct.Struct(">B").pack
++    del struct
++    byte2int = operator.itemgetter(0)
++    indexbytes = operator.getitem
++    iterbytes = iter
++    import io
++
++    StringIO = io.StringIO
++    BytesIO = io.BytesIO
++    del io
++    _assertCountEqual = "assertCountEqual"
++    if sys.version_info[1] <= 1:
++        _assertRaisesRegex = "assertRaisesRegexp"
++        _assertRegex = "assertRegexpMatches"
++    else:
++        _assertRaisesRegex = "assertRaisesRegex"
++        _assertRegex = "assertRegex"
++else:
++
++    def b(s):
++        return s
++
++    # Workaround for standalone backslash
++
++    def u(s):
++        return unicode(s.replace(r"\\", r"\\\\"), "unicode_escape")
++
++    unichr = unichr
++    int2byte = chr
++
++    def byte2int(bs):
++        return ord(bs[0])
++
++    def indexbytes(buf, i):
++        return ord(buf[i])
++
++    iterbytes = functools.partial(itertools.imap, ord)
++    import StringIO
++
++    StringIO = BytesIO = StringIO.StringIO
++    _assertCountEqual = "assertItemsEqual"
++    _assertRaisesRegex = "assertRaisesRegexp"
++    _assertRegex = "assertRegexpMatches"
++_add_doc(b, """Byte literal""")
++_add_doc(u, """Text literal""")
++
++
++def assertCountEqual(self, *args, **kwargs):
++    return getattr(self, _assertCountEqual)(*args, **kwargs)
++
++
++def assertRaisesRegex(self, *args, **kwargs):
++    return getattr(self, _assertRaisesRegex)(*args, **kwargs)
++
++
++def assertRegex(self, *args, **kwargs):
++    return getattr(self, _assertRegex)(*args, **kwargs)
++
++
++if PY3:
++    exec_ = getattr(moves.builtins, "exec")
++
++    def reraise(tp, value, tb=None):
++        try:
++            if value is None:
++                value = tp()
++            if value.__traceback__ is not tb:
++                raise value.with_traceback(tb)
++            raise value
++        finally:
++            value = None
++            tb = None
++
++
++else:
++
++    def exec_(_code_, _globs_=None, _locs_=None):
++        """Execute code in a namespace."""
++        if _globs_ is None:
++            frame = sys._getframe(1)
++            _globs_ = frame.f_globals
++            if _locs_ is None:
++                _locs_ = frame.f_locals
++            del frame
++        elif _locs_ is None:
++            _locs_ = _globs_
++        exec("""exec _code_ in _globs_, _locs_""")
++
++    exec_(
++        """def reraise(tp, value, tb=None):
++    try:
++        raise tp, value, tb
++    finally:
++        tb = None
++"""
++    )
++
++
++if sys.version_info[:2] == (3, 2):
++    exec_(
++        """def raise_from(value, from_value):
++    try:
++        if from_value is None:
++            raise value
++        raise value from from_value
++    finally:
++        value = None
++"""
++    )
++elif sys.version_info[:2] > (3, 2):
++    exec_(
++        """def raise_from(value, from_value):
++    try:
++        raise value from from_value
++    finally:
++        value = None
++"""
++    )
++else:
++
++    def raise_from(value, from_value):
++        raise value
++
++
++print_ = getattr(moves.builtins, "print", None)
++if print_ is None:
++
++    def print_(*args, **kwargs):
++        """The new-style print function for Python 2.4 and 2.5."""
++        fp = kwargs.pop("file", sys.stdout)
++        if fp is None:
++            return
++
++        def write(data):
++            if not isinstance(data, basestring):
++                data = str(data)
++            # If the file has an encoding, encode unicode with it.
++            if (
++                isinstance(fp, file)
++                and isinstance(data, unicode)
++                and fp.encoding is not None
++            ):
++                errors = getattr(fp, "errors", None)
++                if errors is None:
++                    errors = "strict"
++                data = data.encode(fp.encoding, errors)
++            fp.write(data)
++
++        want_unicode = False
++        sep = kwargs.pop("sep", None)
++        if sep is not None:
++            if isinstance(sep, unicode):
++                want_unicode = True
++            elif not isinstance(sep, str):
++                raise TypeError("sep must be None or a string")
++        end = kwargs.pop("end", None)
++        if end is not None:
++            if isinstance(end, unicode):
++                want_unicode = True
++            elif not isinstance(end, str):
++                raise TypeError("end must be None or a string")
++        if kwargs:
++            raise TypeError("invalid keyword arguments to print()")
++        if not want_unicode:
++            for arg in args:
++                if isinstance(arg, unicode):
++                    want_unicode = True
++                    break
++        if want_unicode:
++            newline = unicode("\n")
++            space = unicode(" ")
++        else:
++            newline = "\n"
++            space = " "
++        if sep is None:
++            sep = space
++        if end is None:
++            end = newline
++        for i, arg in enumerate(args):
++            if i:
++                write(sep)
++            write(arg)
++        write(end)
++
++
++if sys.version_info[:2] < (3, 3):
++    _print = print_
++
++    def print_(*args, **kwargs):
++        fp = kwargs.get("file", sys.stdout)
++        flush = kwargs.pop("flush", False)
++        _print(*args, **kwargs)
++        if flush and fp is not None:
++            fp.flush()
++
++
++_add_doc(reraise, """Reraise an exception.""")
++
++if sys.version_info[0:2] < (3, 4):
++
++    def wraps(
++        wrapped,
++        assigned=functools.WRAPPER_ASSIGNMENTS,
++        updated=functools.WRAPPER_UPDATES,
++    ):
++        def wrapper(f):
++            f = functools.wraps(wrapped, assigned, updated)(f)
++            f.__wrapped__ = wrapped
++            return f
++
++        return wrapper
++
++
++else:
++    wraps = functools.wraps
++
++
++def with_metaclass(meta, *bases):
++    """Create a base class with a metaclass."""
++    # This requires a bit of explanation: the basic idea is to make a dummy
++    # metaclass for one level of class instantiation that replaces itself with
++    # the actual metaclass.
++    class metaclass(type):
++        def __new__(cls, name, this_bases, d):
++            return meta(name, bases, d)
++
++        @classmethod
++        def __prepare__(cls, name, this_bases):
++            return meta.__prepare__(name, bases)
++
++    return type.__new__(metaclass, "temporary_class", (), {})
++
++
++def add_metaclass(metaclass):
++    """Class decorator for creating a class with a metaclass."""
++
++    def wrapper(cls):
++        orig_vars = cls.__dict__.copy()
++        slots = orig_vars.get("__slots__")
++        if slots is not None:
++            if isinstance(slots, str):
++                slots = [slots]
++            for slots_var in slots:
++                orig_vars.pop(slots_var)
++        orig_vars.pop("__dict__", None)
++        orig_vars.pop("__weakref__", None)
++        if hasattr(cls, "__qualname__"):
++            orig_vars["__qualname__"] = cls.__qualname__
++        return metaclass(cls.__name__, cls.__bases__, orig_vars)
++
++    return wrapper
++
++
++def ensure_binary(s, encoding="utf-8", errors="strict"):
++    """Coerce **s** to six.binary_type.
++
++    For Python 2:
++      - `unicode` -> encoded to `str`
++      - `str` -> `str`
++
++    For Python 3:
++      - `str` -> encoded to `bytes`
++      - `bytes` -> `bytes`
++    """
++    if isinstance(s, text_type):
++        return s.encode(encoding, errors)
++    elif isinstance(s, binary_type):
++        return s
++    else:
++        raise TypeError("not expecting type '%s'" % type(s))
++
++
++def ensure_str(s, encoding="utf-8", errors="strict"):
++    """Coerce *s* to `str`.
++
++    For Python 2:
++      - `unicode` -> encoded to `str`
++      - `str` -> `str`
++
++    For Python 3:
++      - `str` -> `str`
++      - `bytes` -> decoded to `str`
++    """
++    if not isinstance(s, (text_type, binary_type)):
++        raise TypeError("not expecting type '%s'" % type(s))
++    if PY2 and isinstance(s, text_type):
++        s = s.encode(encoding, errors)
++    elif PY3 and isinstance(s, binary_type):
++        s = s.decode(encoding, errors)
++    return s
++
++
++def ensure_text(s, encoding="utf-8", errors="strict"):
++    """Coerce *s* to six.text_type.
++
++    For Python 2:
++      - `unicode` -> `unicode`
++      - `str` -> `unicode`
++
++    For Python 3:
++      - `str` -> `str`
++      - `bytes` -> decoded to `str`
++    """
++    if isinstance(s, binary_type):
++        return s.decode(encoding, errors)
++    elif isinstance(s, text_type):
++        return s
++    else:
++        raise TypeError("not expecting type '%s'" % type(s))
++
++
++def python_2_unicode_compatible(klass):
++    """
++    A decorator that defines __unicode__ and __str__ methods under Python 2.
++    Under Python 3 it does nothing.
++
++    To support Python 2 and 3 with a single code base, define a __str__ method
++    returning text and apply this decorator to the class.
++    """
++    if PY2:
++        if "__str__" not in klass.__dict__:
++            raise ValueError(
++                "@python_2_unicode_compatible cannot be applied "
++                "to %s because it doesn't define __str__()." % klass.__name__
++            )
++        klass.__unicode__ = klass.__str__
++        klass.__str__ = lambda self: self.__unicode__().encode("utf-8")
++    return klass
++
++
++# Complete the moves implementation.
++# This code is at the end of this module to speed up module loading.
++# Turn this module into a package.
++__path__ = []  # required for PEP 302 and PEP 451
++__package__ = __name__  # see PEP 366 @ReservedAssignment
++if globals().get("__spec__") is not None:
++    __spec__.submodule_search_locations = []  # PEP 451 @UndefinedVariable
++# Remove other six meta path importers, since they cause problems. This can
++# happen if six is removed from sys.modules and then reloaded. (Setuptools does
++# this for some reason.)
++if sys.meta_path:
++    for i, importer in enumerate(sys.meta_path):
++        # Here's some real nastiness: Another "instance" of the six module might
++        # be floating around. Therefore, we can't use isinstance() to check for
++        # the six meta path importer, since the other six instance will have
++        # inserted an importer with different class.
++        if (
++            type(importer).__name__ == "_SixMetaPathImporter"
++            and importer.name == __name__
++        ):
++            del sys.meta_path[i]
++            break
++    del i, importer
++# Finally, add the importer to the meta path import hook.
++sys.meta_path.append(_importer)
+Index: venv/Lib/site-packages/urllib3/packages/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/packages/__init__.py	(date 1573549849475)
++++ venv/Lib/site-packages/urllib3/packages/__init__.py	(date 1573549849475)
+@@ -0,0 +1,5 @@
++from __future__ import absolute_import
++
++from . import ssl_match_hostname
++
++__all__ = ("ssl_match_hostname",)
+Index: venv/Lib/site-packages/urllib3/packages/backports/makefile.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/packages/backports/makefile.py	(date 1573549849490)
++++ venv/Lib/site-packages/urllib3/packages/backports/makefile.py	(date 1573549849490)
+@@ -0,0 +1,52 @@
++# -*- coding: utf-8 -*-
++"""
++backports.makefile
++~~~~~~~~~~~~~~~~~~
++
++Backports the Python 3 ``socket.makefile`` method for use with anything that
++wants to create a "fake" socket object.
++"""
++import io
++
++from socket import SocketIO
++
++
++def backport_makefile(
++    self, mode="r", buffering=None, encoding=None, errors=None, newline=None
++):
++    """
++    Backport of ``socket.makefile`` from Python 3.5.
++    """
++    if not set(mode) <= {"r", "w", "b"}:
++        raise ValueError("invalid mode %r (only r, w, b allowed)" % (mode,))
++    writing = "w" in mode
++    reading = "r" in mode or not writing
++    assert reading or writing
++    binary = "b" in mode
++    rawmode = ""
++    if reading:
++        rawmode += "r"
++    if writing:
++        rawmode += "w"
++    raw = SocketIO(self, rawmode)
++    self._makefile_refs += 1
++    if buffering is None:
++        buffering = -1
++    if buffering < 0:
++        buffering = io.DEFAULT_BUFFER_SIZE
++    if buffering == 0:
++        if not binary:
++            raise ValueError("unbuffered streams must be binary")
++        return raw
++    if reading and writing:
++        buffer = io.BufferedRWPair(raw, raw, buffering)
++    elif reading:
++        buffer = io.BufferedReader(raw, buffering)
++    else:
++        assert writing
++        buffer = io.BufferedWriter(raw, buffering)
++    if binary:
++        return buffer
++    text = io.TextIOWrapper(buffer, encoding, errors, newline)
++    text.mode = mode
++    return text
+Index: venv/Lib/site-packages/urllib3/packages/backports/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/packages/backports/__init__.py	(date 1573549849485)
++++ venv/Lib/site-packages/urllib3/packages/backports/__init__.py	(date 1573549849485)
+@@ -0,0 +1,0 @@
+Index: venv/Lib/site-packages/urllib3/packages/ssl_match_hostname/_implementation.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/packages/ssl_match_hostname/_implementation.py	(date 1573549849503)
++++ venv/Lib/site-packages/urllib3/packages/ssl_match_hostname/_implementation.py	(date 1573549849503)
+@@ -0,0 +1,160 @@
++"""The match_hostname() function from Python 3.3.3, essential when using SSL."""
++
++# Note: This file is under the PSF license as the code comes from the python
++# stdlib.   http://docs.python.org/3/license.html
++
++import re
++import sys
++
++# ipaddress has been backported to 2.6+ in pypi.  If it is installed on the
++# system, use it to handle IPAddress ServerAltnames (this was added in
++# python-3.5) otherwise only do DNS matching.  This allows
++# backports.ssl_match_hostname to continue to be used in Python 2.7.
++try:
++    import ipaddress
++except ImportError:
++    ipaddress = None
++
++__version__ = "3.5.0.1"
++
++
++class CertificateError(ValueError):
++    pass
++
++
++def _dnsname_match(dn, hostname, max_wildcards=1):
++    """Matching according to RFC 6125, section 6.4.3
++
++    http://tools.ietf.org/html/rfc6125#section-6.4.3
++    """
++    pats = []
++    if not dn:
++        return False
++
++    # Ported from python3-syntax:
++    # leftmost, *remainder = dn.split(r'.')
++    parts = dn.split(r".")
++    leftmost = parts[0]
++    remainder = parts[1:]
++
++    wildcards = leftmost.count("*")
++    if wildcards > max_wildcards:
++        # Issue #17980: avoid denials of service by refusing more
++        # than one wildcard per fragment.  A survey of established
++        # policy among SSL implementations showed it to be a
++        # reasonable choice.
++        raise CertificateError(
++            "too many wildcards in certificate DNS name: " + repr(dn)
++        )
++
++    # speed up common case w/o wildcards
++    if not wildcards:
++        return dn.lower() == hostname.lower()
++
++    # RFC 6125, section 6.4.3, subitem 1.
++    # The client SHOULD NOT attempt to match a presented identifier in which
++    # the wildcard character comprises a label other than the left-most label.
++    if leftmost == "*":
++        # When '*' is a fragment by itself, it matches a non-empty dotless
++        # fragment.
++        pats.append("[^.]+")
++    elif leftmost.startswith("xn--") or hostname.startswith("xn--"):
++        # RFC 6125, section 6.4.3, subitem 3.
++        # The client SHOULD NOT attempt to match a presented identifier
++        # where the wildcard character is embedded within an A-label or
++        # U-label of an internationalized domain name.
++        pats.append(re.escape(leftmost))
++    else:
++        # Otherwise, '*' matches any dotless string, e.g. www*
++        pats.append(re.escape(leftmost).replace(r"\*", "[^.]*"))
++
++    # add the remaining fragments, ignore any wildcards
++    for frag in remainder:
++        pats.append(re.escape(frag))
++
++    pat = re.compile(r"\A" + r"\.".join(pats) + r"\Z", re.IGNORECASE)
++    return pat.match(hostname)
++
++
++def _to_unicode(obj):
++    if isinstance(obj, str) and sys.version_info < (3,):
++        obj = unicode(obj, encoding="ascii", errors="strict")
++    return obj
++
++
++def _ipaddress_match(ipname, host_ip):
++    """Exact matching of IP addresses.
++
++    RFC 6125 explicitly doesn't define an algorithm for this
++    (section 1.7.2 - "Out of Scope").
++    """
++    # OpenSSL may add a trailing newline to a subjectAltName's IP address
++    # Divergence from upstream: ipaddress can't handle byte str
++    ip = ipaddress.ip_address(_to_unicode(ipname).rstrip())
++    return ip == host_ip
++
++
++def match_hostname(cert, hostname):
++    """Verify that *cert* (in decoded format as returned by
++    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125
++    rules are followed, but IP addresses are not accepted for *hostname*.
++
++    CertificateError is raised on failure. On success, the function
++    returns nothing.
++    """
++    if not cert:
++        raise ValueError(
++            "empty or no certificate, match_hostname needs a "
++            "SSL socket or SSL context with either "
++            "CERT_OPTIONAL or CERT_REQUIRED"
++        )
++    try:
++        # Divergence from upstream: ipaddress can't handle byte str
++        host_ip = ipaddress.ip_address(_to_unicode(hostname))
++    except ValueError:
++        # Not an IP address (common case)
++        host_ip = None
++    except UnicodeError:
++        # Divergence from upstream: Have to deal with ipaddress not taking
++        # byte strings.  addresses should be all ascii, so we consider it not
++        # an ipaddress in this case
++        host_ip = None
++    except AttributeError:
++        # Divergence from upstream: Make ipaddress library optional
++        if ipaddress is None:
++            host_ip = None
++        else:
++            raise
++    dnsnames = []
++    san = cert.get("subjectAltName", ())
++    for key, value in san:
++        if key == "DNS":
++            if host_ip is None and _dnsname_match(value, hostname):
++                return
++            dnsnames.append(value)
++        elif key == "IP Address":
++            if host_ip is not None and _ipaddress_match(value, host_ip):
++                return
++            dnsnames.append(value)
++    if not dnsnames:
++        # The subject is only checked when there is no dNSName entry
++        # in subjectAltName
++        for sub in cert.get("subject", ()):
++            for key, value in sub:
++                # XXX according to RFC 2818, the most specific Common Name
++                # must be used.
++                if key == "commonName":
++                    if _dnsname_match(value, hostname):
++                        return
++                    dnsnames.append(value)
++    if len(dnsnames) > 1:
++        raise CertificateError(
++            "hostname %r "
++            "doesn't match either of %s" % (hostname, ", ".join(map(repr, dnsnames)))
++        )
++    elif len(dnsnames) == 1:
++        raise CertificateError("hostname %r doesn't match %r" % (hostname, dnsnames[0]))
++    else:
++        raise CertificateError(
++            "no appropriate commonName or subjectAltName fields were found"
++        )
+Index: venv/Lib/site-packages/urllib3/packages/ssl_match_hostname/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3/packages/ssl_match_hostname/__init__.py	(date 1573549849498)
++++ venv/Lib/site-packages/urllib3/packages/ssl_match_hostname/__init__.py	(date 1573549849498)
+@@ -0,0 +1,19 @@
++import sys
++
++try:
++    # Our match_hostname function is the same as 3.5's, so we only want to
++    # import the match_hostname function if it's at least that good.
++    if sys.version_info < (3, 5):
++        raise ImportError("Fallback to vendored code")
++
++    from ssl import CertificateError, match_hostname
++except ImportError:
++    try:
++        # Backport of the function from a pypi module
++        from backports.ssl_match_hostname import CertificateError, match_hostname
++    except ImportError:
++        # Our vendored copy
++        from ._implementation import CertificateError, match_hostname
++
++# Not needed, but documenting what we provide.
++__all__ = ("CertificateError", "match_hostname")
+Index: venv/Lib/site-packages/urllib3-1.25.7.dist-info/WHEEL
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3-1.25.7.dist-info/WHEEL	(date 1573549849572)
++++ venv/Lib/site-packages/urllib3-1.25.7.dist-info/WHEEL	(date 1573549849572)
+@@ -0,0 +1,6 @@
++Wheel-Version: 1.0
++Generator: bdist_wheel (0.33.6)
++Root-Is-Purelib: true
++Tag: py2-none-any
++Tag: py3-none-any
++
+Index: venv/Lib/site-packages/urllib3-1.25.7.dist-info/top_level.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3-1.25.7.dist-info/top_level.txt	(date 1573549849578)
++++ venv/Lib/site-packages/urllib3-1.25.7.dist-info/top_level.txt	(date 1573549849578)
+@@ -0,0 +1,1 @@
++urllib3
+Index: venv/Lib/site-packages/urllib3-1.25.7.dist-info/LICENSE.txt
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3-1.25.7.dist-info/LICENSE.txt	(date 1573549849562)
++++ venv/Lib/site-packages/urllib3-1.25.7.dist-info/LICENSE.txt	(date 1573549849562)
+@@ -0,0 +1,21 @@
++MIT License
++
++Copyright (c) 2008-2019 Andrey Petrov and contributors (see CONTRIBUTORS.txt)
++
++Permission is hereby granted, free of charge, to any person obtaining a copy
++of this software and associated documentation files (the "Software"), to deal
++in the Software without restriction, including without limitation the rights
++to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
++copies of the Software, and to permit persons to whom the Software is
++furnished to do so, subject to the following conditions:
++
++The above copyright notice and this permission notice shall be included in all
++copies or substantial portions of the Software.
++
++THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
++IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
++FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
++AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
++LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
++OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
++SOFTWARE.
+Index: venv/Lib/site-packages/urllib3-1.25.7.dist-info/RECORD
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3-1.25.7.dist-info/RECORD	(date 1573549850879)
++++ venv/Lib/site-packages/urllib3-1.25.7.dist-info/RECORD	(date 1573549850879)
+@@ -0,0 +1,78 @@
++urllib3/__init__.py,sha256=--dxP-3k5qC8gGCQJbU_jJK666_rbCduadrwRB25wZg,2683
++urllib3/_collections.py,sha256=GouVsNzwg6jADZTmimMI6oqmwKSswnMo9dh5tGNVWO4,10792
++urllib3/connection.py,sha256=JaGozqRdvNogTwHDGxbp2N3Hi2MtJQrkbr7b5qcBGXk,15168
++urllib3/connectionpool.py,sha256=2RPMZJU_PhkAbY1tvy3-W_9os4Kdk_XXu8Zi6YSCgSU,36488
++urllib3/exceptions.py,sha256=P3e-p9_LScyIxX7FoR3wU0A6hZmDqFAVCz2wgI3D0lM,6607
++urllib3/fields.py,sha256=kroD76QK-GdHHW7f_AUN4XxDC3OQPI2FFrS9eSL4BCs,8553
++urllib3/filepost.py,sha256=vj0qbrpT1AFzvvW4SuC8M5kJiw7wftHcSr-7b8UpPpw,2440
++urllib3/poolmanager.py,sha256=JYUyBUN3IiEknUdjZ7VJrpCQr6SP7vi0WwSndrn8XpE,17053
++urllib3/request.py,sha256=hhoHvEEatyd9Tn5EbGjQ0emn-ENMCyY591yNWTneINA,6018
++urllib3/response.py,sha256=O2DVzBeWOzyxZDZ8k0EDFU3GW1jWXk_b03mS0O1ybxs,27836
++urllib3/contrib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
++urllib3/contrib/_appengine_environ.py,sha256=PCxFG7RoB-AOkIWQWGBIg1yZnK0dwPxWcNx7BTpZFBI,909
++urllib3/contrib/appengine.py,sha256=9RyUW5vKy4VPa2imtwBNWYKILrypr-K6UXEHUYsf0JY,11010
++urllib3/contrib/ntlmpool.py,sha256=a402AwGN_Ll3N-4ur_AS6UrU-ycUtlnYqoBF76lORg8,4160
++urllib3/contrib/pyopenssl.py,sha256=Lp95UgpAQsI5-zTGy42-oxU_2n6XyneC7gNHw3qx0Cw,16423
++urllib3/contrib/securetransport.py,sha256=iKzVUAxKnChsADR5YMwc05oEixXDzAk0xPU0g-rc2z8,32275
++urllib3/contrib/socks.py,sha256=nzDMgDIFJWVubKHqvIn2-SKCO91hhJInP92WgHChGzA,7036
++urllib3/contrib/_securetransport/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
++urllib3/contrib/_securetransport/bindings.py,sha256=mullWYFaghBdRWla6HYU-TBgFRTPLBEfxj3jplbeJmQ,16886
++urllib3/contrib/_securetransport/low_level.py,sha256=V7GnujxnWZh2N2sMsV5N4d9Imymokkm3zBwgt77_bSE,11956
++urllib3/packages/__init__.py,sha256=h4BLhD4tLaBx1adaDtKXfupsgqY0wWLXb_f1_yVlV6A,108
++urllib3/packages/six.py,sha256=adx4z-eM_D0Vvu0IIqVzFACQ_ux9l64y7DkSEfbxCDs,32536
++urllib3/packages/backports/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
++urllib3/packages/backports/makefile.py,sha256=005wrvH-_pWSnTFqQ2sdzzh4zVCtQUUQ4mR2Yyxwc0A,1418
++urllib3/packages/ssl_match_hostname/__init__.py,sha256=ywgKMtfHi1-DrXlzPfVAhzsLzzqcK7GT6eLgdode1Fg,688
++urllib3/packages/ssl_match_hostname/_implementation.py,sha256=6dZ-q074g7XhsJ27MFCgkct8iVNZB3sMZvKhf-KUVy0,5679
++urllib3/util/__init__.py,sha256=bWNaav_OT-1L7-sxm59cGb59rDORlbhb_4noduM5m0U,1038
++urllib3/util/connection.py,sha256=NsxUAKQ98GKywta--zg57CdVpeTCI6N-GElCq78Dl8U,4637
++urllib3/util/queue.py,sha256=myTX3JDHntglKQNBf3b6dasHH-uF-W59vzGSQiFdAfI,497
++urllib3/util/request.py,sha256=C-6-AWffxZG03AdRGoY59uqsn4CVItKU6gjxz7Hc3Mc,3815
++urllib3/util/response.py,sha256=_WbTQr8xRQuJuY2rTIZxVdJD6mnEOtQupjaK_bF_Vj8,2573
++urllib3/util/retry.py,sha256=Ui74h44gLIIWkAxT9SK3A2mEvu55-odWgJMw3LiUNGk,15450
++urllib3/util/ssl_.py,sha256=engc91JD3PYAEPdoqdDnMnWtUT549Ma2nNTWJiAWRtc,14151
++urllib3/util/timeout.py,sha256=bCtaS_xVKaTDJ5VMlroXBfCnPUDNVGZqik7-z83issg,9871
++urllib3/util/url.py,sha256=P0jDIHCnMJb4x1qILZYnn3hkcxOSWYrDtvzjiC_d218,14096
++urllib3/util/wait.py,sha256=k46KzqIYu3Vnzla5YW3EvtInNlU_QycFqQAghIOxoAg,5406
++urllib3-1.25.7.dist-info/LICENSE.txt,sha256=fA0TbuBYU4mt8tJWcbuZaHofdZKfRlt_Fu4_Ado3JV4,1115
++urllib3-1.25.7.dist-info/METADATA,sha256=9YvStEd7yBTKiX8vv2nhP5io758PIFay1wO4lZwo0Bo,38768
++urllib3-1.25.7.dist-info/WHEEL,sha256=8zNYZbwQSXoB9IfXOjPfeNwvAsALAjffgk27FqvCWbo,110
++urllib3-1.25.7.dist-info/top_level.txt,sha256=EMiXL2sKrTcmrMxIHTqdc3ET54pQI2Y072LexFEemvo,8
++urllib3-1.25.7.dist-info/RECORD,,
++urllib3-1.25.7.dist-info/INSTALLER,sha256=zuuue4knoyJ-UwPPXg8fezS7VCrXJQrAP7zeNuwvFQg,4
++urllib3/contrib/_securetransport/__pycache__/bindings.cpython-37.pyc,,
++urllib3/contrib/_securetransport/__pycache__/low_level.cpython-37.pyc,,
++urllib3/contrib/_securetransport/__pycache__/__init__.cpython-37.pyc,,
++urllib3/contrib/__pycache__/appengine.cpython-37.pyc,,
++urllib3/contrib/__pycache__/ntlmpool.cpython-37.pyc,,
++urllib3/contrib/__pycache__/pyopenssl.cpython-37.pyc,,
++urllib3/contrib/__pycache__/securetransport.cpython-37.pyc,,
++urllib3/contrib/__pycache__/socks.cpython-37.pyc,,
++urllib3/contrib/__pycache__/_appengine_environ.cpython-37.pyc,,
++urllib3/contrib/__pycache__/__init__.cpython-37.pyc,,
++urllib3/packages/backports/__pycache__/makefile.cpython-37.pyc,,
++urllib3/packages/backports/__pycache__/__init__.cpython-37.pyc,,
++urllib3/packages/ssl_match_hostname/__pycache__/_implementation.cpython-37.pyc,,
++urllib3/packages/ssl_match_hostname/__pycache__/__init__.cpython-37.pyc,,
++urllib3/packages/__pycache__/six.cpython-37.pyc,,
++urllib3/packages/__pycache__/__init__.cpython-37.pyc,,
++urllib3/util/__pycache__/connection.cpython-37.pyc,,
++urllib3/util/__pycache__/queue.cpython-37.pyc,,
++urllib3/util/__pycache__/request.cpython-37.pyc,,
++urllib3/util/__pycache__/response.cpython-37.pyc,,
++urllib3/util/__pycache__/retry.cpython-37.pyc,,
++urllib3/util/__pycache__/ssl_.cpython-37.pyc,,
++urllib3/util/__pycache__/timeout.cpython-37.pyc,,
++urllib3/util/__pycache__/url.cpython-37.pyc,,
++urllib3/util/__pycache__/wait.cpython-37.pyc,,
++urllib3/util/__pycache__/__init__.cpython-37.pyc,,
++urllib3/__pycache__/connection.cpython-37.pyc,,
++urllib3/__pycache__/connectionpool.cpython-37.pyc,,
++urllib3/__pycache__/exceptions.cpython-37.pyc,,
++urllib3/__pycache__/fields.cpython-37.pyc,,
++urllib3/__pycache__/filepost.cpython-37.pyc,,
++urllib3/__pycache__/poolmanager.cpython-37.pyc,,
++urllib3/__pycache__/request.cpython-37.pyc,,
++urllib3/__pycache__/response.cpython-37.pyc,,
++urllib3/__pycache__/_collections.cpython-37.pyc,,
++urllib3/__pycache__/__init__.cpython-37.pyc,,
+Index: venv/Lib/site-packages/urllib3-1.25.7.dist-info/INSTALLER
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3-1.25.7.dist-info/INSTALLER	(date 1573549850864)
++++ venv/Lib/site-packages/urllib3-1.25.7.dist-info/INSTALLER	(date 1573549850864)
+@@ -0,0 +1,1 @@
++pip
+Index: venv/Lib/site-packages/urllib3-1.25.7.dist-info/METADATA
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/urllib3-1.25.7.dist-info/METADATA	(date 1573549849568)
++++ venv/Lib/site-packages/urllib3-1.25.7.dist-info/METADATA	(date 1573549849568)
+@@ -0,0 +1,1229 @@
++Metadata-Version: 2.1
++Name: urllib3
++Version: 1.25.7
++Summary: HTTP library with thread-safe connection pooling, file post, and more.
++Home-page: https://urllib3.readthedocs.io/
++Author: Andrey Petrov
++Author-email: andrey.petrov@shazow.net
++License: MIT
++Project-URL: Documentation, https://urllib3.readthedocs.io/
++Project-URL: Code, https://github.com/urllib3/urllib3
++Project-URL: Issue tracker, https://github.com/urllib3/urllib3/issues
++Keywords: urllib httplib threadsafe filepost http https ssl pooling
++Platform: UNKNOWN
++Classifier: Environment :: Web Environment
++Classifier: Intended Audience :: Developers
++Classifier: License :: OSI Approved :: MIT License
++Classifier: Operating System :: OS Independent
++Classifier: Programming Language :: Python
++Classifier: Programming Language :: Python :: 2
++Classifier: Programming Language :: Python :: 2.7
++Classifier: Programming Language :: Python :: 3
++Classifier: Programming Language :: Python :: 3.4
++Classifier: Programming Language :: Python :: 3.5
++Classifier: Programming Language :: Python :: 3.6
++Classifier: Programming Language :: Python :: 3.7
++Classifier: Programming Language :: Python :: 3.8
++Classifier: Programming Language :: Python :: Implementation :: CPython
++Classifier: Programming Language :: Python :: Implementation :: PyPy
++Classifier: Topic :: Internet :: WWW/HTTP
++Classifier: Topic :: Software Development :: Libraries
++Requires-Python: >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <4
++Provides-Extra: brotli
++Requires-Dist: brotlipy (>=0.6.0) ; extra == 'brotli'
++Provides-Extra: secure
++Requires-Dist: pyOpenSSL (>=0.14) ; extra == 'secure'
++Requires-Dist: cryptography (>=1.3.4) ; extra == 'secure'
++Requires-Dist: idna (>=2.0.0) ; extra == 'secure'
++Requires-Dist: certifi ; extra == 'secure'
++Requires-Dist: ipaddress ; (python_version == "2.7") and extra == 'secure'
++Provides-Extra: socks
++Requires-Dist: PySocks (!=1.5.7,<2.0,>=1.5.6) ; extra == 'socks'
++
++urllib3
++=======
++
++urllib3 is a powerful, *sanity-friendly* HTTP client for Python. Much of the
++Python ecosystem already uses urllib3 and you should too.
++urllib3 brings many critical features that are missing from the Python
++standard libraries:
++
++- Thread safety.
++- Connection pooling.
++- Client-side SSL/TLS verification.
++- File uploads with multipart encoding.
++- Helpers for retrying requests and dealing with HTTP redirects.
++- Support for gzip, deflate, and brotli encoding.
++- Proxy support for HTTP and SOCKS.
++- 100% test coverage.
++
++urllib3 is powerful and easy to use::
++
++    >>> import urllib3
++    >>> http = urllib3.PoolManager()
++    >>> r = http.request('GET', 'http://httpbin.org/robots.txt')
++    >>> r.status
++    200
++    >>> r.data
++    'User-agent: *\nDisallow: /deny\n'
++
++
++Installing
++----------
++
++urllib3 can be installed with `pip <https://pip.pypa.io>`_::
++
++    $ pip install urllib3
++
++Alternatively, you can grab the latest source code from `GitHub <https://github.com/urllib3/urllib3>`_::
++
++    $ git clone git://github.com/urllib3/urllib3.git
++    $ python setup.py install
++
++
++Documentation
++-------------
++
++urllib3 has usage and reference documentation at `urllib3.readthedocs.io <https://urllib3.readthedocs.io>`_.
++
++
++Contributing
++------------
++
++urllib3 happily accepts contributions. Please see our
++`contributing documentation <https://urllib3.readthedocs.io/en/latest/contributing.html>`_
++for some tips on getting started.
++
++
++Security Disclosures
++--------------------
++
++To report a security vulnerability, please use the
++`Tidelift security contact <https://tidelift.com/security>`_.
++Tidelift will coordinate the fix and disclosure with maintainers.
++
++Maintainers
++-----------
++
++- `@sethmlarson <https://github.com/sethmlarson>`_ (Seth M. Larson)
++- `@theacodes <https://github.com/theacodes>`_ (Thea Flowers)
++- `@haikuginger <https://github.com/haikuginger>`_ (Jess Shapiro)
++- `@lukasa <https://github.com/lukasa>`_ (Cory Benfield)
++- `@sigmavirus24 <https://github.com/sigmavirus24>`_ (Ian Stapleton Cordasco)
++- `@shazow <https://github.com/shazow>`_ (Andrey Petrov)
++
++👋
++
++
++Sponsorship
++-----------
++
++.. |tideliftlogo| image:: https://nedbatchelder.com/pix/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White_small.png
++   :width: 75
++   :alt: Tidelift
++
++.. list-table::
++   :widths: 10 100
++
++   * - |tideliftlogo|
++     - Professional support for urllib3 is available as part of the `Tidelift
++       Subscription`_.  Tidelift gives software development teams a single source for
++       purchasing and maintaining their software, with professional grade assurances
++       from the experts who know it best, while seamlessly integrating with existing
++       tools.
++
++.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-urllib3?utm_source=pypi-urllib3&utm_medium=referral&utm_campaign=readme
++
++If your company benefits from this library, please consider `sponsoring its
++development <https://urllib3.readthedocs.io/en/latest/contributing.html#sponsorship-project-grants>`_.
++
++Sponsors include:
++
++- Abbott (2018-2019), sponsored `@sethmlarson <https://github.com/sethmlarson>`_'s work on urllib3.
++- Google Cloud Platform (2018-2019), sponsored `@theacodes <https://github.com/theacodes>`_'s work on urllib3.
++- Akamai (2017-2018), sponsored `@haikuginger <https://github.com/haikuginger>`_'s work on urllib3
++- Hewlett Packard Enterprise (2016-2017), sponsored `@Lukasa’s <https://github.com/Lukasa>`_ work on urllib3.
++
++
++Changes
++=======
++
++1.25.7 (2019-11-11)
++-------------------
++
++* Preserve ``chunked`` parameter on retries (Pull #1715, Pull #1734)
++
++* Allow unset ``SERVER_SOFTWARE`` in App Engine (Pull #1704, Issue #1470)
++
++* Fix issue where URL fragment was sent within the request target. (Pull #1732)
++
++* Fix issue where an empty query section in a URL would fail to parse. (Pull #1732)
++
++* Remove TLS 1.3 support in SecureTransport due to Apple removing support (Pull #1703)
++
++
++1.25.6 (2019-09-24)
++-------------------
++
++* Fix issue where tilde (``~``) characters were incorrectly
++  percent-encoded in the path. (Pull #1692)
++
++
++1.25.5 (2019-09-19)
++-------------------
++
++* Add mitigation for BPO-37428 affecting Python <3.7.4 and OpenSSL 1.1.1+ which
++  caused certificate verification to be enabled when using ``cert_reqs=CERT_NONE``.
++  (Issue #1682)
++
++
++1.25.4 (2019-09-19)
++-------------------
++
++* Propagate Retry-After header settings to subsequent retries. (Pull #1607)
++
++* Fix edge case where Retry-After header was still respected even when
++  explicitly opted out of. (Pull #1607)
++
++* Remove dependency on ``rfc3986`` for URL parsing.
++
++* Fix issue where URLs containing invalid characters within ``Url.auth`` would
++  raise an exception instead of percent-encoding those characters.
++
++* Add support for ``HTTPResponse.auto_close = False`` which makes HTTP responses
++  work well with BufferedReaders and other ``io`` module features. (Pull #1652)
++
++* Percent-encode invalid characters in URL for ``HTTPConnectionPool.request()`` (Pull #1673)
++
++
++1.25.3 (2019-05-23)
++-------------------
++
++* Change ``HTTPSConnection`` to load system CA certificates
++  when ``ca_certs``, ``ca_cert_dir``, and ``ssl_context`` are
++  unspecified. (Pull #1608, Issue #1603)
++
++* Upgrade bundled rfc3986 to v1.3.2. (Pull #1609, Issue #1605)
++
++
++1.25.2 (2019-04-28)
++-------------------
++
++* Change ``is_ipaddress`` to not detect IPvFuture addresses. (Pull #1583)
++
++* Change ``parse_url`` to percent-encode invalid characters within the
++  path, query, and target components. (Pull #1586)
++
++
++1.25.1 (2019-04-24)
++-------------------
++
++* Add support for Google's ``Brotli`` package. (Pull #1572, Pull #1579)
++
++* Upgrade bundled rfc3986 to v1.3.1 (Pull #1578)
++
++
++1.25 (2019-04-22)
++-----------------
++
++* Require and validate certificates by default when using HTTPS (Pull #1507)
++
++* Upgraded ``urllib3.utils.parse_url()`` to be RFC 3986 compliant. (Pull #1487)
++
++* Added support for ``key_password`` for ``HTTPSConnectionPool`` to use
++  encrypted ``key_file`` without creating your own ``SSLContext`` object. (Pull #1489)
++
++* Add TLSv1.3 support to CPython, pyOpenSSL, and SecureTransport ``SSLContext``
++  implementations. (Pull #1496)
++
++* Switched the default multipart header encoder from RFC 2231 to HTML 5 working draft. (Issue #303, PR #1492)
++
++* Fixed issue where OpenSSL would block if an encrypted client private key was
++  given and no password was given. Instead an ``SSLError`` is raised. (Pull #1489)
++
++* Added support for Brotli content encoding. It is enabled automatically if
++  ``brotlipy`` package is installed which can be requested with
++  ``urllib3[brotli]`` extra. (Pull #1532)
++
++* Drop ciphers using DSS key exchange from default TLS cipher suites.
++  Improve default ciphers when using SecureTransport. (Pull #1496)
++
++* Implemented a more efficient ``HTTPResponse.__iter__()`` method. (Issue #1483)
++
++1.24.3 (2019-05-01)
++-------------------
++
++* Apply fix for CVE-2019-9740. (Pull #1591)
++
++1.24.2 (2019-04-17)
++-------------------
++
++* Don't load system certificates by default when any other ``ca_certs``, ``ca_certs_dir`` or
++  ``ssl_context`` parameters are specified.
++
++* Remove Authorization header regardless of case when redirecting to cross-site. (Issue #1510)
++
++* Add support for IPv6 addresses in subjectAltName section of certificates. (Issue #1269)
++
++
++1.24.1 (2018-11-02)
++-------------------
++
++* Remove quadratic behavior within ``GzipDecoder.decompress()`` (Issue #1467)
++
++* Restored functionality of ``ciphers`` parameter for ``create_urllib3_context()``. (Issue #1462)
++
++
++1.24 (2018-10-16)
++-----------------
++
++* Allow key_server_hostname to be specified when initializing a PoolManager to allow custom SNI to be overridden. (Pull #1449)
++
++* Test against Python 3.7 on AppVeyor. (Pull #1453)
++
++* Early-out ipv6 checks when running on App Engine. (Pull #1450)
++
++* Change ambiguous description of backoff_factor (Pull #1436)
++
++* Add ability to handle multiple Content-Encodings (Issue #1441 and Pull #1442)
++
++* Skip DNS names that can't be idna-decoded when using pyOpenSSL (Issue #1405).
++
++* Add a server_hostname parameter to HTTPSConnection which allows for
++  overriding the SNI hostname sent in the handshake. (Pull #1397)
++
++* Drop support for EOL Python 2.6 (Pull #1429 and Pull #1430)
++
++* Fixed bug where responses with header Content-Type: message/* erroneously
++  raised HeaderParsingError, resulting in a warning being logged. (Pull #1439)
++
++* Move urllib3 to src/urllib3 (Pull #1409)
++
++
++1.23 (2018-06-04)
++-----------------
++
++* Allow providing a list of headers to strip from requests when redirecting
++  to a different host. Defaults to the ``Authorization`` header. Different
++  headers can be set via ``Retry.remove_headers_on_redirect``. (Issue #1316)
++
++* Fix ``util.selectors._fileobj_to_fd`` to accept ``long`` (Issue #1247).
++
++* Dropped Python 3.3 support. (Pull #1242)
++
++* Put the connection back in the pool when calling stream() or read_chunked() on
++  a chunked HEAD response. (Issue #1234)
++
++* Fixed pyOpenSSL-specific ssl client authentication issue when clients
++  attempted to auth via certificate + chain (Issue #1060)
++
++* Add the port to the connectionpool connect print (Pull #1251)
++
++* Don't use the ``uuid`` module to create multipart data boundaries. (Pull #1380)
++
++* ``read_chunked()`` on a closed response returns no chunks. (Issue #1088)
++
++* Add Python 2.6 support to ``contrib.securetransport`` (Pull #1359)
++
++* Added support for auth info in url for SOCKS proxy (Pull #1363)
++
++
++1.22 (2017-07-20)
++-----------------
++
++* Fixed missing brackets in ``HTTP CONNECT`` when connecting to IPv6 address via
++  IPv6 proxy. (Issue #1222)
++
++* Made the connection pool retry on ``SSLError``.  The original ``SSLError``
++  is available on ``MaxRetryError.reason``. (Issue #1112)
++
++* Drain and release connection before recursing on retry/redirect.  Fixes
++  deadlocks with a blocking connectionpool. (Issue #1167)
++
++* Fixed compatibility for cookiejar. (Issue #1229)
++
++* pyopenssl: Use vendored version of ``six``. (Issue #1231)
++
++
++1.21.1 (2017-05-02)
++-------------------
++
++* Fixed SecureTransport issue that would cause long delays in response body
++  delivery. (Pull #1154)
++
++* Fixed regression in 1.21 that threw exceptions when users passed the
++  ``socket_options`` flag to the ``PoolManager``.  (Issue #1165)
++
++* Fixed regression in 1.21 that threw exceptions when users passed the
++  ``assert_hostname`` or ``assert_fingerprint`` flag to the ``PoolManager``.
++  (Pull #1157)
++
++
++1.21 (2017-04-25)
++-----------------
++
++* Improved performance of certain selector system calls on Python 3.5 and
++  later. (Pull #1095)
++
++* Resolved issue where the PyOpenSSL backend would not wrap SysCallError
++  exceptions appropriately when sending data. (Pull #1125)
++
++* Selectors now detects a monkey-patched select module after import for modules
++  that patch the select module like eventlet, greenlet. (Pull #1128)
++
++* Reduced memory consumption when streaming zlib-compressed responses
++  (as opposed to raw deflate streams). (Pull #1129)
++
++* Connection pools now use the entire request context when constructing the
++  pool key. (Pull #1016)
++
++* ``PoolManager.connection_from_*`` methods now accept a new keyword argument,
++  ``pool_kwargs``, which are merged with the existing ``connection_pool_kw``.
++  (Pull #1016)
++
++* Add retry counter for ``status_forcelist``. (Issue #1147)
++
++* Added ``contrib`` module for using SecureTransport on macOS:
++  ``urllib3.contrib.securetransport``.  (Pull #1122)
++
++* urllib3 now only normalizes the case of ``http://`` and ``https://`` schemes:
++  for schemes it does not recognise, it assumes they are case-sensitive and
++  leaves them unchanged.
++  (Issue #1080)
++
++
++1.20 (2017-01-19)
++-----------------
++
++* Added support for waiting for I/O using selectors other than select,
++  improving urllib3's behaviour with large numbers of concurrent connections.
++  (Pull #1001)
++
++* Updated the date for the system clock check. (Issue #1005)
++
++* ConnectionPools now correctly consider hostnames to be case-insensitive.
++  (Issue #1032)
++
++* Outdated versions of PyOpenSSL now cause the PyOpenSSL contrib module
++  to fail when it is injected, rather than at first use. (Pull #1063)
++
++* Outdated versions of cryptography now cause the PyOpenSSL contrib module
++  to fail when it is injected, rather than at first use. (Issue #1044)
++
++* Automatically attempt to rewind a file-like body object when a request is
++  retried or redirected. (Pull #1039)
++
++* Fix some bugs that occur when modules incautiously patch the queue module.
++  (Pull #1061)
++
++* Prevent retries from occurring on read timeouts for which the request method
++  was not in the method whitelist. (Issue #1059)
++
++* Changed the PyOpenSSL contrib module to lazily load idna to avoid
++  unnecessarily bloating the memory of programs that don't need it. (Pull
++  #1076)
++
++* Add support for IPv6 literals with zone identifiers. (Pull #1013)
++
++* Added support for socks5h:// and socks4a:// schemes when working with SOCKS
++  proxies, and controlled remote DNS appropriately. (Issue #1035)
++
++
++1.19.1 (2016-11-16)
++-------------------
++
++* Fixed AppEngine import that didn't function on Python 3.5. (Pull #1025)
++
++
++1.19 (2016-11-03)
++-----------------
++
++* urllib3 now respects Retry-After headers on 413, 429, and 503 responses when
++  using the default retry logic. (Pull #955)
++
++* Remove markers from setup.py to assist ancient setuptools versions. (Issue
++  #986)
++
++* Disallow superscripts and other integerish things in URL ports. (Issue #989)
++
++* Allow urllib3's HTTPResponse.stream() method to continue to work with
++  non-httplib underlying FPs. (Pull #990)
++
++* Empty filenames in multipart headers are now emitted as such, rather than
++  being suppressed. (Issue #1015)
++
++* Prefer user-supplied Host headers on chunked uploads. (Issue #1009)
++
++
++1.18.1 (2016-10-27)
++-------------------
++
++* CVE-2016-9015. Users who are using urllib3 version 1.17 or 1.18 along with
++  PyOpenSSL injection and OpenSSL 1.1.0 *must* upgrade to this version. This
++  release fixes a vulnerability whereby urllib3 in the above configuration
++  would silently fail to validate TLS certificates due to erroneously setting
++  invalid flags in OpenSSL's ``SSL_CTX_set_verify`` function. These erroneous
++  flags do not cause a problem in OpenSSL versions before 1.1.0, which
++  interprets the presence of any flag as requesting certificate validation.
++
++  There is no PR for this patch, as it was prepared for simultaneous disclosure
++  and release. The master branch received the same fix in PR #1010.
++
++
++1.18 (2016-09-26)
++-----------------
++
++* Fixed incorrect message for IncompleteRead exception. (PR #973)
++
++* Accept ``iPAddress`` subject alternative name fields in TLS certificates.
++  (Issue #258)
++
++* Fixed consistency of ``HTTPResponse.closed`` between Python 2 and 3.
++  (Issue #977)
++
++* Fixed handling of wildcard certificates when using PyOpenSSL. (Issue #979)
++
++
++1.17 (2016-09-06)
++-----------------
++
++* Accept ``SSLContext`` objects for use in SSL/TLS negotiation. (Issue #835)
++
++* ConnectionPool debug log now includes scheme, host, and port. (Issue #897)
++
++* Substantially refactored documentation. (Issue #887)
++
++* Used URLFetch default timeout on AppEngine, rather than hardcoding our own.
++  (Issue #858)
++
++* Normalize the scheme and host in the URL parser (Issue #833)
++
++* ``HTTPResponse`` contains the last ``Retry`` object, which now also
++  contains retries history. (Issue #848)
++
++* Timeout can no longer be set as boolean, and must be greater than zero.
++  (PR #924)
++
++* Removed pyasn1 and ndg-httpsclient from dependencies used for PyOpenSSL. We
++  now use cryptography and idna, both of which are already dependencies of
++  PyOpenSSL. (PR #930)
++
++* Fixed infinite loop in ``stream`` when amt=None. (Issue #928)
++
++* Try to use the operating system's certificates when we are using an
++  ``SSLContext``. (PR #941)
++
++* Updated cipher suite list to allow ChaCha20+Poly1305. AES-GCM is preferred to
++  ChaCha20, but ChaCha20 is then preferred to everything else. (PR #947)
++
++* Updated cipher suite list to remove 3DES-based cipher suites. (PR #958)
++
++* Removed the cipher suite fallback to allow HIGH ciphers. (PR #958)
++
++* Implemented ``length_remaining`` to determine remaining content
++  to be read. (PR #949)
++
++* Implemented ``enforce_content_length`` to enable exceptions when
++  incomplete data chunks are received. (PR #949)
++
++* Dropped connection start, dropped connection reset, redirect, forced retry,
++  and new HTTPS connection log levels to DEBUG, from INFO. (PR #967)
++
++
++1.16 (2016-06-11)
++-----------------
++
++* Disable IPv6 DNS when IPv6 connections are not possible. (Issue #840)
++
++* Provide ``key_fn_by_scheme`` pool keying mechanism that can be
++  overridden. (Issue #830)
++
++* Normalize scheme and host to lowercase for pool keys, and include
++  ``source_address``. (Issue #830)
++
++* Cleaner exception chain in Python 3 for ``_make_request``.
++  (Issue #861)
++
++* Fixed installing ``urllib3[socks]`` extra. (Issue #864)
++
++* Fixed signature of ``ConnectionPool.close`` so it can actually safely be
++  called by subclasses. (Issue #873)
++
++* Retain ``release_conn`` state across retries. (Issues #651, #866)
++
++* Add customizable ``HTTPConnectionPool.ResponseCls``, which defaults to
++  ``HTTPResponse`` but can be replaced with a subclass. (Issue #879)
++
++
++1.15.1 (2016-04-11)
++-------------------
++
++* Fix packaging to include backports module. (Issue #841)
++
++
++1.15 (2016-04-06)
++-----------------
++
++* Added Retry(raise_on_status=False). (Issue #720)
++
++* Always use setuptools, no more distutils fallback. (Issue #785)
++
++* Dropped support for Python 3.2. (Issue #786)
++
++* Chunked transfer encoding when requesting with ``chunked=True``.
++  (Issue #790)
++
++* Fixed regression with IPv6 port parsing. (Issue #801)
++
++* Append SNIMissingWarning messages to allow users to specify it in
++  the PYTHONWARNINGS environment variable. (Issue #816)
++
++* Handle unicode headers in Py2. (Issue #818)
++
++* Log certificate when there is a hostname mismatch. (Issue #820)
++
++* Preserve order of request/response headers. (Issue #821)
++
++
++1.14 (2015-12-29)
++-----------------
++
++* contrib: SOCKS proxy support! (Issue #762)
++
++* Fixed AppEngine handling of transfer-encoding header and bug
++  in Timeout defaults checking. (Issue #763)
++
++
++1.13.1 (2015-12-18)
++-------------------
++
++* Fixed regression in IPv6 + SSL for match_hostname. (Issue #761)
++
++
++1.13 (2015-12-14)
++-----------------
++
++* Fixed ``pip install urllib3[secure]`` on modern pip. (Issue #706)
++
++* pyopenssl: Fixed SSL3_WRITE_PENDING error. (Issue #717)
++
++* pyopenssl: Support for TLSv1.1 and TLSv1.2. (Issue #696)
++
++* Close connections more defensively on exception. (Issue #734)
++
++* Adjusted ``read_chunked`` to handle gzipped, chunk-encoded bodies without
++  repeatedly flushing the decoder, to function better on Jython. (Issue #743)
++
++* Accept ``ca_cert_dir`` for SSL-related PoolManager configuration. (Issue #758)
++
++
++1.12 (2015-09-03)
++-----------------
++
++* Rely on ``six`` for importing ``httplib`` to work around
++  conflicts with other Python 3 shims. (Issue #688)
++
++* Add support for directories of certificate authorities, as supported by
++  OpenSSL. (Issue #701)
++
++* New exception: ``NewConnectionError``, raised when we fail to establish
++  a new connection, usually ``ECONNREFUSED`` socket error.
++
++
++1.11 (2015-07-21)
++-----------------
++
++* When ``ca_certs`` is given, ``cert_reqs`` defaults to
++  ``'CERT_REQUIRED'``. (Issue #650)
++
++* ``pip install urllib3[secure]`` will install Certifi and
++  PyOpenSSL as dependencies. (Issue #678)
++
++* Made ``HTTPHeaderDict`` usable as a ``headers`` input value
++  (Issues #632, #679)
++
++* Added `urllib3.contrib.appengine <https://urllib3.readthedocs.io/en/latest/contrib.html#google-app-engine>`_
++  which has an ``AppEngineManager`` for using ``URLFetch`` in a
++  Google AppEngine environment. (Issue #664)
++
++* Dev: Added test suite for AppEngine. (Issue #631)
++
++* Fix performance regression when using PyOpenSSL. (Issue #626)
++
++* Passing incorrect scheme (e.g. ``foo://``) will raise
++  ``ValueError`` instead of ``AssertionError`` (backwards
++  compatible for now, but please migrate). (Issue #640)
++
++* Fix pools not getting replenished when an error occurs during a
++  request using ``release_conn=False``. (Issue #644)
++
++* Fix pool-default headers not applying for url-encoded requests
++  like GET. (Issue #657)
++
++* log.warning in Python 3 when headers are skipped due to parsing
++  errors. (Issue #642)
++
++* Close and discard connections if an error occurs during read.
++  (Issue #660)
++
++* Fix host parsing for IPv6 proxies. (Issue #668)
++
++* Separate warning type SubjectAltNameWarning, now issued once
++  per host. (Issue #671)
++
++* Fix ``httplib.IncompleteRead`` not getting converted to
++  ``ProtocolError`` when using ``HTTPResponse.stream()``
++  (Issue #674)
++
++1.10.4 (2015-05-03)
++-------------------
++
++* Migrate tests to Tornado 4. (Issue #594)
++
++* Append default warning configuration rather than overwrite.
++  (Issue #603)
++
++* Fix streaming decoding regression. (Issue #595)
++
++* Fix chunked requests losing state across keep-alive connections.
++  (Issue #599)
++
++* Fix hanging when chunked HEAD response has no body. (Issue #605)
++
++
++1.10.3 (2015-04-21)
++-------------------
++
++* Emit ``InsecurePlatformWarning`` when SSLContext object is missing.
++  (Issue #558)
++
++* Fix regression of duplicate header keys being discarded.
++  (Issue #563)
++
++* ``Response.stream()`` returns a generator for chunked responses.
++  (Issue #560)
++
++* Set upper-bound timeout when waiting for a socket in PyOpenSSL.
++  (Issue #585)
++
++* Work on platforms without `ssl` module for plain HTTP requests.
++  (Issue #587)
++
++* Stop relying on the stdlib's default cipher list. (Issue #588)
++
++
++1.10.2 (2015-02-25)
++-------------------
++
++* Fix file descriptor leakage on retries. (Issue #548)
++
++* Removed RC4 from default cipher list. (Issue #551)
++
++* Header performance improvements. (Issue #544)
++
++* Fix PoolManager not obeying redirect retry settings. (Issue #553)
++
++
++1.10.1 (2015-02-10)
++-------------------
++
++* Pools can be used as context managers. (Issue #545)
++
++* Don't re-use connections which experienced an SSLError. (Issue #529)
++
++* Don't fail when gzip decoding an empty stream. (Issue #535)
++
++* Add sha256 support for fingerprint verification. (Issue #540)
++
++* Fixed handling of header values containing commas. (Issue #533)
++
++
++1.10 (2014-12-14)
++-----------------
++
++* Disabled SSLv3. (Issue #473)
++
++* Add ``Url.url`` property to return the composed url string. (Issue #394)
++
++* Fixed PyOpenSSL + gevent ``WantWriteError``. (Issue #412)
++
++* ``MaxRetryError.reason`` will always be an exception, not string.
++  (Issue #481)
++
++* Fixed SSL-related timeouts not being detected as timeouts. (Issue #492)
++
++* Py3: Use ``ssl.create_default_context()`` when available. (Issue #473)
++
++* Emit ``InsecureRequestWarning`` for *every* insecure HTTPS request.
++  (Issue #496)
++
++* Emit ``SecurityWarning`` when certificate has no ``subjectAltName``.
++  (Issue #499)
++
++* Close and discard sockets which experienced SSL-related errors.
++  (Issue #501)
++
++* Handle ``body`` param in ``.request(...)``. (Issue #513)
++
++* Respect timeout with HTTPS proxy. (Issue #505)
++
++* PyOpenSSL: Handle ZeroReturnError exception. (Issue #520)
++
++
++1.9.1 (2014-09-13)
++------------------
++
++* Apply socket arguments before binding. (Issue #427)
++
++* More careful checks if fp-like object is closed. (Issue #435)
++
++* Fixed packaging issues of some development-related files not
++  getting included. (Issue #440)
++
++* Allow performing *only* fingerprint verification. (Issue #444)
++
++* Emit ``SecurityWarning`` if system clock is waaay off. (Issue #445)
++
++* Fixed PyOpenSSL compatibility with PyPy. (Issue #450)
++
++* Fixed ``BrokenPipeError`` and ``ConnectionError`` handling in Py3.
++  (Issue #443)
++
++
++
++1.9 (2014-07-04)
++----------------
++
++* Shuffled around development-related files. If you're maintaining a distro
++  package of urllib3, you may need to tweak things. (Issue #415)
++
++* Unverified HTTPS requests will trigger a warning on the first request. See
++  our new `security documentation
++  <https://urllib3.readthedocs.io/en/latest/security.html>`_ for details.
++  (Issue #426)
++
++* New retry logic and ``urllib3.util.retry.Retry`` configuration object.
++  (Issue #326)
++
++* All raised exceptions should now wrapped in a
++  ``urllib3.exceptions.HTTPException``-extending exception. (Issue #326)
++
++* All errors during a retry-enabled request should be wrapped in
++  ``urllib3.exceptions.MaxRetryError``, including timeout-related exceptions
++  which were previously exempt. Underlying error is accessible from the
++  ``.reason`` property. (Issue #326)
++
++* ``urllib3.exceptions.ConnectionError`` renamed to
++  ``urllib3.exceptions.ProtocolError``. (Issue #326)
++
++* Errors during response read (such as IncompleteRead) are now wrapped in
++  ``urllib3.exceptions.ProtocolError``. (Issue #418)
++
++* Requesting an empty host will raise ``urllib3.exceptions.LocationValueError``.
++  (Issue #417)
++
++* Catch read timeouts over SSL connections as
++  ``urllib3.exceptions.ReadTimeoutError``. (Issue #419)
++
++* Apply socket arguments before connecting. (Issue #427)
++
++
++1.8.3 (2014-06-23)
++------------------
++
++* Fix TLS verification when using a proxy in Python 3.4.1. (Issue #385)
++
++* Add ``disable_cache`` option to ``urllib3.util.make_headers``. (Issue #393)
++
++* Wrap ``socket.timeout`` exception with
++  ``urllib3.exceptions.ReadTimeoutError``. (Issue #399)
++
++* Fixed proxy-related bug where connections were being reused incorrectly.
++  (Issues #366, #369)
++
++* Added ``socket_options`` keyword parameter which allows to define
++  ``setsockopt`` configuration of new sockets. (Issue #397)
++
++* Removed ``HTTPConnection.tcp_nodelay`` in favor of
++  ``HTTPConnection.default_socket_options``. (Issue #397)
++
++* Fixed ``TypeError`` bug in Python 2.6.4. (Issue #411)
++
++
++1.8.2 (2014-04-17)
++------------------
++
++* Fix ``urllib3.util`` not being included in the package.
++
++
++1.8.1 (2014-04-17)
++------------------
++
++* Fix AppEngine bug of HTTPS requests going out as HTTP. (Issue #356)
++
++* Don't install ``dummyserver`` into ``site-packages`` as it's only needed
++  for the test suite. (Issue #362)
++
++* Added support for specifying ``source_address``. (Issue #352)
++
++
++1.8 (2014-03-04)
++----------------
++
++* Improved url parsing in ``urllib3.util.parse_url`` (properly parse '@' in
++  username, and blank ports like 'hostname:').
++
++* New ``urllib3.connection`` module which contains all the HTTPConnection
++  objects.
++
++* Several ``urllib3.util.Timeout``-related fixes. Also changed constructor
++  signature to a more sensible order. [Backwards incompatible]
++  (Issues #252, #262, #263)
++
++* Use ``backports.ssl_match_hostname`` if it's installed. (Issue #274)
++
++* Added ``.tell()`` method to ``urllib3.response.HTTPResponse`` which
++  returns the number of bytes read so far. (Issue #277)
++
++* Support for platforms without threading. (Issue #289)
++
++* Expand default-port comparison in ``HTTPConnectionPool.is_same_host``
++  to allow a pool with no specified port to be considered equal to to an
++  HTTP/HTTPS url with port 80/443 explicitly provided. (Issue #305)
++
++* Improved default SSL/TLS settings to avoid vulnerabilities.
++  (Issue #309)
++
++* Fixed ``urllib3.poolmanager.ProxyManager`` not retrying on connect errors.
++  (Issue #310)
++
++* Disable Nagle's Algorithm on the socket for non-proxies. A subset of requests
++  will send the entire HTTP request ~200 milliseconds faster; however, some of
++  the resulting TCP packets will be smaller. (Issue #254)
++
++* Increased maximum number of SubjectAltNames in ``urllib3.contrib.pyopenssl``
++  from the default 64 to 1024 in a single certificate. (Issue #318)
++
++* Headers are now passed and stored as a custom
++  ``urllib3.collections_.HTTPHeaderDict`` object rather than a plain ``dict``.
++  (Issue #329, #333)
++
++* Headers no longer lose their case on Python 3. (Issue #236)
++
++* ``urllib3.contrib.pyopenssl`` now uses the operating system's default CA
++  certificates on inject. (Issue #332)
++
++* Requests with ``retries=False`` will immediately raise any exceptions without
++  wrapping them in ``MaxRetryError``. (Issue #348)
++
++* Fixed open socket leak with SSL-related failures. (Issue #344, #348)
++
++
++1.7.1 (2013-09-25)
++------------------
++
++* Added granular timeout support with new ``urllib3.util.Timeout`` class.
++  (Issue #231)
++
++* Fixed Python 3.4 support. (Issue #238)
++
++
++1.7 (2013-08-14)
++----------------
++
++* More exceptions are now pickle-able, with tests. (Issue #174)
++
++* Fixed redirecting with relative URLs in Location header. (Issue #178)
++
++* Support for relative urls in ``Location: ...`` header. (Issue #179)
++
++* ``urllib3.response.HTTPResponse`` now inherits from ``io.IOBase`` for bonus
++  file-like functionality. (Issue #187)
++
++* Passing ``assert_hostname=False`` when creating a HTTPSConnectionPool will
++  skip hostname verification for SSL connections. (Issue #194)
++
++* New method ``urllib3.response.HTTPResponse.stream(...)`` which acts as a
++  generator wrapped around ``.read(...)``. (Issue #198)
++
++* IPv6 url parsing enforces brackets around the hostname. (Issue #199)
++
++* Fixed thread race condition in
++  ``urllib3.poolmanager.PoolManager.connection_from_host(...)`` (Issue #204)
++
++* ``ProxyManager`` requests now include non-default port in ``Host: ...``
++  header. (Issue #217)
++
++* Added HTTPS proxy support in ``ProxyManager``. (Issue #170 #139)
++
++* New ``RequestField`` object can be passed to the ``fields=...`` param which
++  can specify headers. (Issue #220)
++
++* Raise ``urllib3.exceptions.ProxyError`` when connecting to proxy fails.
++  (Issue #221)
++
++* Use international headers when posting file names. (Issue #119)
++
++* Improved IPv6 support. (Issue #203)
++
++
++1.6 (2013-04-25)
++----------------
++
++* Contrib: Optional SNI support for Py2 using PyOpenSSL. (Issue #156)
++
++* ``ProxyManager`` automatically adds ``Host: ...`` header if not given.
++
++* Improved SSL-related code. ``cert_req`` now optionally takes a string like
++  "REQUIRED" or "NONE". Same with ``ssl_version`` takes strings like "SSLv23"
++  The string values reflect the suffix of the respective constant variable.
++  (Issue #130)
++
++* Vendored ``socksipy`` now based on Anorov's fork which handles unexpectedly
++  closed proxy connections and larger read buffers. (Issue #135)
++
++* Ensure the connection is closed if no data is received, fixes connection leak
++  on some platforms. (Issue #133)
++
++* Added SNI support for SSL/TLS connections on Py32+. (Issue #89)
++
++* Tests fixed to be compatible with Py26 again. (Issue #125)
++
++* Added ability to choose SSL version by passing an ``ssl.PROTOCOL_*`` constant
++  to the ``ssl_version`` parameter of ``HTTPSConnectionPool``. (Issue #109)
++
++* Allow an explicit content type to be specified when encoding file fields.
++  (Issue #126)
++
++* Exceptions are now pickleable, with tests. (Issue #101)
++
++* Fixed default headers not getting passed in some cases. (Issue #99)
++
++* Treat "content-encoding" header value as case-insensitive, per RFC 2616
++  Section 3.5. (Issue #110)
++
++* "Connection Refused" SocketErrors will get retried rather than raised.
++  (Issue #92)
++
++* Updated vendored ``six``, no longer overrides the global ``six`` module
++  namespace. (Issue #113)
++
++* ``urllib3.exceptions.MaxRetryError`` contains a ``reason`` property holding
++  the exception that prompted the final retry. If ``reason is None`` then it
++  was due to a redirect. (Issue #92, #114)
++
++* Fixed ``PoolManager.urlopen()`` from not redirecting more than once.
++  (Issue #149)
++
++* Don't assume ``Content-Type: text/plain`` for multi-part encoding parameters
++  that are not files. (Issue #111)
++
++* Pass `strict` param down to ``httplib.HTTPConnection``. (Issue #122)
++
++* Added mechanism to verify SSL certificates by fingerprint (md5, sha1) or
++  against an arbitrary hostname (when connecting by IP or for misconfigured
++  servers). (Issue #140)
++
++* Streaming decompression support. (Issue #159)
++
++
++1.5 (2012-08-02)
++----------------
++
++* Added ``urllib3.add_stderr_logger()`` for quickly enabling STDERR debug
++  logging in urllib3.
++
++* Native full URL parsing (including auth, path, query, fragment) available in
++  ``urllib3.util.parse_url(url)``.
++
++* Built-in redirect will switch method to 'GET' if status code is 303.
++  (Issue #11)
++
++* ``urllib3.PoolManager`` strips the scheme and host before sending the request
++  uri. (Issue #8)
++
++* New ``urllib3.exceptions.DecodeError`` exception for when automatic decoding,
++  based on the Content-Type header, fails.
++
++* Fixed bug with pool depletion and leaking connections (Issue #76). Added
++  explicit connection closing on pool eviction. Added
++  ``urllib3.PoolManager.clear()``.
++
++* 99% -> 100% unit test coverage.
++
++
++1.4 (2012-06-16)
++----------------
++
++* Minor AppEngine-related fixes.
++
++* Switched from ``mimetools.choose_boundary`` to ``uuid.uuid4()``.
++
++* Improved url parsing. (Issue #73)
++
++* IPv6 url support. (Issue #72)
++
++
++1.3 (2012-03-25)
++----------------
++
++* Removed pre-1.0 deprecated API.
++
++* Refactored helpers into a ``urllib3.util`` submodule.
++
++* Fixed multipart encoding to support list-of-tuples for keys with multiple
++  values. (Issue #48)
++
++* Fixed multiple Set-Cookie headers in response not getting merged properly in
++  Python 3. (Issue #53)
++
++* AppEngine support with Py27. (Issue #61)
++
++* Minor ``encode_multipart_formdata`` fixes related to Python 3 strings vs
++  bytes.
++
++
++1.2.2 (2012-02-06)
++------------------
++
++* Fixed packaging bug of not shipping ``test-requirements.txt``. (Issue #47)
++
++
++1.2.1 (2012-02-05)
++------------------
++
++* Fixed another bug related to when ``ssl`` module is not available. (Issue #41)
++
++* Location parsing errors now raise ``urllib3.exceptions.LocationParseError``
++  which inherits from ``ValueError``.
++
++
++1.2 (2012-01-29)
++----------------
++
++* Added Python 3 support (tested on 3.2.2)
++
++* Dropped Python 2.5 support (tested on 2.6.7, 2.7.2)
++
++* Use ``select.poll`` instead of ``select.select`` for platforms that support
++  it.
++
++* Use ``Queue.LifoQueue`` instead of ``Queue.Queue`` for more aggressive
++  connection reusing. Configurable by overriding ``ConnectionPool.QueueCls``.
++
++* Fixed ``ImportError`` during install when ``ssl`` module is not available.
++  (Issue #41)
++
++* Fixed ``PoolManager`` redirects between schemes (such as HTTP -> HTTPS) not
++  completing properly. (Issue #28, uncovered by Issue #10 in v1.1)
++
++* Ported ``dummyserver`` to use ``tornado`` instead of ``webob`` +
++  ``eventlet``. Removed extraneous unsupported dummyserver testing backends.
++  Added socket-level tests.
++
++* More tests. Achievement Unlocked: 99% Coverage.
++
++
++1.1 (2012-01-07)
++----------------
++
++* Refactored ``dummyserver`` to its own root namespace module (used for
++  testing).
++
++* Added hostname verification for ``VerifiedHTTPSConnection`` by vendoring in
++  Py32's ``ssl_match_hostname``. (Issue #25)
++
++* Fixed cross-host HTTP redirects when using ``PoolManager``. (Issue #10)
++
++* Fixed ``decode_content`` being ignored when set through ``urlopen``. (Issue
++  #27)
++
++* Fixed timeout-related bugs. (Issues #17, #23)
++
++
++1.0.2 (2011-11-04)
++------------------
++
++* Fixed typo in ``VerifiedHTTPSConnection`` which would only present as a bug if
++  you're using the object manually. (Thanks pyos)
++
++* Made RecentlyUsedContainer (and consequently PoolManager) more thread-safe by
++  wrapping the access log in a mutex. (Thanks @christer)
++
++* Made RecentlyUsedContainer more dict-like (corrected ``__delitem__`` and
++  ``__getitem__`` behaviour), with tests. Shouldn't affect core urllib3 code.
++
++
++1.0.1 (2011-10-10)
++------------------
++
++* Fixed a bug where the same connection would get returned into the pool twice,
++  causing extraneous "HttpConnectionPool is full" log warnings.
++
++
++1.0 (2011-10-08)
++----------------
++
++* Added ``PoolManager`` with LRU expiration of connections (tested and
++  documented).
++* Added ``ProxyManager`` (needs tests, docs, and confirmation that it works
++  with HTTPS proxies).
++* Added optional partial-read support for responses when
++  ``preload_content=False``. You can now make requests and just read the headers
++  without loading the content.
++* Made response decoding optional (default on, same as before).
++* Added optional explicit boundary string for ``encode_multipart_formdata``.
++* Convenience request methods are now inherited from ``RequestMethods``. Old
++  helpers like ``get_url`` and ``post_url`` should be abandoned in favour of
++  the new ``request(method, url, ...)``.
++* Refactored code to be even more decoupled, reusable, and extendable.
++* License header added to ``.py`` files.
++* Embiggened the documentation: Lots of Sphinx-friendly docstrings in the code
++  and docs in ``docs/`` and on https://urllib3.readthedocs.io/.
++* Embettered all the things!
++* Started writing this file.
++
++
++0.4.1 (2011-07-17)
++------------------
++
++* Minor bug fixes, code cleanup.
++
++
++0.4 (2011-03-01)
++----------------
++
++* Better unicode support.
++* Added ``VerifiedHTTPSConnection``.
++* Added ``NTLMConnectionPool`` in contrib.
++* Minor improvements.
++
++
++0.3.1 (2010-07-13)
++------------------
++
++* Added ``assert_host_name`` optional parameter. Now compatible with proxies.
++
++
++0.3 (2009-12-10)
++----------------
++
++* Added HTTPS support.
++* Minor bug fixes.
++* Refactored, broken backwards compatibility with 0.2.
++* API to be treated as stable from this version forward.
++
++
++0.2 (2008-11-17)
++----------------
++
++* Added unit tests.
++* Bug fixes.
++
++
++0.1 (2008-11-16)
++----------------
++
++* First release.
++
++
+Index: venv/Lib/site-packages/selenium/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/__init__.py	(date 1573549839680)
++++ venv/Lib/site-packages/selenium/__init__.py	(date 1573549839680)
+@@ -0,0 +1,19 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++
++__version__ = "3.141.0"
+Index: venv/Lib/site-packages/selenium/common/exceptions.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/common/exceptions.py	(date 1573549839694)
++++ venv/Lib/site-packages/selenium/common/exceptions.py	(date 1573549839694)
+@@ -0,0 +1,327 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++"""
++Exceptions that may happen in all the webdriver code.
++"""
++
++
++class WebDriverException(Exception):
++    """
++    Base webdriver exception.
++    """
++
++    def __init__(self, msg=None, screen=None, stacktrace=None):
++        self.msg = msg
++        self.screen = screen
++        self.stacktrace = stacktrace
++
++    def __str__(self):
++        exception_msg = "Message: %s\n" % self.msg
++        if self.screen is not None:
++            exception_msg += "Screenshot: available via screen\n"
++        if self.stacktrace is not None:
++            stacktrace = "\n".join(self.stacktrace)
++            exception_msg += "Stacktrace:\n%s" % stacktrace
++        return exception_msg
++
++
++class ErrorInResponseException(WebDriverException):
++    """
++    Thrown when an error has occurred on the server side.
++
++    This may happen when communicating with the firefox extension
++    or the remote driver server.
++    """
++    def __init__(self, response, msg):
++        WebDriverException.__init__(self, msg)
++        self.response = response
++
++
++class InvalidSwitchToTargetException(WebDriverException):
++    """
++    Thrown when frame or window target to be switched doesn't exist.
++    """
++    pass
++
++
++class NoSuchFrameException(InvalidSwitchToTargetException):
++    """
++    Thrown when frame target to be switched doesn't exist.
++    """
++    pass
++
++
++class NoSuchWindowException(InvalidSwitchToTargetException):
++    """
++    Thrown when window target to be switched doesn't exist.
++
++    To find the current set of active window handles, you can get a list
++    of the active window handles in the following way::
++
++        print driver.window_handles
++
++    """
++    pass
++
++
++class NoSuchElementException(WebDriverException):
++    """
++    Thrown when element could not be found.
++
++    If you encounter this exception, you may want to check the following:
++        * Check your selector used in your find_by...
++        * Element may not yet be on the screen at the time of the find operation,
++          (webpage is still loading) see selenium.webdriver.support.wait.WebDriverWait()
++          for how to write a wait wrapper to wait for an element to appear.
++    """
++    pass
++
++
++class NoSuchAttributeException(WebDriverException):
++    """
++    Thrown when the attribute of element could not be found.
++
++    You may want to check if the attribute exists in the particular browser you are
++    testing against.  Some browsers may have different property names for the same
++    property.  (IE8's .innerText vs. Firefox .textContent)
++    """
++    pass
++
++
++class StaleElementReferenceException(WebDriverException):
++    """
++    Thrown when a reference to an element is now "stale".
++
++    Stale means the element no longer appears on the DOM of the page.
++
++
++    Possible causes of StaleElementReferenceException include, but not limited to:
++        * You are no longer on the same page, or the page may have refreshed since the element
++          was located.
++        * The element may have been removed and re-added to the screen, since it was located.
++          Such as an element being relocated.
++          This can happen typically with a javascript framework when values are updated and the
++          node is rebuilt.
++        * Element may have been inside an iframe or another context which was refreshed.
++    """
++    pass
++
++
++class InvalidElementStateException(WebDriverException):
++    """
++    Thrown when a command could not be completed because the element is in an invalid state.
++
++    This can be caused by attempting to clear an element that isn't both editable and resettable.
++    """
++    pass
++
++
++class UnexpectedAlertPresentException(WebDriverException):
++    """
++    Thrown when an unexpected alert is appeared.
++
++    Usually raised when when an expected modal is blocking webdriver form executing any
++    more commands.
++    """
++    def __init__(self, msg=None, screen=None, stacktrace=None, alert_text=None):
++        super(UnexpectedAlertPresentException, self).__init__(msg, screen, stacktrace)
++        self.alert_text = alert_text
++
++    def __str__(self):
++        return "Alert Text: %s\n%s" % (self.alert_text, super(UnexpectedAlertPresentException, self).__str__())
++
++
++class NoAlertPresentException(WebDriverException):
++    """
++    Thrown when switching to no presented alert.
++
++    This can be caused by calling an operation on the Alert() class when an alert is
++    not yet on the screen.
++    """
++    pass
++
++
++class ElementNotVisibleException(InvalidElementStateException):
++    """
++    Thrown when an element is present on the DOM, but
++    it is not visible, and so is not able to be interacted with.
++
++    Most commonly encountered when trying to click or read text
++    of an element that is hidden from view.
++    """
++    pass
++
++
++class ElementNotInteractableException(InvalidElementStateException):
++    """
++    Thrown when an element is present in the DOM but interactions
++    with that element will hit another element do to paint order
++    """
++    pass
++
++
++class ElementNotSelectableException(InvalidElementStateException):
++    """
++    Thrown when trying to select an unselectable element.
++
++    For example, selecting a 'script' element.
++    """
++    pass
++
++
++class InvalidCookieDomainException(WebDriverException):
++    """
++    Thrown when attempting to add a cookie under a different domain
++    than the current URL.
++    """
++    pass
++
++
++class UnableToSetCookieException(WebDriverException):
++    """
++    Thrown when a driver fails to set a cookie.
++    """
++    pass
++
++
++class RemoteDriverServerException(WebDriverException):
++    """
++    """
++    pass
++
++
++class TimeoutException(WebDriverException):
++    """
++    Thrown when a command does not complete in enough time.
++    """
++    pass
++
++
++class MoveTargetOutOfBoundsException(WebDriverException):
++    """
++    Thrown when the target provided to the `ActionsChains` move()
++    method is invalid, i.e. out of document.
++    """
++    pass
++
++
++class UnexpectedTagNameException(WebDriverException):
++    """
++    Thrown when a support class did not get an expected web element.
++    """
++    pass
++
++
++class InvalidSelectorException(NoSuchElementException):
++    """
++    Thrown when the selector which is used to find an element does not return
++    a WebElement. Currently this only happens when the selector is an xpath
++    expression and it is either syntactically invalid (i.e. it is not a
++    xpath expression) or the expression does not select WebElements
++    (e.g. "count(//input)").
++    """
++    pass
++
++
++class ImeNotAvailableException(WebDriverException):
++    """
++    Thrown when IME support is not available. This exception is thrown for every IME-related
++    method call if IME support is not available on the machine.
++    """
++    pass
++
++
++class ImeActivationFailedException(WebDriverException):
++    """
++    Thrown when activating an IME engine has failed.
++    """
++    pass
++
++
++class InvalidArgumentException(WebDriverException):
++    """
++    The arguments passed to a command are either invalid or malformed.
++    """
++    pass
++
++
++class JavascriptException(WebDriverException):
++    """
++    An error occurred while executing JavaScript supplied by the user.
++    """
++    pass
++
++
++class NoSuchCookieException(WebDriverException):
++    """
++    No cookie matching the given path name was found amongst the associated cookies of the
++    current browsing context's active document.
++    """
++    pass
++
++
++class ScreenshotException(WebDriverException):
++    """
++    A screen capture was made impossible.
++    """
++    pass
++
++
++class ElementClickInterceptedException(WebDriverException):
++    """
++    The Element Click command could not be completed because the element receiving the events
++    is obscuring the element that was requested clicked.
++    """
++    pass
++
++
++class InsecureCertificateException(WebDriverException):
++    """
++    Navigation caused the user agent to hit a certificate warning, which is usually the result
++    of an expired or invalid TLS certificate.
++    """
++    pass
++
++
++class InvalidCoordinatesException(WebDriverException):
++    """
++    The coordinates provided to an interactions operation are invalid.
++    """
++    pass
++
++
++class InvalidSessionIdException(WebDriverException):
++    """
++    Occurs if the given session id is not in the list of active sessions, meaning the session
++    either does not exist or that it's not active.
++    """
++    pass
++
++
++class SessionNotCreatedException(WebDriverException):
++    """
++    A new session could not be created.
++    """
++    pass
++
++
++class UnknownMethodException(WebDriverException):
++    """
++    The requested command matched a known URL but did not match an method for that URL.
++    """
++    pass
+Index: venv/Lib/site-packages/selenium/common/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/common/__init__.py	(date 1573549839688)
++++ venv/Lib/site-packages/selenium/common/__init__.py	(date 1573549839688)
+@@ -0,0 +1,18 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from . import exceptions  # noqa
+Index: venv/Lib/site-packages/selenium/webdriver/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/__init__.py	(date 1573549839704)
++++ venv/Lib/site-packages/selenium/webdriver/__init__.py	(date 1573549839704)
+@@ -0,0 +1,39 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from .firefox.webdriver import WebDriver as Firefox  # noqa
++from .firefox.firefox_profile import FirefoxProfile  # noqa
++from .firefox.options import Options as FirefoxOptions  # noqa
++from .chrome.webdriver import WebDriver as Chrome  # noqa
++from .chrome.options import Options as ChromeOptions  # noqa
++from .ie.webdriver import WebDriver as Ie  # noqa
++from .ie.options import Options as IeOptions  # noqa
++from .edge.webdriver import WebDriver as Edge  # noqa
++from .opera.webdriver import WebDriver as Opera  # noqa
++from .safari.webdriver import WebDriver as Safari  # noqa
++from .blackberry.webdriver import WebDriver as BlackBerry  # noqa
++from .phantomjs.webdriver import WebDriver as PhantomJS  # noqa
++from .android.webdriver import WebDriver as Android  # noqa
++from .webkitgtk.webdriver import WebDriver as WebKitGTK # noqa
++from .webkitgtk.options import Options as WebKitGTKOptions # noqa
++from .remote.webdriver import WebDriver as Remote  # noqa
++from .common.desired_capabilities import DesiredCapabilities  # noqa
++from .common.action_chains import ActionChains  # noqa
++from .common.touch_actions import TouchActions  # noqa
++from .common.proxy import Proxy  # noqa
++
++__version__ = '3.14.1'
+Index: venv/Lib/site-packages/selenium/webdriver/ie/webdriver.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/ie/webdriver.py	(date 1573549840020)
++++ venv/Lib/site-packages/selenium/webdriver/ie/webdriver.py	(date 1573549840020)
+@@ -0,0 +1,105 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++import warnings
++
++from selenium.webdriver.common import utils
++from selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver
++from .service import Service
++from .options import Options
++
++DEFAULT_TIMEOUT = 30
++DEFAULT_PORT = 0
++DEFAULT_HOST = None
++DEFAULT_LOG_LEVEL = None
++DEFAULT_SERVICE_LOG_PATH = None
++
++
++class WebDriver(RemoteWebDriver):
++    """ Controls the IEServerDriver and allows you to drive Internet Explorer """
++
++    def __init__(self, executable_path='IEDriverServer.exe', capabilities=None,
++                 port=DEFAULT_PORT, timeout=DEFAULT_TIMEOUT, host=DEFAULT_HOST,
++                 log_level=DEFAULT_LOG_LEVEL, service_log_path=DEFAULT_SERVICE_LOG_PATH, options=None,
++                 ie_options=None, desired_capabilities=None, log_file=None, keep_alive=False):
++        """
++        Creates a new instance of the chrome driver.
++
++        Starts the service and then creates new instance of chrome driver.
++
++        :Args:
++         - executable_path - path to the executable. If the default is used it assumes the executable is in the $PATH
++         - capabilities: capabilities Dictionary object
++         - port - port you would like the service to run, if left as 0, a free port will be found.
++         - timeout - no longer used, kept for backward compatibility
++         - host - IP address for the service
++         - log_level - log level you would like the service to run.
++         - service_log_path - target of logging of service, may be "stdout", "stderr" or file path.
++         - options - IE Options instance, providing additional IE options
++         - ie_options - Deprecated argument for options
++         - desired_capabilities - alias of capabilities; this will make the signature consistent with RemoteWebDriver.
++         - log_file - Deprecated argument for service_log_path
++         - keep_alive - Whether to configure RemoteConnection to use HTTP keep-alive.
++        """
++        if log_file:
++            warnings.warn('use service_log_path instead of log_file',
++                          DeprecationWarning, stacklevel=2)
++            service_log_path = log_file
++        if ie_options:
++            warnings.warn('use options instead of ie_options',
++                          DeprecationWarning, stacklevel=2)
++            options = ie_options
++        self.port = port
++        if self.port == 0:
++            self.port = utils.free_port()
++        self.host = host
++
++        # If both capabilities and desired capabilities are set, ignore desired capabilities.
++        if capabilities is None and desired_capabilities:
++            capabilities = desired_capabilities
++
++        if options is None:
++            if capabilities is None:
++                capabilities = self.create_options().to_capabilities()
++        else:
++            if capabilities is None:
++                capabilities = options.to_capabilities()
++            else:
++                # desired_capabilities stays as passed in
++                capabilities.update(options.to_capabilities())
++
++        self.iedriver = Service(
++            executable_path,
++            port=self.port,
++            host=self.host,
++            log_level=log_level,
++            log_file=service_log_path)
++
++        self.iedriver.start()
++
++        RemoteWebDriver.__init__(
++            self,
++            command_executor='http://localhost:%d' % self.port,
++            desired_capabilities=capabilities,
++            keep_alive=keep_alive)
++        self._is_remote = False
++
++    def quit(self):
++        RemoteWebDriver.quit(self)
++        self.iedriver.stop()
++
++    def create_options(self):
++        return Options()
+Index: venv/Lib/site-packages/selenium/webdriver/ie/service.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/ie/service.py	(date 1573549840016)
++++ venv/Lib/site-packages/selenium/webdriver/ie/service.py	(date 1573549840016)
+@@ -0,0 +1,50 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from selenium.webdriver.common import service
++
++
++class Service(service.Service):
++    """
++    Object that manages the starting and stopping of the IEDriver
++    """
++
++    def __init__(self, executable_path, port=0, host=None, log_level=None, log_file=None):
++        """
++        Creates a new instance of the Service
++
++        :Args:
++         - executable_path : Path to the IEDriver
++         - port : Port the service is running on
++         - host : IP address the service port is bound
++         - log_level : Level of logging of service, may be "FATAL", "ERROR", "WARN", "INFO", "DEBUG", "TRACE".
++           Default is "FATAL".
++         - log_file : Target of logging of service, may be "stdout", "stderr" or file path.
++           Default is "stdout"."""
++        self.service_args = []
++        if host is not None:
++            self.service_args.append("--host=%s" % host)
++        if log_level is not None:
++            self.service_args.append("--log-level=%s" % log_level)
++        if log_file is not None:
++            self.service_args.append("--log-file=%s" % log_file)
++
++        service.Service.__init__(self, executable_path, port=port,
++                                 start_error_message="Please download from http://selenium-release.storage.googleapis.com/index.html and read up at https://github.com/SeleniumHQ/selenium/wiki/InternetExplorerDriver")
++
++    def command_line_args(self):
++        return ["--port=%d" % self.port] + self.service_args
+Index: venv/Lib/site-packages/selenium/webdriver/ie/options.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/ie/options.py	(date 1573549840011)
++++ venv/Lib/site-packages/selenium/webdriver/ie/options.py	(date 1573549840011)
+@@ -0,0 +1,351 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
++
++
++class ElementScrollBehavior(object):
++    TOP = 0
++    BOTTOM = 1
++
++
++class Options(object):
++
++    KEY = 'se:ieOptions'
++    SWITCHES = 'ie.browserCommandLineSwitches'
++
++    BROWSER_ATTACH_TIMEOUT = 'browserAttachTimeout'
++    ELEMENT_SCROLL_BEHAVIOR = 'elementScrollBehavior'
++    ENSURE_CLEAN_SESSION = 'ie.ensureCleanSession'
++    FILE_UPLOAD_DIALOG_TIMEOUT = 'ie.fileUploadDialogTimeout'
++    FORCE_CREATE_PROCESS_API = 'ie.forceCreateProcessApi'
++    FORCE_SHELL_WINDOWS_API = 'ie.forceShellWindowsApi'
++    FULL_PAGE_SCREENSHOT = 'ie.enableFullPageScreenshot'
++    IGNORE_PROTECTED_MODE_SETTINGS = 'ignoreProtectedModeSettings'
++    IGNORE_ZOOM_LEVEL = 'ignoreZoomSetting'
++    INITIAL_BROWSER_URL = 'initialBrowserUrl'
++    NATIVE_EVENTS = 'nativeEvents'
++    PERSISTENT_HOVER = 'enablePersistentHover'
++    REQUIRE_WINDOW_FOCUS = 'requireWindowFocus'
++    USE_PER_PROCESS_PROXY = 'ie.usePerProcessProxy'
++    VALIDATE_COOKIE_DOCUMENT_TYPE = 'ie.validateCookieDocumentType'
++
++    def __init__(self):
++        self._arguments = []
++        self._options = {}
++        self._additional = {}
++        self._caps = DesiredCapabilities.INTERNETEXPLORER.copy()
++
++    @property
++    def arguments(self):
++        """ Returns a list of browser process arguments """
++        return self._arguments
++
++    def add_argument(self, argument):
++        """ Add argument to be used for the browser process """
++        if argument is None:
++            raise ValueError()
++        self._arguments.append(argument)
++
++    @property
++    def options(self):
++        """ Returns a dictionary of browser options """
++        return self._options
++
++    @property
++    def capabilities(self):
++        return self._caps
++
++    def set_capability(self, name, value):
++        """Sets a capability."""
++        self._caps[name] = value
++
++    @property
++    def browser_attach_timeout(self):
++        """ Returns the options Browser Attach Timeout in milliseconds """
++        return self._options.get(self.BROWSER_ATTACH_TIMEOUT)
++
++    @browser_attach_timeout.setter
++    def browser_attach_timeout(self, value):
++        """
++        Sets the options Browser Attach Timeout
++
++        :Args:
++         - value: Timeout in milliseconds
++
++        """
++        if not isinstance(value, int):
++            raise ValueError('Browser Attach Timeout must be an integer.')
++        self._options[self.BROWSER_ATTACH_TIMEOUT] = value
++
++    @property
++    def element_scroll_behavior(self):
++        """ Returns the options Element Scroll Behavior in milliseconds """
++        return self._options.get(self.ELEMENT_SCROLL_BEHAVIOR)
++
++    @element_scroll_behavior.setter
++    def element_scroll_behavior(self, value):
++        """
++        Sets the options Element Scroll Behavior
++
++        :Args:
++         - value: 0 - Top, 1 - Bottom
++
++        """
++        if value not in [ElementScrollBehavior.TOP, ElementScrollBehavior.BOTTOM]:
++            raise ValueError('Element Scroll Behavior out of range.')
++        self._options[self.ELEMENT_SCROLL_BEHAVIOR] = value
++
++    @property
++    def ensure_clean_session(self):
++        """ Returns the options Ensure Clean Session value """
++        return self._options.get(self.ENSURE_CLEAN_SESSION)
++
++    @ensure_clean_session.setter
++    def ensure_clean_session(self, value):
++        """
++        Sets the options Ensure Clean Session value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.ENSURE_CLEAN_SESSION] = value
++
++    @property
++    def file_upload_dialog_timeout(self):
++        """ Returns the options File Upload Dialog Timeout in milliseconds """
++        return self._options.get(self.FILE_UPLOAD_DIALOG_TIMEOUT)
++
++    @file_upload_dialog_timeout.setter
++    def file_upload_dialog_timeout(self, value):
++        """
++        Sets the options File Upload Dialog Timeout value
++
++        :Args:
++         - value: Timeout in milliseconds
++
++        """
++        if not isinstance(value, int):
++            raise ValueError('File Upload Dialog Timeout must be an integer.')
++        self._options[self.FILE_UPLOAD_DIALOG_TIMEOUT] = value
++
++    @property
++    def force_create_process_api(self):
++        """ Returns the options Force Create Process Api value """
++        return self._options.get(self.FORCE_CREATE_PROCESS_API)
++
++    @force_create_process_api.setter
++    def force_create_process_api(self, value):
++        """
++        Sets the options Force Create Process Api value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.FORCE_CREATE_PROCESS_API] = value
++
++    @property
++    def force_shell_windows_api(self):
++        """ Returns the options Force Shell Windows Api value """
++        return self._options.get(self.FORCE_SHELL_WINDOWS_API)
++
++    @force_shell_windows_api.setter
++    def force_shell_windows_api(self, value):
++        """
++        Sets the options Force Shell Windows Api value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.FORCE_SHELL_WINDOWS_API] = value
++
++    @property
++    def full_page_screenshot(self):
++        """ Returns the options Full Page Screenshot value """
++        return self._options.get(self.FULL_PAGE_SCREENSHOT)
++
++    @full_page_screenshot.setter
++    def full_page_screenshot(self, value):
++        """
++        Sets the options Full Page Screenshot value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.FULL_PAGE_SCREENSHOT] = value
++
++    @property
++    def ignore_protected_mode_settings(self):
++        """ Returns the options Ignore Protected Mode Settings value """
++        return self._options.get(self.IGNORE_PROTECTED_MODE_SETTINGS)
++
++    @ignore_protected_mode_settings.setter
++    def ignore_protected_mode_settings(self, value):
++        """
++        Sets the options Ignore Protected Mode Settings value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.IGNORE_PROTECTED_MODE_SETTINGS] = value
++
++    @property
++    def ignore_zoom_level(self):
++        """ Returns the options Ignore Zoom Level value """
++        return self._options.get(self.IGNORE_ZOOM_LEVEL)
++
++    @ignore_zoom_level.setter
++    def ignore_zoom_level(self, value):
++        """
++        Sets the options Ignore Zoom Level value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.IGNORE_ZOOM_LEVEL] = value
++
++    @property
++    def initial_browser_url(self):
++        """ Returns the options Initial Browser Url value """
++        return self._options.get(self.INITIAL_BROWSER_URL)
++
++    @initial_browser_url.setter
++    def initial_browser_url(self, value):
++        """
++        Sets the options Initial Browser Url value
++
++        :Args:
++         - value: URL string
++
++        """
++        self._options[self.INITIAL_BROWSER_URL] = value
++
++    @property
++    def native_events(self):
++        """ Returns the options Native Events value """
++        return self._options.get(self.NATIVE_EVENTS)
++
++    @native_events.setter
++    def native_events(self, value):
++        """
++        Sets the options Native Events value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.NATIVE_EVENTS] = value
++
++    @property
++    def persistent_hover(self):
++        """ Returns the options Persistent Hover value """
++        return self._options.get(self.PERSISTENT_HOVER)
++
++    @persistent_hover.setter
++    def persistent_hover(self, value):
++        """
++        Sets the options Persistent Hover value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.PERSISTENT_HOVER] = value
++
++    @property
++    def require_window_focus(self):
++        """ Returns the options Require Window Focus value """
++        return self._options.get(self.REQUIRE_WINDOW_FOCUS)
++
++    @require_window_focus.setter
++    def require_window_focus(self, value):
++        """
++        Sets the options Require Window Focus value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.REQUIRE_WINDOW_FOCUS] = value
++
++    @property
++    def use_per_process_proxy(self):
++        """ Returns the options User Per Process Proxy value """
++        return self._options.get(self.USE_PER_PROCESS_PROXY)
++
++    @use_per_process_proxy.setter
++    def use_per_process_proxy(self, value):
++        """
++        Sets the options User Per Process Proxy value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.USE_PER_PROCESS_PROXY] = value
++
++    @property
++    def validate_cookie_document_type(self):
++        """ Returns the options Validate Cookie Document Type value """
++        return self._options.get(self.VALIDATE_COOKIE_DOCUMENT_TYPE)
++
++    @validate_cookie_document_type.setter
++    def validate_cookie_document_type(self, value):
++        """
++        Sets the options Validate Cookie Document Type value
++
++        :Args:
++         - value: boolean value
++
++        """
++        self._options[self.VALIDATE_COOKIE_DOCUMENT_TYPE] = value
++
++    @property
++    def additional_options(self):
++        """ Returns the additional options """
++        return self._additional
++
++    def add_additional_option(self, name, value):
++        """
++        Adds an additional option not yet added as a safe option for IE
++
++        :Args:
++         - name: name of the option to add
++         - value: value of the option to add
++
++        """
++        self._additional[name] = value
++
++    def to_capabilities(self):
++        """ Marshals the IE options to a the correct object """
++        caps = self._caps
++
++        opts = self._options.copy()
++        if len(self._arguments) > 0:
++            opts[self.SWITCHES] = ' '.join(self._arguments)
++
++        if len(self._additional) > 0:
++            opts.update(self._additional)
++
++        if len(opts) > 0:
++            caps[Options.KEY] = opts
++        return caps
+Index: venv/Lib/site-packages/selenium/webdriver/ie/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/ie/__init__.py	(date 1573549840006)
++++ venv/Lib/site-packages/selenium/webdriver/ie/__init__.py	(date 1573549840006)
+@@ -0,0 +1,16 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
+Index: venv/Lib/site-packages/selenium/webdriver/edge/webdriver.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/edge/webdriver.py	(date 1573549839921)
++++ venv/Lib/site-packages/selenium/webdriver/edge/webdriver.py	(date 1573549839921)
+@@ -0,0 +1,71 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++import warnings
++
++from selenium.webdriver.common import utils
++from selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver
++from selenium.webdriver.remote.remote_connection import RemoteConnection
++from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
++from .service import Service
++
++
++class WebDriver(RemoteWebDriver):
++
++    def __init__(self, executable_path='MicrosoftWebDriver.exe',
++                 capabilities=None, port=0, verbose=False, service_log_path=None,
++                 log_path=None, keep_alive=False):
++        """
++        Creates a new instance of the chrome driver.
++
++        Starts the service and then creates new instance of chrome driver.
++
++        :Args:
++         - executable_path - path to the executable. If the default is used it assumes the executable is in the $PATH
++         - capabilities - Dictionary object with non-browser specific
++           capabilities only, such as "proxy" or "loggingPref".
++         - port - port you would like the service to run, if left as 0, a free port will be found.
++         - verbose - whether to set verbose logging in the service
++         - service_log_path - Where to log information from the driver.
++         - log_path: Deprecated argument for service_log_path
++         - keep_alive - Whether to configure ChromeRemoteConnection to use HTTP keep-alive.
++         """
++        if log_path:
++            warnings.warn('use service_log_path instead of log_path',
++                          DeprecationWarning, stacklevel=2)
++            service_log_path = log_path
++
++        self.port = port
++        if self.port == 0:
++            self.port = utils.free_port()
++
++        self.edge_service = Service(executable_path, port=self.port, verbose=verbose, log_path=service_log_path)
++        self.edge_service.start()
++
++        if capabilities is None:
++            capabilities = DesiredCapabilities.EDGE
++
++        RemoteWebDriver.__init__(
++            self,
++            command_executor=RemoteConnection('http://localhost:%d' % self.port,
++                                              resolve_ip=False,
++                                              keep_alive=keep_alive),
++            desired_capabilities=capabilities)
++        self._is_remote = False
++
++    def quit(self):
++        RemoteWebDriver.quit(self)
++        self.edge_service.stop()
+Index: venv/Lib/site-packages/selenium/webdriver/edge/service.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/edge/service.py	(date 1573549839916)
++++ venv/Lib/site-packages/selenium/webdriver/edge/service.py	(date 1573549839916)
+@@ -0,0 +1,57 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from selenium.webdriver.common import service
++
++
++class Service(service.Service):
++
++    def __init__(self, executable_path, port=0, verbose=False, log_path=None):
++        """
++        Creates a new instance of the EdgeDriver service.
++
++        EdgeDriver provides an interface for Microsoft WebDriver to use
++        with Microsoft Edge.
++
++        :param executable_path: Path to the Microsoft WebDriver binary.
++        :param port: Run the remote service on a specified port.
++            Defaults to 0, which binds to a random open port of the
++            system's choosing.
++        :verbose: Whether to make the webdriver more verbose (passes the
++            --verbose option to the binary). Defaults to False.
++        :param log_path: Optional path for the webdriver binary to log to.
++            Defaults to None which disables logging.
++
++        """
++
++        self.service_args = []
++        if verbose:
++            self.service_args.append("--verbose")
++
++        params = {
++            "executable": executable_path,
++            "port": port,
++            "start_error_message": "Please download from http://go.microsoft.com/fwlink/?LinkId=619687"
++        }
++
++        if log_path:
++            params["log_file"] = open(log_path, "a+")
++
++        service.Service.__init__(self, **params)
++
++    def command_line_args(self):
++        return ["--port=%d" % self.port] + self.service_args
+Index: venv/Lib/site-packages/selenium/webdriver/edge/options.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/edge/options.py	(date 1573549839911)
++++ venv/Lib/site-packages/selenium/webdriver/edge/options.py	(date 1573549839911)
+@@ -0,0 +1,54 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
++
++
++class Options(object):
++
++    def __init__(self):
++        self._page_load_strategy = "normal"
++        self._caps = DesiredCapabilities.EDGE.copy()
++
++    @property
++    def page_load_strategy(self):
++        return self._page_load_strategy
++
++    @page_load_strategy.setter
++    def page_load_strategy(self, value):
++        if value not in ['normal', 'eager', 'none']:
++            raise ValueError("Page Load Strategy should be 'normal', 'eager' or 'none'.")
++        self._page_load_strategy = value
++
++    @property
++    def capabilities(self):
++        return self._caps
++
++    def set_capability(self, name, value):
++        """Sets a capability."""
++        self._caps[name] = value
++
++    def to_capabilities(self):
++        """
++            Creates a capabilities with all the options that have been set and
++
++            returns a dictionary with everything
++        """
++        caps = self._caps
++        caps['pageLoadStrategy'] = self._page_load_strategy
++
++        return caps
+Index: venv/Lib/site-packages/selenium/webdriver/edge/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/edge/__init__.py	(date 1573549839906)
++++ venv/Lib/site-packages/selenium/webdriver/edge/__init__.py	(date 1573549839906)
+@@ -0,0 +1,16 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
+Index: venv/Lib/site-packages/selenium/webdriver/opera/webdriver.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/opera/webdriver.py	(date 1573549840037)
++++ venv/Lib/site-packages/selenium/webdriver/opera/webdriver.py	(date 1573549840037)
+@@ -0,0 +1,83 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++import warnings
++
++from selenium.webdriver.chrome.webdriver import WebDriver as ChromiumDriver
++from .options import Options
++
++
++class OperaDriver(ChromiumDriver):
++    """Controls the new OperaDriver and allows you
++    to drive the Opera browser based on Chromium."""
++
++    def __init__(self, executable_path=None, port=0,
++                 options=None, service_args=None,
++                 desired_capabilities=None, service_log_path=None,
++                 opera_options=None, keep_alive=True):
++        """
++        Creates a new instance of the operadriver.
++
++        Starts the service and then creates new instance of operadriver.
++
++        :Args:
++         - executable_path - path to the executable. If the default is used
++                             it assumes the executable is in the $PATH
++         - port - port you would like the service to run, if left as 0,
++                  a free port will be found.
++         - options: this takes an instance of OperaOptions
++         - service_args - List of args to pass to the driver service
++         - desired_capabilities: Dictionary object with non-browser specific
++         - service_log_path - Where to log information from the driver.
++         - opera_options - Deprecated argument for options
++           capabilities only, such as "proxy" or "loggingPref".
++        """
++        if opera_options:
++            warnings.warn('use options instead of opera_options',
++                          DeprecationWarning, stacklevel=2)
++            options = opera_options
++
++        executable_path = (executable_path if executable_path is not None
++                           else "operadriver")
++        ChromiumDriver.__init__(self,
++                                executable_path=executable_path,
++                                port=port,
++                                options=options,
++                                service_args=service_args,
++                                desired_capabilities=desired_capabilities,
++                                service_log_path=service_log_path,
++                                keep_alive=keep_alive)
++
++    def create_options(self):
++        return Options()
++
++
++class WebDriver(OperaDriver):
++    class ServiceType:
++        CHROMIUM = 2
++
++    def __init__(self,
++                 desired_capabilities=None,
++                 executable_path=None,
++                 port=0,
++                 service_log_path=None,
++                 service_args=None,
++                 options=None):
++        OperaDriver.__init__(self, executable_path=executable_path,
++                             port=port, options=options,
++                             service_args=service_args,
++                             desired_capabilities=desired_capabilities,
++                             service_log_path=service_log_path)
+Index: venv/Lib/site-packages/selenium/webdriver/opera/options.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/opera/options.py	(date 1573549840030)
++++ venv/Lib/site-packages/selenium/webdriver/opera/options.py	(date 1573549840030)
+@@ -0,0 +1,115 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from selenium.webdriver.chrome.options import Options as ChromeOptions
++from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
++
++
++class Options(ChromeOptions):
++    KEY = "operaOptions"
++
++    def __init__(self):
++        ChromeOptions.__init__(self)
++        self._android_package_name = ''
++        self._android_device_socket = ''
++        self._android_command_line_file = ''
++        self._caps = DesiredCapabilities.OPERA.copy()
++
++    @property
++    def capabilities(self):
++        return self._caps
++
++    def set_capability(self, name, value):
++        """Sets a capability."""
++        self._caps[name] = value
++
++    @property
++    def android_package_name(self):
++        """
++        Returns the name of the Opera package
++        """
++        return self._android_package_name
++
++    @android_package_name.setter
++    def android_package_name(self, value):
++        """
++        Allows you to set the package name
++
++        :Args:
++         - value: devtools socket name
++        """
++        self._android_package_name = value
++
++    @property
++    def android_device_socket(self):
++        """
++        Returns the name of the devtools socket
++        """
++        return self._android_device_socket
++
++    @android_device_socket.setter
++    def android_device_socket(self, value):
++        """
++        Allows you to set the devtools socket name
++
++        :Args:
++         - value: devtools socket name
++        """
++        self._android_device_socket = value
++
++    @property
++    def android_command_line_file(self):
++        """
++        Returns the path of the command line file
++        """
++        return self._android_command_line_file
++
++    @android_command_line_file.setter
++    def android_command_line_file(self, value):
++        """
++        Allows you to set where the command line file lives
++
++        :Args:
++         - value: command line file path
++        """
++        self._android_command_line_file = value
++
++    def to_capabilities(self):
++        """
++            Creates a capabilities with all the options that have been set and
++
++            returns a dictionary with everything
++        """
++        capabilities = ChromeOptions.to_capabilities(self)
++        capabilities.update(self._caps)
++        opera_options = capabilities[self.KEY]
++
++        if self.android_package_name:
++            opera_options["androidPackage"] = self.android_package_name
++        if self.android_device_socket:
++            opera_options["androidDeviceSocket"] = self.android_device_socket
++        if self.android_command_line_file:
++            opera_options["androidCommandLineFile"] = \
++                self.android_command_line_file
++        return capabilities
++
++
++class AndroidOptions(Options):
++
++    def __init__(self):
++        Options.__init__(self)
++        self.android_package_name = 'com.opera.browser'
+Index: venv/Lib/site-packages/selenium/webdriver/opera/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/opera/__init__.py	(date 1573549840026)
++++ venv/Lib/site-packages/selenium/webdriver/opera/__init__.py	(date 1573549840026)
+@@ -0,0 +1,16 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
+Index: venv/Lib/site-packages/selenium/webdriver/chrome/webdriver.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/chrome/webdriver.py	(date 1573549839759)
++++ venv/Lib/site-packages/selenium/webdriver/chrome/webdriver.py	(date 1573549839759)
+@@ -0,0 +1,161 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++import warnings
++
++from selenium.webdriver.remote.webdriver import WebDriver as RemoteWebDriver
++from .remote_connection import ChromeRemoteConnection
++from .service import Service
++from .options import Options
++
++
++class WebDriver(RemoteWebDriver):
++    """
++    Controls the ChromeDriver and allows you to drive the browser.
++
++    You will need to download the ChromeDriver executable from
++    http://chromedriver.storage.googleapis.com/index.html
++    """
++
++    def __init__(self, executable_path="chromedriver", port=0,
++                 options=None, service_args=None,
++                 desired_capabilities=None, service_log_path=None,
++                 chrome_options=None, keep_alive=True):
++        """
++        Creates a new instance of the chrome driver.
++
++        Starts the service and then creates new instance of chrome driver.
++
++        :Args:
++         - executable_path - path to the executable. If the default is used it assumes the executable is in the $PATH
++         - port - port you would like the service to run, if left as 0, a free port will be found.
++         - options - this takes an instance of ChromeOptions
++         - service_args - List of args to pass to the driver service
++         - desired_capabilities - Dictionary object with non-browser specific
++           capabilities only, such as "proxy" or "loggingPref".
++         - service_log_path - Where to log information from the driver.
++         - chrome_options - Deprecated argument for options
++         - keep_alive - Whether to configure ChromeRemoteConnection to use HTTP keep-alive.
++        """
++        if chrome_options:
++            warnings.warn('use options instead of chrome_options',
++                          DeprecationWarning, stacklevel=2)
++            options = chrome_options
++
++        if options is None:
++            # desired_capabilities stays as passed in
++            if desired_capabilities is None:
++                desired_capabilities = self.create_options().to_capabilities()
++        else:
++            if desired_capabilities is None:
++                desired_capabilities = options.to_capabilities()
++            else:
++                desired_capabilities.update(options.to_capabilities())
++
++        self.service = Service(
++            executable_path,
++            port=port,
++            service_args=service_args,
++            log_path=service_log_path)
++        self.service.start()
++
++        try:
++            RemoteWebDriver.__init__(
++                self,
++                command_executor=ChromeRemoteConnection(
++                    remote_server_addr=self.service.service_url,
++                    keep_alive=keep_alive),
++                desired_capabilities=desired_capabilities)
++        except Exception:
++            self.quit()
++            raise
++        self._is_remote = False
++
++    def launch_app(self, id):
++        """Launches Chrome app specified by id."""
++        return self.execute("launchApp", {'id': id})
++
++    def get_network_conditions(self):
++        """
++        Gets Chrome network emulation settings.
++
++        :Returns:
++            A dict. For example:
++
++            {'latency': 4, 'download_throughput': 2, 'upload_throughput': 2,
++            'offline': False}
++
++        """
++        return self.execute("getNetworkConditions")['value']
++
++    def set_network_conditions(self, **network_conditions):
++        """
++        Sets Chrome network emulation settings.
++
++        :Args:
++         - network_conditions: A dict with conditions specification.
++
++        :Usage:
++            driver.set_network_conditions(
++                offline=False,
++                latency=5,  # additional latency (ms)
++                download_throughput=500 * 1024,  # maximal throughput
++                upload_throughput=500 * 1024)  # maximal throughput
++
++            Note: 'throughput' can be used to set both (for download and upload).
++        """
++        self.execute("setNetworkConditions", {
++            'network_conditions': network_conditions
++        })
++
++    def execute_cdp_cmd(self, cmd, cmd_args):
++        """
++        Execute Chrome Devtools Protocol command and get returned result
++
++        The command and command args should follow chrome devtools protocol domains/commands, refer to link
++        https://chromedevtools.github.io/devtools-protocol/
++
++        :Args:
++         - cmd: A str, command name
++         - cmd_args: A dict, command args. empty dict {} if there is no command args
++
++        :Usage:
++            driver.execute_cdp_cmd('Network.getResponseBody', {'requestId': requestId})
++
++        :Returns:
++            A dict, empty dict {} if there is no result to return.
++            For example to getResponseBody:
++
++            {'base64Encoded': False, 'body': 'response body string'}
++
++        """
++        return self.execute("executeCdpCommand", {'cmd': cmd, 'params': cmd_args})['value']
++
++    def quit(self):
++        """
++        Closes the browser and shuts down the ChromeDriver executable
++        that is started when starting the ChromeDriver
++        """
++        try:
++            RemoteWebDriver.quit(self)
++        except Exception:
++            # We don't care about the message because something probably has gone wrong
++            pass
++        finally:
++            self.service.stop()
++
++    def create_options(self):
++        return Options()
+Index: venv/Lib/site-packages/selenium/webdriver/chrome/service.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/chrome/service.py	(date 1573549839754)
++++ venv/Lib/site-packages/selenium/webdriver/chrome/service.py	(date 1573549839754)
+@@ -0,0 +1,45 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from selenium.webdriver.common import service
++
++
++class Service(service.Service):
++    """
++    Object that manages the starting and stopping of the ChromeDriver
++    """
++
++    def __init__(self, executable_path, port=0, service_args=None,
++                 log_path=None, env=None):
++        """
++        Creates a new instance of the Service
++
++        :Args:
++         - executable_path : Path to the ChromeDriver
++         - port : Port the service is running on
++         - service_args : List of args to pass to the chromedriver service
++         - log_path : Path for the chromedriver service to log to"""
++
++        self.service_args = service_args or []
++        if log_path:
++            self.service_args.append('--log-path=%s' % log_path)
++
++        service.Service.__init__(self, executable_path, port=port, env=env,
++                                 start_error_message="Please see https://sites.google.com/a/chromium.org/chromedriver/home")
++
++    def command_line_args(self):
++        return ["--port=%d" % self.port] + self.service_args
+Index: venv/Lib/site-packages/selenium/webdriver/chrome/options.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/chrome/options.py	(date 1573549839742)
++++ venv/Lib/site-packages/selenium/webdriver/chrome/options.py	(date 1573549839742)
+@@ -0,0 +1,211 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++import base64
++import os
++import platform
++import warnings
++
++from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
++
++
++class Options(object):
++    KEY = "goog:chromeOptions"
++
++    def __init__(self):
++        self._binary_location = ''
++        self._arguments = []
++        self._extension_files = []
++        self._extensions = []
++        self._experimental_options = {}
++        self._debugger_address = None
++        self._caps = DesiredCapabilities.CHROME.copy()
++
++    @property
++    def binary_location(self):
++        """
++        Returns the location of the binary otherwise an empty string
++        """
++        return self._binary_location
++
++    @binary_location.setter
++    def binary_location(self, value):
++        """
++        Allows you to set where the chromium binary lives
++
++        :Args:
++         - value: path to the Chromium binary
++        """
++        self._binary_location = value
++
++    @property
++    def capabilities(self):
++        return self._caps
++
++    def set_capability(self, name, value):
++        """Sets a capability."""
++        self._caps[name] = value
++
++    @property
++    def debugger_address(self):
++        """
++        Returns the address of the remote devtools instance
++        """
++        return self._debugger_address
++
++    @debugger_address.setter
++    def debugger_address(self, value):
++        """
++        Allows you to set the address of the remote devtools instance
++        that the ChromeDriver instance will try to connect to during an
++        active wait.
++
++        :Args:
++         - value: address of remote devtools instance if any (hostname[:port])
++        """
++        self._debugger_address = value
++
++    @property
++    def arguments(self):
++        """
++        Returns a list of arguments needed for the browser
++        """
++        return self._arguments
++
++    def add_argument(self, argument):
++        """
++        Adds an argument to the list
++
++        :Args:
++         - Sets the arguments
++        """
++        if argument:
++            self._arguments.append(argument)
++        else:
++            raise ValueError("argument can not be null")
++
++    @property
++    def extensions(self):
++        """
++        Returns a list of encoded extensions that will be loaded into chrome
++
++        """
++        encoded_extensions = []
++        for ext in self._extension_files:
++            file_ = open(ext, 'rb')
++            # Should not use base64.encodestring() which inserts newlines every
++            # 76 characters (per RFC 1521).  Chromedriver has to remove those
++            # unnecessary newlines before decoding, causing performance hit.
++            encoded_extensions.append(base64.b64encode(file_.read()).decode('UTF-8'))
++
++            file_.close()
++        return encoded_extensions + self._extensions
++
++    def add_extension(self, extension):
++        """
++        Adds the path to the extension to a list that will be used to extract it
++        to the ChromeDriver
++
++        :Args:
++         - extension: path to the \*.crx file
++        """
++        if extension:
++            extension_to_add = os.path.abspath(os.path.expanduser(extension))
++            if os.path.exists(extension_to_add):
++                self._extension_files.append(extension_to_add)
++            else:
++                raise IOError("Path to the extension doesn't exist")
++        else:
++            raise ValueError("argument can not be null")
++
++    def add_encoded_extension(self, extension):
++        """
++        Adds Base64 encoded string with extension data to a list that will be used to extract it
++        to the ChromeDriver
++
++        :Args:
++         - extension: Base64 encoded string with extension data
++        """
++        if extension:
++            self._extensions.append(extension)
++        else:
++            raise ValueError("argument can not be null")
++
++    @property
++    def experimental_options(self):
++        """
++        Returns a dictionary of experimental options for chrome.
++        """
++        return self._experimental_options
++
++    def add_experimental_option(self, name, value):
++        """
++        Adds an experimental option which is passed to chrome.
++
++        Args:
++          name: The experimental option name.
++          value: The option value.
++        """
++        self._experimental_options[name] = value
++
++    @property
++    def headless(self):
++        """
++        Returns whether or not the headless argument is set
++        """
++        return '--headless' in self._arguments
++
++    @headless.setter
++    def headless(self, value):
++        """
++        Sets the headless argument
++
++        Args:
++          value: boolean value indicating to set the headless option
++        """
++        args = {'--headless'}
++        if platform.system().lower() == 'windows':
++            args.add('--disable-gpu')
++        if value is True:
++            self._arguments.extend(args)
++        else:
++            self._arguments = list(set(self._arguments) - args)
++
++    def set_headless(self, headless=True):
++        """ Deprecated, options.headless = True """
++        warnings.warn('use setter for headless property instead of set_headless',
++                      DeprecationWarning, stacklevel=2)
++        self.headless = headless
++
++    def to_capabilities(self):
++        """
++            Creates a capabilities with all the options that have been set and
++
++            returns a dictionary with everything
++        """
++        caps = self._caps
++        chrome_options = self.experimental_options.copy()
++        chrome_options["extensions"] = self.extensions
++        if self.binary_location:
++            chrome_options["binary"] = self.binary_location
++        chrome_options["args"] = self.arguments
++        if self.debugger_address:
++            chrome_options["debuggerAddress"] = self.debugger_address
++
++        caps[self.KEY] = chrome_options
++
++        return caps
+Index: venv/Lib/site-packages/selenium/webdriver/chrome/remote_connection.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/chrome/remote_connection.py	(date 1573549839749)
++++ venv/Lib/site-packages/selenium/webdriver/chrome/remote_connection.py	(date 1573549839749)
+@@ -0,0 +1,28 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++from selenium.webdriver.remote.remote_connection import RemoteConnection
++
++
++class ChromeRemoteConnection(RemoteConnection):
++
++    def __init__(self, remote_server_addr, keep_alive=True):
++        RemoteConnection.__init__(self, remote_server_addr, keep_alive)
++        self._commands["launchApp"] = ('POST', '/session/$sessionId/chromium/launch_app')
++        self._commands["setNetworkConditions"] = ('POST', '/session/$sessionId/chromium/network_conditions')
++        self._commands["getNetworkConditions"] = ('GET', '/session/$sessionId/chromium/network_conditions')
++        self._commands['executeCdpCommand'] = ('POST', '/session/$sessionId/goog/cdp/execute')
+Index: venv/Lib/site-packages/selenium/webdriver/chrome/__init__.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/chrome/__init__.py	(date 1573549839738)
++++ venv/Lib/site-packages/selenium/webdriver/chrome/__init__.py	(date 1573549839738)
+@@ -0,0 +1,16 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
+Index: venv/Lib/site-packages/selenium/webdriver/common/touch_actions.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/common/touch_actions.py	(date 1573549839838)
++++ venv/Lib/site-packages/selenium/webdriver/common/touch_actions.py	(date 1573549839838)
+@@ -0,0 +1,192 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++"""
++The Touch Actions implementation
++"""
++
++from selenium.webdriver.remote.command import Command
++
++
++class TouchActions(object):
++    """
++    Generate touch actions. Works like ActionChains; actions are stored in the
++    TouchActions object and are fired with perform().
++    """
++
++    def __init__(self, driver):
++        """
++        Creates a new TouchActions object.
++
++        :Args:
++         - driver: The WebDriver instance which performs user actions.
++           It should be with touchscreen enabled.
++        """
++        self._driver = driver
++        self._actions = []
++
++    def perform(self):
++        """
++        Performs all stored actions.
++        """
++        for action in self._actions:
++            action()
++
++    def tap(self, on_element):
++        """
++        Taps on a given element.
++
++        :Args:
++         - on_element: The element to tap.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.SINGLE_TAP, {'element': on_element.id}))
++        return self
++
++    def double_tap(self, on_element):
++        """
++        Double taps on a given element.
++
++        :Args:
++         - on_element: The element to tap.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.DOUBLE_TAP, {'element': on_element.id}))
++        return self
++
++    def tap_and_hold(self, xcoord, ycoord):
++        """
++        Touch down at given coordinates.
++
++        :Args:
++         - xcoord: X Coordinate to touch down.
++         - ycoord: Y Coordinate to touch down.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.TOUCH_DOWN, {
++                'x': int(xcoord),
++                'y': int(ycoord)}))
++        return self
++
++    def move(self, xcoord, ycoord):
++        """
++        Move held tap to specified location.
++
++        :Args:
++         - xcoord: X Coordinate to move.
++         - ycoord: Y Coordinate to move.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.TOUCH_MOVE, {
++                'x': int(xcoord),
++                'y': int(ycoord)}))
++        return self
++
++    def release(self, xcoord, ycoord):
++        """
++        Release previously issued tap 'and hold' command at specified location.
++
++        :Args:
++         - xcoord: X Coordinate to release.
++         - ycoord: Y Coordinate to release.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.TOUCH_UP, {
++                'x': int(xcoord),
++                'y': int(ycoord)}))
++        return self
++
++    def scroll(self, xoffset, yoffset):
++        """
++        Touch and scroll, moving by xoffset and yoffset.
++
++        :Args:
++         - xoffset: X offset to scroll to.
++         - yoffset: Y offset to scroll to.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.TOUCH_SCROLL, {
++                'xoffset': int(xoffset),
++                'yoffset': int(yoffset)}))
++        return self
++
++    def scroll_from_element(self, on_element, xoffset, yoffset):
++        """
++        Touch and scroll starting at on_element, moving by xoffset and yoffset.
++
++        :Args:
++         - on_element: The element where scroll starts.
++         - xoffset: X offset to scroll to.
++         - yoffset: Y offset to scroll to.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.TOUCH_SCROLL, {
++                'element': on_element.id,
++                'xoffset': int(xoffset),
++                'yoffset': int(yoffset)}))
++        return self
++
++    def long_press(self, on_element):
++        """
++        Long press on an element.
++
++        :Args:
++         - on_element: The element to long press.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.LONG_PRESS, {'element': on_element.id}))
++        return self
++
++    def flick(self, xspeed, yspeed):
++        """
++        Flicks, starting anywhere on the screen.
++
++        :Args:
++         - xspeed: The X speed in pixels per second.
++         - yspeed: The Y speed in pixels per second.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.FLICK, {
++                'xspeed': int(xspeed),
++                'yspeed': int(yspeed)}))
++        return self
++
++    def flick_element(self, on_element, xoffset, yoffset, speed):
++        """
++        Flick starting at on_element, and moving by the xoffset and yoffset
++        with specified speed.
++
++        :Args:
++         - on_element: Flick will start at center of element.
++         - xoffset: X offset to flick to.
++         - yoffset: Y offset to flick to.
++         - speed: Pixels per second to flick.
++        """
++        self._actions.append(lambda: self._driver.execute(
++            Command.FLICK, {
++                'element': on_element.id,
++                'xoffset': int(xoffset),
++                'yoffset': int(yoffset),
++                'speed': int(speed)}))
++        return self
++
++    # Context manager so TouchActions can be used in a 'with .. as' statements.
++    def __enter__(self):
++        return self  # Return created instance of self.
++
++    def __exit__(self, _type, _value, _traceback):
++        pass  # Do nothing, does not require additional cleanup.
+Index: venv/Lib/site-packages/selenium/webdriver/common/desired_capabilities.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/common/desired_capabilities.py	(date 1573549839819)
++++ venv/Lib/site-packages/selenium/webdriver/common/desired_capabilities.py	(date 1573549839819)
+@@ -0,0 +1,128 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
++# specific language governing permissions and limitations
++# under the License.
++
++"""
++The Desired Capabilities implementation.
++"""
++
++
++class DesiredCapabilities(object):
++    """
++    Set of default supported desired capabilities.
++
++    Use this as a starting point for creating a desired capabilities object for
++    requesting remote webdrivers for connecting to selenium server or selenium grid.
++
++    Usage Example::
++
++        from selenium import webdriver
++
++        selenium_grid_url = "http://198.0.0.1:4444/wd/hub"
++
++        # Create a desired capabilities object as a starting point.
++        capabilities = DesiredCapabilities.FIREFOX.copy()
++        capabilities['platform'] = "WINDOWS"
++        capabilities['version'] = "10"
++
++        # Instantiate an instance of Remote WebDriver with the desired capabilities.
++        driver = webdriver.Remote(desired_capabilities=capabilities,
++                                  command_executor=selenium_grid_url)
++
++    Note: Always use '.copy()' on the DesiredCapabilities object to avoid the side
++    effects of altering the Global class instance.
++
++    """
++
++    FIREFOX = {
++        "browserName": "firefox",
++        "marionette": True,
++        "acceptInsecureCerts": True,
++    }
++
++    INTERNETEXPLORER = {
++        "browserName": "internet explorer",
++        "version": "",
++        "platform": "WINDOWS",
++    }
++
++    EDGE = {
++        "browserName": "MicrosoftEdge",
++        "version": "",
++        "platform": "WINDOWS"
++    }
++
++    CHROME = {
++        "browserName": "chrome",
++        "version": "",
++        "platform": "ANY",
++    }
++
++    OPERA = {
++        "browserName": "opera",
++        "version": "",
++        "platform": "ANY",
++    }
++
++    SAFARI = {
++        "browserName": "safari",
++        "version": "",
++        "platform": "MAC",
++    }
++
++    HTMLUNIT = {
++        "browserName": "htmlunit",
++        "version": "",
++        "platform": "ANY",
++    }
++
++    HTMLUNITWITHJS = {
++        "browserName": "htmlunit",
++        "version": "firefox",
++        "platform": "ANY",
++        "javascriptEnabled": True,
++    }
++
++    IPHONE = {
++        "browserName": "iPhone",
++        "version": "",
++        "platform": "MAC",
++    }
++
++    IPAD = {
++        "browserName": "iPad",
++        "version": "",
++        "platform": "MAC",
++    }
++
++    ANDROID = {
++        "browserName": "android",
++        "version": "",
++        "platform": "ANDROID",
++    }
++
++    PHANTOMJS = {
++        "browserName": "phantomjs",
++        "version": "",
++        "platform": "ANY",
++        "javascriptEnabled": True,
++    }
++
++    WEBKITGTK = {
++        "browserName": "MiniBrowser",
++        "version": "",
++        "platform": "ANY",
++    }
+Index: venv/Lib/site-packages/selenium/webdriver/common/proxy.py
+IDEA additional info:
+Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
+<+>UTF-8
+===================================================================
+--- venv/Lib/site-packages/selenium/webdriver/common/proxy.py	(date 1573549839829)
++++ venv/Lib/site-packages/selenium/webdriver/common/proxy.py	(date 1573549839829)
+@@ -0,0 +1,334 @@
++# Licensed to the Software Freedom Conservancy (SFC) under one
++# or more contributor license agreements.  See the NOTICE file
++# distributed with this work for additional information
++# regarding copyright ownership.  The SFC licenses this file
++# to you under the Apache License, Version 2.0 (the
++# "License"); you may not use this file except in compliance
++# with the License.  You may obtain a copy of the License at
++#
++#   http://www.apache.org/licenses/LICENSE-2.0
++#
++# Unless required by applicable law or agreed to in writing,
++# software distributed under the License is distributed on an
++# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
++# KIND, either express or implied.  See the License for the
